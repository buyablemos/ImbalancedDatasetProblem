{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T22:11:01.491399Z",
     "start_time": "2025-05-23T22:11:01.486196Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from torchinfo import summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from CNNVAE import CNNVAE\n",
    "from ResNet34 import ResNetTrainer\n",
    "from VAE import VAE\n",
    "from GAN import GAN\n",
    "from CNNGAN import CNNGAN"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T22:09:15.054671Z",
     "start_time": "2025-05-23T22:09:15.048528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parametry modelu\n",
    "IMG_SIZE = 128\n",
    "CHANNELS = 3\n",
    "LATENT_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "# Konfiguracja\n",
    "result_dir = 'results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/GAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNGAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/VAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNVAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "name = 'cnnvae'\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(f\"Training device: {device}\")"
   ],
   "id": "62222097ab218943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: mps\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "id": "45054a3b231c03ba"
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install torch==2.2.2+cu121 torchvision==0.17.2+cu121 --index-url https://download.pytorch.org/whl/cu121"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T22:09:17.543278Z",
     "start_time": "2025-05-23T22:09:16.278113Z"
    }
   },
   "id": "fb11c78706a074a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement torch==2.2.2+cu121 (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for torch==2.2.2+cu121\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T22:09:19.044874Z",
     "start_time": "2025-05-23T22:09:17.545028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CapsuleNegativeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(1.1, 1.15),fill=255),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = CapsuleNegativeDataset(\n",
    "    root_dir=os.path.join(kagglehub.dataset_download(\"tladilebohang/capsule-defects\"), 'capsule/negative'),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Podzia≈Ç danych\n",
    "full_indices = np.arange(len(dataset))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    full_indices, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#Print first image\n",
    "data_iter = iter(train_loader)\n",
    "images = next(data_iter)  # \n",
    "\n",
    "image = images[0]\n",
    "\n",
    "image = (image+1)/2\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "image = np.array(image)\n",
    "print(image.shape)\n",
    "\n",
    "plt.imshow(image)\n"
   ],
   "id": "c2cbc3f89ac195e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "(128, 128, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x334685790>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "source": [
    "class GANLogger:\n",
    "    def __init__(self, save_dir: str, filename: str = \"history\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.filename = filename\n",
    "        # Initialize empty history\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"epoch\": [],\n",
    "            \"gen_loss\": [],\n",
    "            \"disc_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "        }\n",
    "\n",
    "    def log(self, epoch: int, gen_loss: float, disc_loss: float, val_loss: float):\n",
    "        \"\"\"Append one epoch‚Äôs metrics.\"\"\"\n",
    "        self.history[\"epoch\"].append(epoch)\n",
    "        self.history[\"gen_loss\"].append(gen_loss)\n",
    "        self.history[\"disc_loss\"].append(disc_loss)\n",
    "        self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    def save_json(self):\n",
    "        \"\"\"Dump the history to a JSON file.\"\"\"\n",
    "        json_path = self.save_dir / f\"{self.filename}.json\"\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T21:56:20.035887Z",
     "start_time": "2025-05-23T21:56:20.028288Z"
    }
   },
   "id": "577d6982f00cf188",
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "class VAELogger:\n",
    "    def __init__(self, save_dir: str, filename: str = \"history\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.filename = filename\n",
    "        # Initialize empty history\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"epoch\": [],\n",
    "            \"train_loss\": [],\n",
    "            \"validation_loss\": [],\n",
    "        }\n",
    "\n",
    "    def log(self, epoch: int, train_loss: float, val_loss: float):\n",
    "        \"\"\"Append one epoch‚Äôs metrics.\"\"\"\n",
    "        self.history[\"epoch\"].append(epoch)\n",
    "        self.history[\"train_loss\"].append(train_loss)\n",
    "        self.history[\"validation_loss\"].append(val_loss)\n",
    "\n",
    "    def save_json(self):\n",
    "        \"\"\"Dump the history to a JSON file.\"\"\"\n",
    "        json_path = self.save_dir / f\"{self.filename}.json\"\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T21:56:20.044578Z",
     "start_time": "2025-05-23T21:56:20.038107Z"
    }
   },
   "id": "3a2c57e2a592d6f5",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# VAE"
   ],
   "id": "175252c98d87c348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T22:11:23.879391Z",
     "start_time": "2025-05-23T22:11:23.739139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicjalizacja\n",
    "\n",
    "#model = VAE(device=device,result_dir='results/VAE',load_pretrained=True).to(device)\n",
    "\n",
    "model = CNNVAE(device=device,result_dir='results/CNNVAE',load_pretrained=True).to(device)\n",
    "\n",
    "\n",
    "if isinstance(model,CNNVAE):\n",
    "    result_dir = 'results/CNNVAE'\n",
    "    name = 'cnnvae'\n",
    "\n",
    "\n",
    "if isinstance(model,VAE):\n",
    "    result_dir = 'results/VAE'\n",
    "    name = 'vae'\n",
    "\n",
    "dummy_input = torch.zeros(1, CHANNELS, IMG_SIZE, IMG_SIZE).to(device)  \n",
    "summary(model, input_data=dummy_input)"
   ],
   "id": "882bc931ff30fb55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded best VAE weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNVAE                                   [1, 3, 128, 128]          --\n",
       "‚îú‚îÄSequential: 1-1                        [1, 512, 4, 4]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-1                       [1, 32, 64, 64]           1,568\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-2                         [1, 32, 64, 64]           --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-3                       [1, 64, 32, 32]           32,832\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-4                         [1, 64, 32, 32]           --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-5                       [1, 128, 16, 16]          131,200\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-6                         [1, 128, 16, 16]          --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-7                       [1, 256, 8, 8]            524,544\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-8                         [1, 256, 8, 8]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-9                       [1, 512, 4, 4]            2,097,664\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-10                        [1, 512, 4, 4]            --\n",
       "‚îú‚îÄLinear: 1-2                            [1, 256]                  2,097,408\n",
       "‚îú‚îÄLinear: 1-3                            [1, 256]                  2,097,408\n",
       "‚îú‚îÄSequential: 1-4                        [1, 8192]                 --\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-11                      [1, 8192]                 2,105,344\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-12                        [1, 8192]                 --\n",
       "‚îú‚îÄSequential: 1-5                        [1, 3, 128, 128]          --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose2d: 2-13             [1, 256, 8, 8]            2,097,408\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-14                        [1, 256, 8, 8]            --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose2d: 2-15             [1, 128, 16, 16]          524,416\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-16                        [1, 128, 16, 16]          --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose2d: 2-17             [1, 64, 32, 32]           131,136\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-18                        [1, 64, 32, 32]           --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose2d: 2-19             [1, 32, 64, 64]           32,800\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-20                        [1, 32, 64, 64]           --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose2d: 2-21             [1, 3, 128, 128]          1,539\n",
       "‚îÇ    ‚îî‚îÄSigmoid: 2-22                     [1, 3, 128, 128]          --\n",
       "==========================================================================================\n",
       "Total params: 11,875,267\n",
       "Trainable params: 11,875,267\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 709.39\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 4.46\n",
       "Params size (MB): 47.50\n",
       "Estimated Total Size (MB): 52.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pƒôtla treningowa VAE"
   ],
   "id": "aff417ff6963f777"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T22:11:31.895788Z",
     "start_time": "2025-05-23T22:11:25.189823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# G≈Ç√≥wna pƒôtla\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss=model.train_vae(epoch=epoch,dataloader=train_loader)\n",
    "\n",
    "\n",
    "    model.visualize_reconstruction(epoch=epoch,dataloader=train_loader)\n",
    "    torch.save(model.state_dict(), f'{result_dir}/{name}_best.pth')\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}')\n",
    "    "
   ],
   "id": "5cf3ac85ec700e8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Loss: 0.0372\n",
      "Epoch 1: Train Loss = 0.0372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# G≈Ç√≥wna pƒôtla\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, EPOCHS \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m----> 4\u001B[0m     train_loss\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_vae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     model\u001B[38;5;241m.\u001B[39mvisualize_reconstruction(epoch\u001B[38;5;241m=\u001B[39mepoch,dataloader\u001B[38;5;241m=\u001B[39mtrain_loader)\n\u001B[1;32m      8\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_best.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/CNNVAE.py:127\u001B[0m, in \u001B[0;36mCNNVAE.train_vae\u001B[0;34m(self, epoch, dataloader)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    126\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m    128\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[0;32mIn[60], line 10\u001B[0m, in \u001B[0;36mCapsuleNegativeDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 10\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_paths\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[1;32m     12\u001B[0m         image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(image)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/PIL/Image.py:922\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mconvert\u001B[39m(\n\u001B[1;32m    875\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m    876\u001B[0m ):\n\u001B[1;32m    877\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 922\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    924\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    926\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:291\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    290\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[0;32m--> 291\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GAN/DCGAN\n"
   ],
   "id": "a47f7bb4a4abea06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "4409c3b41585d707"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:00:26.680712Z",
     "start_time": "2025-05-17T18:00:26.381887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "gan=GAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/GAN\",load_pretrained=True)\n",
    "\n",
    "#gan=CNNGAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/CNNGAN\",load_pretrained=True)\n"
   ],
   "id": "b5afe5d6e075f27f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No generator checkpoint found.\n",
      "[INFO] No discriminator checkpoint found.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pƒôtla g≈Ç√≥wna treningu GAN/CNNGAN"
   ],
   "id": "948724baeb4e984b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:23:25.578736Z",
     "start_time": "2025-05-17T18:00:44.377503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "logger = GANLogger(save_dir=\"results/GAN\", filename=\"gan_metrics\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    g_loss, d_loss = gan.train_gan(epoch, dataloader=train_loader)\n",
    "\n",
    "    # Wizualizacja rekonstrukcji\n",
    "    gan.visualize_reconstruction(epoch)\n",
    "\n",
    "\n",
    "    gan.save_generator()\n",
    "    gan.save_discriminator()\n",
    "\n",
    "    print(f'Epoch {epoch}: Generator Loss = {g_loss:.4f}, Discriminator Loss = {d_loss:.4f} \\n')\n",
    "\n",
    "logger.save_json()"
   ],
   "id": "9cab0271a1e48783",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Generator Loss = 9.7600, Discriminator Loss = 6.0208\n",
      "Epoch 1: Validation Loss = 2.9360\n",
      "Epoch 1: Validation Loss = 2.9360 Generator Loss = 9.7600, Discriminator Loss = 6.0208 \n",
      " (Best G: 9.7600 Best D: 6.0208) \n",
      "Epoch 2: Generator Loss = 10.8798, Discriminator Loss = 4.6944\n",
      "Epoch 2: Validation Loss = 3.9196\n",
      "Epoch 2: Validation Loss = 3.9196 Generator Loss = 10.8798, Discriminator Loss = 4.6944 \n",
      " (Best G: 9.7600 Best D: 4.6944) \n",
      "Epoch 3: Generator Loss = 6.0911, Discriminator Loss = 7.9562\n",
      "Epoch 3: Validation Loss = 4.2210\n",
      "Epoch 3: Validation Loss = 4.2210 Generator Loss = 6.0911, Discriminator Loss = 7.9562 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 4: Generator Loss = 8.4777, Discriminator Loss = 6.2495\n",
      "Epoch 4: Validation Loss = 3.9419\n",
      "Epoch 4: Validation Loss = 3.9419 Generator Loss = 8.4777, Discriminator Loss = 6.2495 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 5: Generator Loss = 6.8712, Discriminator Loss = 7.2053\n",
      "Epoch 5: Validation Loss = 6.1037\n",
      "Epoch 5: Validation Loss = 6.1037 Generator Loss = 6.8712, Discriminator Loss = 7.2053 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 6: Generator Loss = 7.1771, Discriminator Loss = 7.8558\n",
      "Epoch 6: Validation Loss = 5.6923\n",
      "Epoch 6: Validation Loss = 5.6923 Generator Loss = 7.1771, Discriminator Loss = 7.8558 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 7: Generator Loss = 6.7497, Discriminator Loss = 8.0135\n",
      "Epoch 7: Validation Loss = 7.5345\n",
      "Epoch 7: Validation Loss = 7.5345 Generator Loss = 6.7497, Discriminator Loss = 8.0135 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 8: Generator Loss = 6.5106, Discriminator Loss = 8.9580\n",
      "Epoch 8: Validation Loss = 5.7820\n",
      "Epoch 8: Validation Loss = 5.7820 Generator Loss = 6.5106, Discriminator Loss = 8.9580 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 9: Generator Loss = 6.0248, Discriminator Loss = 8.9274\n",
      "Epoch 9: Validation Loss = 7.4620\n",
      "Epoch 9: Validation Loss = 7.4620 Generator Loss = 6.0248, Discriminator Loss = 8.9274 \n",
      " (Best G: 6.0248 Best D: 4.6944) \n",
      "Epoch 10: Generator Loss = 5.7788, Discriminator Loss = 9.8934\n",
      "Epoch 10: Validation Loss = 6.7545\n",
      "Epoch 10: Validation Loss = 6.7545 Generator Loss = 5.7788, Discriminator Loss = 9.8934 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 11: Generator Loss = 6.4598, Discriminator Loss = 8.8831\n",
      "Epoch 11: Validation Loss = 5.3651\n",
      "Epoch 11: Validation Loss = 5.3651 Generator Loss = 6.4598, Discriminator Loss = 8.8831 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 12: Generator Loss = 7.8274, Discriminator Loss = 8.3136\n",
      "Epoch 12: Validation Loss = 4.8210\n",
      "Epoch 12: Validation Loss = 4.8210 Generator Loss = 7.8274, Discriminator Loss = 8.3136 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 13: Generator Loss = 7.2280, Discriminator Loss = 7.5629\n",
      "Epoch 13: Validation Loss = 5.3835\n",
      "Epoch 13: Validation Loss = 5.3835 Generator Loss = 7.2280, Discriminator Loss = 7.5629 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 14: Generator Loss = 7.2760, Discriminator Loss = 9.3887\n",
      "Epoch 14: Validation Loss = 6.2387\n",
      "Epoch 14: Validation Loss = 6.2387 Generator Loss = 7.2760, Discriminator Loss = 9.3887 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 15: Generator Loss = 6.0328, Discriminator Loss = 9.4507\n",
      "Epoch 15: Validation Loss = 6.8202\n",
      "Epoch 15: Validation Loss = 6.8202 Generator Loss = 6.0328, Discriminator Loss = 9.4507 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 16: Generator Loss = 8.4924, Discriminator Loss = 8.6682\n",
      "Epoch 16: Validation Loss = 5.6520\n",
      "Epoch 16: Validation Loss = 5.6520 Generator Loss = 8.4924, Discriminator Loss = 8.6682 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 17: Generator Loss = 6.8802, Discriminator Loss = 9.2018\n",
      "Epoch 17: Validation Loss = 5.8690\n",
      "Epoch 17: Validation Loss = 5.8690 Generator Loss = 6.8802, Discriminator Loss = 9.2018 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 18: Generator Loss = 8.2514, Discriminator Loss = 11.6568\n",
      "Epoch 18: Validation Loss = 9.7483\n",
      "Epoch 18: Validation Loss = 9.7483 Generator Loss = 8.2514, Discriminator Loss = 11.6568 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 19: Generator Loss = 16.3807, Discriminator Loss = 18.7847\n",
      "Epoch 19: Validation Loss = 11.4975\n",
      "Epoch 19: Validation Loss = 11.4975 Generator Loss = 16.3807, Discriminator Loss = 18.7847 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 20: Generator Loss = 5.0693, Discriminator Loss = 14.1214\n",
      "Epoch 20: Validation Loss = 5.0641\n",
      "Epoch 20: Validation Loss = 5.0641 Generator Loss = 5.0693, Discriminator Loss = 14.1214 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 21: Generator Loss = 7.4322, Discriminator Loss = 7.7999\n",
      "Epoch 21: Validation Loss = 6.2296\n",
      "Epoch 21: Validation Loss = 6.2296 Generator Loss = 7.4322, Discriminator Loss = 7.7999 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 22: Generator Loss = 7.7827, Discriminator Loss = 12.2524\n",
      "Epoch 22: Validation Loss = 8.1272\n",
      "Epoch 22: Validation Loss = 8.1272 Generator Loss = 7.7827, Discriminator Loss = 12.2524 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 23: Generator Loss = 5.9959, Discriminator Loss = 10.9871\n",
      "Epoch 23: Validation Loss = 5.7139\n",
      "Epoch 23: Validation Loss = 5.7139 Generator Loss = 5.9959, Discriminator Loss = 10.9871 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 24: Generator Loss = 7.7428, Discriminator Loss = 8.8565\n",
      "Epoch 24: Validation Loss = 5.7387\n",
      "Epoch 24: Validation Loss = 5.7387 Generator Loss = 7.7428, Discriminator Loss = 8.8565 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 25: Generator Loss = 7.0892, Discriminator Loss = 8.8652\n",
      "Epoch 25: Validation Loss = 6.3497\n",
      "Epoch 25: Validation Loss = 6.3497 Generator Loss = 7.0892, Discriminator Loss = 8.8652 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 26: Generator Loss = 8.5085, Discriminator Loss = 9.6297\n",
      "Epoch 26: Validation Loss = 6.1187\n",
      "Epoch 26: Validation Loss = 6.1187 Generator Loss = 8.5085, Discriminator Loss = 9.6297 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 27: Generator Loss = 7.4014, Discriminator Loss = 9.4336\n",
      "Epoch 27: Validation Loss = 5.9016\n",
      "Epoch 27: Validation Loss = 5.9016 Generator Loss = 7.4014, Discriminator Loss = 9.4336 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 28: Generator Loss = 8.7855, Discriminator Loss = 9.2025\n",
      "Epoch 28: Validation Loss = 6.0268\n",
      "Epoch 28: Validation Loss = 6.0268 Generator Loss = 8.7855, Discriminator Loss = 9.2025 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 29: Generator Loss = 8.0426, Discriminator Loss = 9.4309\n",
      "Epoch 29: Validation Loss = 6.1933\n",
      "Epoch 29: Validation Loss = 6.1933 Generator Loss = 8.0426, Discriminator Loss = 9.4309 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 30: Generator Loss = 7.5750, Discriminator Loss = 9.7880\n",
      "Epoch 30: Validation Loss = 7.0656\n",
      "Epoch 30: Validation Loss = 7.0656 Generator Loss = 7.5750, Discriminator Loss = 9.7880 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 31: Generator Loss = 10.0115, Discriminator Loss = 11.5336\n",
      "Epoch 31: Validation Loss = 6.5779\n",
      "Epoch 31: Validation Loss = 6.5779 Generator Loss = 10.0115, Discriminator Loss = 11.5336 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 32: Generator Loss = 11.3884, Discriminator Loss = 11.3615\n",
      "Epoch 32: Validation Loss = 5.4061\n",
      "Epoch 32: Validation Loss = 5.4061 Generator Loss = 11.3884, Discriminator Loss = 11.3615 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 33: Generator Loss = 11.1881, Discriminator Loss = 12.0632\n",
      "Epoch 33: Validation Loss = 6.4756\n",
      "Epoch 33: Validation Loss = 6.4756 Generator Loss = 11.1881, Discriminator Loss = 12.0632 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 34: Generator Loss = 8.0954, Discriminator Loss = 10.1929\n",
      "Epoch 34: Validation Loss = 5.6536\n",
      "Epoch 34: Validation Loss = 5.6536 Generator Loss = 8.0954, Discriminator Loss = 10.1929 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 35: Generator Loss = 9.0739, Discriminator Loss = 11.5730\n",
      "Epoch 35: Validation Loss = 6.3876\n",
      "Epoch 35: Validation Loss = 6.3876 Generator Loss = 9.0739, Discriminator Loss = 11.5730 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 36: Generator Loss = 8.2376, Discriminator Loss = 9.5577\n",
      "Epoch 36: Validation Loss = 5.8133\n",
      "Epoch 36: Validation Loss = 5.8133 Generator Loss = 8.2376, Discriminator Loss = 9.5577 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 37: Generator Loss = 8.7708, Discriminator Loss = 12.8774\n",
      "Epoch 37: Validation Loss = 6.6520\n",
      "Epoch 37: Validation Loss = 6.6520 Generator Loss = 8.7708, Discriminator Loss = 12.8774 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 38: Generator Loss = 7.8435, Discriminator Loss = 9.3010\n",
      "Epoch 38: Validation Loss = 5.6867\n",
      "Epoch 38: Validation Loss = 5.6867 Generator Loss = 7.8435, Discriminator Loss = 9.3010 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 39: Generator Loss = 8.3640, Discriminator Loss = 9.1453\n",
      "Epoch 39: Validation Loss = 6.8827\n",
      "Epoch 39: Validation Loss = 6.8827 Generator Loss = 8.3640, Discriminator Loss = 9.1453 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 40: Generator Loss = 10.6794, Discriminator Loss = 11.5714\n",
      "Epoch 40: Validation Loss = 6.2367\n",
      "Epoch 40: Validation Loss = 6.2367 Generator Loss = 10.6794, Discriminator Loss = 11.5714 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 41: Generator Loss = 8.8970, Discriminator Loss = 10.5110\n",
      "Epoch 41: Validation Loss = 5.8545\n",
      "Epoch 41: Validation Loss = 5.8545 Generator Loss = 8.8970, Discriminator Loss = 10.5110 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 42: Generator Loss = 7.8549, Discriminator Loss = 9.3911\n",
      "Epoch 42: Validation Loss = 6.5189\n",
      "Epoch 42: Validation Loss = 6.5189 Generator Loss = 7.8549, Discriminator Loss = 9.3911 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 43: Generator Loss = 9.3701, Discriminator Loss = 11.2540\n",
      "Epoch 43: Validation Loss = 7.2181\n",
      "Epoch 43: Validation Loss = 7.2181 Generator Loss = 9.3701, Discriminator Loss = 11.2540 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 44: Generator Loss = 10.4711, Discriminator Loss = 9.6586\n",
      "Epoch 44: Validation Loss = 5.6944\n",
      "Epoch 44: Validation Loss = 5.6944 Generator Loss = 10.4711, Discriminator Loss = 9.6586 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 45: Generator Loss = 8.8965, Discriminator Loss = 9.5122\n",
      "Epoch 45: Validation Loss = 6.9263\n",
      "Epoch 45: Validation Loss = 6.9263 Generator Loss = 8.8965, Discriminator Loss = 9.5122 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 46: Generator Loss = 8.5896, Discriminator Loss = 10.1490\n",
      "Epoch 46: Validation Loss = 5.6369\n",
      "Epoch 46: Validation Loss = 5.6369 Generator Loss = 8.5896, Discriminator Loss = 10.1490 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 47: Generator Loss = 12.7093, Discriminator Loss = 15.4660\n",
      "Epoch 47: Validation Loss = 8.1884\n",
      "Epoch 47: Validation Loss = 8.1884 Generator Loss = 12.7093, Discriminator Loss = 15.4660 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 48: Generator Loss = 8.9752, Discriminator Loss = 15.4798\n",
      "Epoch 48: Validation Loss = 7.3785\n",
      "Epoch 48: Validation Loss = 7.3785 Generator Loss = 8.9752, Discriminator Loss = 15.4798 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 49: Generator Loss = 7.1422, Discriminator Loss = 10.7152\n",
      "Epoch 49: Validation Loss = 6.4174\n",
      "Epoch 49: Validation Loss = 6.4174 Generator Loss = 7.1422, Discriminator Loss = 10.7152 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 50: Generator Loss = 10.7533, Discriminator Loss = 10.5298\n",
      "Epoch 50: Validation Loss = 6.4444\n",
      "Epoch 50: Validation Loss = 6.4444 Generator Loss = 10.7533, Discriminator Loss = 10.5298 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 51: Generator Loss = 7.3053, Discriminator Loss = 9.9422\n",
      "Epoch 51: Validation Loss = 6.5720\n",
      "Epoch 51: Validation Loss = 6.5720 Generator Loss = 7.3053, Discriminator Loss = 9.9422 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 52: Generator Loss = 10.0106, Discriminator Loss = 10.0316\n",
      "Epoch 52: Validation Loss = 6.2146\n",
      "Epoch 52: Validation Loss = 6.2146 Generator Loss = 10.0106, Discriminator Loss = 10.0316 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 53: Generator Loss = 9.3780, Discriminator Loss = 9.5859\n",
      "Epoch 53: Validation Loss = 6.3818\n",
      "Epoch 53: Validation Loss = 6.3818 Generator Loss = 9.3780, Discriminator Loss = 9.5859 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 54: Generator Loss = 7.6329, Discriminator Loss = 10.3953\n",
      "Epoch 54: Validation Loss = 6.5021\n",
      "Epoch 54: Validation Loss = 6.5021 Generator Loss = 7.6329, Discriminator Loss = 10.3953 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 55: Generator Loss = 9.4691, Discriminator Loss = 9.9421\n",
      "Epoch 55: Validation Loss = 6.2121\n",
      "Epoch 55: Validation Loss = 6.2121 Generator Loss = 9.4691, Discriminator Loss = 9.9421 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 56: Generator Loss = 9.8803, Discriminator Loss = 9.3385\n",
      "Epoch 56: Validation Loss = 5.8402\n",
      "Epoch 56: Validation Loss = 5.8402 Generator Loss = 9.8803, Discriminator Loss = 9.3385 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 57: Generator Loss = 10.1579, Discriminator Loss = 11.5265\n",
      "Epoch 57: Validation Loss = 7.2376\n",
      "Epoch 57: Validation Loss = 7.2376 Generator Loss = 10.1579, Discriminator Loss = 11.5265 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 58: Generator Loss = 7.9020, Discriminator Loss = 10.1574\n",
      "Epoch 58: Validation Loss = 6.2369\n",
      "Epoch 58: Validation Loss = 6.2369 Generator Loss = 7.9020, Discriminator Loss = 10.1574 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 59: Generator Loss = 9.1546, Discriminator Loss = 9.2143\n",
      "Epoch 59: Validation Loss = 5.7740\n",
      "Epoch 59: Validation Loss = 5.7740 Generator Loss = 9.1546, Discriminator Loss = 9.2143 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 60: Generator Loss = 9.5956, Discriminator Loss = 9.0083\n",
      "Epoch 60: Validation Loss = 6.2174\n",
      "Epoch 60: Validation Loss = 6.2174 Generator Loss = 9.5956, Discriminator Loss = 9.0083 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 61: Generator Loss = 9.2675, Discriminator Loss = 11.0052\n",
      "Epoch 61: Validation Loss = 6.9308\n",
      "Epoch 61: Validation Loss = 6.9308 Generator Loss = 9.2675, Discriminator Loss = 11.0052 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 62: Generator Loss = 10.3259, Discriminator Loss = 10.0751\n",
      "Epoch 62: Validation Loss = 5.7865\n",
      "Epoch 62: Validation Loss = 5.7865 Generator Loss = 10.3259, Discriminator Loss = 10.0751 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 63: Generator Loss = 11.1798, Discriminator Loss = 8.9983\n",
      "Epoch 63: Validation Loss = 6.3331\n",
      "Epoch 63: Validation Loss = 6.3331 Generator Loss = 11.1798, Discriminator Loss = 8.9983 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 64: Generator Loss = 8.9328, Discriminator Loss = 10.3830\n",
      "Epoch 64: Validation Loss = 6.5170\n",
      "Epoch 64: Validation Loss = 6.5170 Generator Loss = 8.9328, Discriminator Loss = 10.3830 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 65: Generator Loss = 9.3504, Discriminator Loss = 9.5454\n",
      "Epoch 65: Validation Loss = 5.9216\n",
      "Epoch 65: Validation Loss = 5.9216 Generator Loss = 9.3504, Discriminator Loss = 9.5454 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 66: Generator Loss = 14.7593, Discriminator Loss = 11.3602\n",
      "Epoch 66: Validation Loss = 9.3720\n",
      "Epoch 66: Validation Loss = 9.3720 Generator Loss = 14.7593, Discriminator Loss = 11.3602 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 67: Generator Loss = 7.7505, Discriminator Loss = 11.8777\n",
      "Epoch 67: Validation Loss = 7.1198\n",
      "Epoch 67: Validation Loss = 7.1198 Generator Loss = 7.7505, Discriminator Loss = 11.8777 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 68: Generator Loss = 10.2320, Discriminator Loss = 10.4063\n",
      "Epoch 68: Validation Loss = 6.1706\n",
      "Epoch 68: Validation Loss = 6.1706 Generator Loss = 10.2320, Discriminator Loss = 10.4063 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 69: Generator Loss = 10.9933, Discriminator Loss = 11.8952\n",
      "Epoch 69: Validation Loss = 5.9212\n",
      "Epoch 69: Validation Loss = 5.9212 Generator Loss = 10.9933, Discriminator Loss = 11.8952 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 70: Generator Loss = 9.7209, Discriminator Loss = 9.0801\n",
      "Epoch 70: Validation Loss = 6.5784\n",
      "Epoch 70: Validation Loss = 6.5784 Generator Loss = 9.7209, Discriminator Loss = 9.0801 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 71: Generator Loss = 8.4650, Discriminator Loss = 11.5433\n",
      "Epoch 71: Validation Loss = 6.0600\n",
      "Epoch 71: Validation Loss = 6.0600 Generator Loss = 8.4650, Discriminator Loss = 11.5433 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 72: Generator Loss = 9.3177, Discriminator Loss = 9.3518\n",
      "Epoch 72: Validation Loss = 6.1252\n",
      "Epoch 72: Validation Loss = 6.1252 Generator Loss = 9.3177, Discriminator Loss = 9.3518 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 73: Generator Loss = 8.8661, Discriminator Loss = 9.7675\n",
      "Epoch 73: Validation Loss = 6.3743\n",
      "Epoch 73: Validation Loss = 6.3743 Generator Loss = 8.8661, Discriminator Loss = 9.7675 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 74: Generator Loss = 9.3710, Discriminator Loss = 9.6935\n",
      "Epoch 74: Validation Loss = 6.1652\n",
      "Epoch 74: Validation Loss = 6.1652 Generator Loss = 9.3710, Discriminator Loss = 9.6935 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 75: Generator Loss = 8.8199, Discriminator Loss = 9.5980\n",
      "Epoch 75: Validation Loss = 7.0621\n",
      "Epoch 75: Validation Loss = 7.0621 Generator Loss = 8.8199, Discriminator Loss = 9.5980 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 76: Generator Loss = 11.2167, Discriminator Loss = 10.4470\n",
      "Epoch 76: Validation Loss = 6.2545\n",
      "Epoch 76: Validation Loss = 6.2545 Generator Loss = 11.2167, Discriminator Loss = 10.4470 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 77: Generator Loss = 9.0543, Discriminator Loss = 9.4446\n",
      "Epoch 77: Validation Loss = 6.2311\n",
      "Epoch 77: Validation Loss = 6.2311 Generator Loss = 9.0543, Discriminator Loss = 9.4446 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 78: Generator Loss = 9.4611, Discriminator Loss = 9.5200\n",
      "Epoch 78: Validation Loss = 6.3124\n",
      "Epoch 78: Validation Loss = 6.3124 Generator Loss = 9.4611, Discriminator Loss = 9.5200 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 79: Generator Loss = 8.9467, Discriminator Loss = 9.7114\n",
      "Epoch 79: Validation Loss = 6.2820\n",
      "Epoch 79: Validation Loss = 6.2820 Generator Loss = 8.9467, Discriminator Loss = 9.7114 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 80: Generator Loss = 10.8387, Discriminator Loss = 9.5582\n",
      "Epoch 80: Validation Loss = 5.9377\n",
      "Epoch 80: Validation Loss = 5.9377 Generator Loss = 10.8387, Discriminator Loss = 9.5582 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 81: Generator Loss = 9.6686, Discriminator Loss = 9.3759\n",
      "Epoch 81: Validation Loss = 6.3553\n",
      "Epoch 81: Validation Loss = 6.3553 Generator Loss = 9.6686, Discriminator Loss = 9.3759 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 82: Generator Loss = 9.9403, Discriminator Loss = 10.1423\n",
      "Epoch 82: Validation Loss = 6.3655\n",
      "Epoch 82: Validation Loss = 6.3655 Generator Loss = 9.9403, Discriminator Loss = 10.1423 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 83: Generator Loss = 16.0487, Discriminator Loss = 13.0372\n",
      "Epoch 83: Validation Loss = 10.3665\n",
      "Epoch 83: Validation Loss = 10.3665 Generator Loss = 16.0487, Discriminator Loss = 13.0372 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 84: Generator Loss = 14.0520, Discriminator Loss = 14.4460\n",
      "Epoch 84: Validation Loss = 5.7804\n",
      "Epoch 84: Validation Loss = 5.7804 Generator Loss = 14.0520, Discriminator Loss = 14.4460 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 85: Generator Loss = 10.2647, Discriminator Loss = 11.6830\n",
      "Epoch 85: Validation Loss = 6.0652\n",
      "Epoch 85: Validation Loss = 6.0652 Generator Loss = 10.2647, Discriminator Loss = 11.6830 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 86: Generator Loss = 9.1737, Discriminator Loss = 9.8950\n",
      "Epoch 86: Validation Loss = 6.7488\n",
      "Epoch 86: Validation Loss = 6.7488 Generator Loss = 9.1737, Discriminator Loss = 9.8950 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 87: Generator Loss = 10.4040, Discriminator Loss = 10.1856\n",
      "Epoch 87: Validation Loss = 6.2716\n",
      "Epoch 87: Validation Loss = 6.2716 Generator Loss = 10.4040, Discriminator Loss = 10.1856 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 88: Generator Loss = 8.2801, Discriminator Loss = 9.7203\n",
      "Epoch 88: Validation Loss = 6.0276\n",
      "Epoch 88: Validation Loss = 6.0276 Generator Loss = 8.2801, Discriminator Loss = 9.7203 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 89: Generator Loss = 9.5269, Discriminator Loss = 9.3891\n",
      "Epoch 89: Validation Loss = 6.2236\n",
      "Epoch 89: Validation Loss = 6.2236 Generator Loss = 9.5269, Discriminator Loss = 9.3891 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 90: Generator Loss = 9.3459, Discriminator Loss = 10.7435\n",
      "Epoch 90: Validation Loss = 6.2344\n",
      "Epoch 90: Validation Loss = 6.2344 Generator Loss = 9.3459, Discriminator Loss = 10.7435 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 91: Generator Loss = 8.6239, Discriminator Loss = 9.4446\n",
      "Epoch 91: Validation Loss = 6.0278\n",
      "Epoch 91: Validation Loss = 6.0278 Generator Loss = 8.6239, Discriminator Loss = 9.4446 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 92: Generator Loss = 9.4070, Discriminator Loss = 9.3086\n",
      "Epoch 92: Validation Loss = 6.0108\n",
      "Epoch 92: Validation Loss = 6.0108 Generator Loss = 9.4070, Discriminator Loss = 9.3086 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 93: Generator Loss = 9.0641, Discriminator Loss = 9.3480\n",
      "Epoch 93: Validation Loss = 6.2258\n",
      "Epoch 93: Validation Loss = 6.2258 Generator Loss = 9.0641, Discriminator Loss = 9.3480 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 94: Generator Loss = 8.6298, Discriminator Loss = 9.6332\n",
      "Epoch 94: Validation Loss = 6.1580\n",
      "Epoch 94: Validation Loss = 6.1580 Generator Loss = 8.6298, Discriminator Loss = 9.6332 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 95: Generator Loss = 8.2834, Discriminator Loss = 9.8089\n",
      "Epoch 95: Validation Loss = 6.4274\n",
      "Epoch 95: Validation Loss = 6.4274 Generator Loss = 8.2834, Discriminator Loss = 9.8089 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 96: Generator Loss = 13.2943, Discriminator Loss = 10.9014\n",
      "Epoch 96: Validation Loss = 6.5657\n",
      "Epoch 96: Validation Loss = 6.5657 Generator Loss = 13.2943, Discriminator Loss = 10.9014 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 97: Generator Loss = 8.2452, Discriminator Loss = 10.3358\n",
      "Epoch 97: Validation Loss = 6.2864\n",
      "Epoch 97: Validation Loss = 6.2864 Generator Loss = 8.2452, Discriminator Loss = 10.3358 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 98: Generator Loss = 10.2484, Discriminator Loss = 10.1881\n",
      "Epoch 98: Validation Loss = 6.3180\n",
      "Epoch 98: Validation Loss = 6.3180 Generator Loss = 10.2484, Discriminator Loss = 10.1881 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 99: Generator Loss = 10.2433, Discriminator Loss = 9.6786\n",
      "Epoch 99: Validation Loss = 6.1502\n",
      "Epoch 99: Validation Loss = 6.1502 Generator Loss = 10.2433, Discriminator Loss = 9.6786 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 100: Generator Loss = 9.7001, Discriminator Loss = 9.7680\n",
      "Epoch 100: Validation Loss = 6.6652\n",
      "Epoch 100: Validation Loss = 6.6652 Generator Loss = 9.7001, Discriminator Loss = 9.7680 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 101: Generator Loss = 8.7411, Discriminator Loss = 9.9749\n",
      "Epoch 101: Validation Loss = 6.5329\n",
      "Epoch 101: Validation Loss = 6.5329 Generator Loss = 8.7411, Discriminator Loss = 9.9749 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 102: Generator Loss = 10.0488, Discriminator Loss = 9.5822\n",
      "Epoch 102: Validation Loss = 6.1691\n",
      "Epoch 102: Validation Loss = 6.1691 Generator Loss = 10.0488, Discriminator Loss = 9.5822 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 103: Generator Loss = 9.3257, Discriminator Loss = 9.3915\n",
      "Epoch 103: Validation Loss = 6.2021\n",
      "Epoch 103: Validation Loss = 6.2021 Generator Loss = 9.3257, Discriminator Loss = 9.3915 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 104: Generator Loss = 9.2542, Discriminator Loss = 9.6408\n",
      "Epoch 104: Validation Loss = 6.1798\n",
      "Epoch 104: Validation Loss = 6.1798 Generator Loss = 9.2542, Discriminator Loss = 9.6408 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 105: Generator Loss = 9.8067, Discriminator Loss = 9.4759\n",
      "Epoch 105: Validation Loss = 6.1515\n",
      "Epoch 105: Validation Loss = 6.1515 Generator Loss = 9.8067, Discriminator Loss = 9.4759 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 106: Generator Loss = 10.0596, Discriminator Loss = 9.6529\n",
      "Epoch 106: Validation Loss = 6.8081\n",
      "Epoch 106: Validation Loss = 6.8081 Generator Loss = 10.0596, Discriminator Loss = 9.6529 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 107: Generator Loss = 11.5908, Discriminator Loss = 10.3321\n",
      "Epoch 107: Validation Loss = 6.2512\n",
      "Epoch 107: Validation Loss = 6.2512 Generator Loss = 11.5908, Discriminator Loss = 10.3321 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 108: Generator Loss = 9.8209, Discriminator Loss = 9.4614\n",
      "Epoch 108: Validation Loss = 5.7649\n",
      "Epoch 108: Validation Loss = 5.7649 Generator Loss = 9.8209, Discriminator Loss = 9.4614 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 109: Generator Loss = 9.7632, Discriminator Loss = 9.2189\n",
      "Epoch 109: Validation Loss = 6.8193\n",
      "Epoch 109: Validation Loss = 6.8193 Generator Loss = 9.7632, Discriminator Loss = 9.2189 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 110: Generator Loss = 12.3400, Discriminator Loss = 11.2618\n",
      "Epoch 110: Validation Loss = 5.7349\n",
      "Epoch 110: Validation Loss = 5.7349 Generator Loss = 12.3400, Discriminator Loss = 11.2618 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 111: Generator Loss = 15.6670, Discriminator Loss = 14.1237\n",
      "Epoch 111: Validation Loss = 9.9103\n",
      "Epoch 111: Validation Loss = 9.9103 Generator Loss = 15.6670, Discriminator Loss = 14.1237 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 112: Generator Loss = 11.1372, Discriminator Loss = 13.6074\n",
      "Epoch 112: Validation Loss = 8.2132\n",
      "Epoch 112: Validation Loss = 8.2132 Generator Loss = 11.1372, Discriminator Loss = 13.6074 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 113: Generator Loss = 12.2133, Discriminator Loss = 11.7937\n",
      "Epoch 113: Validation Loss = 6.3614\n",
      "Epoch 113: Validation Loss = 6.3614 Generator Loss = 12.2133, Discriminator Loss = 11.7937 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 114: Generator Loss = 7.9501, Discriminator Loss = 9.1726\n",
      "Epoch 114: Validation Loss = 5.8597\n",
      "Epoch 114: Validation Loss = 5.8597 Generator Loss = 7.9501, Discriminator Loss = 9.1726 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 115: Generator Loss = 9.9093, Discriminator Loss = 9.3233\n",
      "Epoch 115: Validation Loss = 6.5729\n",
      "Epoch 115: Validation Loss = 6.5729 Generator Loss = 9.9093, Discriminator Loss = 9.3233 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 116: Generator Loss = 7.4362, Discriminator Loss = 9.7766\n",
      "Epoch 116: Validation Loss = 6.7968\n",
      "Epoch 116: Validation Loss = 6.7968 Generator Loss = 7.4362, Discriminator Loss = 9.7766 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 117: Generator Loss = 9.2078, Discriminator Loss = 10.0442\n",
      "Epoch 117: Validation Loss = 6.4666\n",
      "Epoch 117: Validation Loss = 6.4666 Generator Loss = 9.2078, Discriminator Loss = 10.0442 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 118: Generator Loss = 8.4336, Discriminator Loss = 9.6268\n",
      "Epoch 118: Validation Loss = 6.0250\n",
      "Epoch 118: Validation Loss = 6.0250 Generator Loss = 8.4336, Discriminator Loss = 9.6268 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 119: Generator Loss = 9.7813, Discriminator Loss = 9.8193\n",
      "Epoch 119: Validation Loss = 6.2971\n",
      "Epoch 119: Validation Loss = 6.2971 Generator Loss = 9.7813, Discriminator Loss = 9.8193 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 120: Generator Loss = 10.6430, Discriminator Loss = 9.7817\n",
      "Epoch 120: Validation Loss = 6.7054\n",
      "Epoch 120: Validation Loss = 6.7054 Generator Loss = 10.6430, Discriminator Loss = 9.7817 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 121: Generator Loss = 8.2107, Discriminator Loss = 10.1998\n",
      "Epoch 121: Validation Loss = 6.3430\n",
      "Epoch 121: Validation Loss = 6.3430 Generator Loss = 8.2107, Discriminator Loss = 10.1998 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 122: Generator Loss = 9.9017, Discriminator Loss = 9.5484\n",
      "Epoch 122: Validation Loss = 6.1098\n",
      "Epoch 122: Validation Loss = 6.1098 Generator Loss = 9.9017, Discriminator Loss = 9.5484 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 123: Generator Loss = 10.3975, Discriminator Loss = 9.2710\n",
      "Epoch 123: Validation Loss = 6.0967\n",
      "Epoch 123: Validation Loss = 6.0967 Generator Loss = 10.3975, Discriminator Loss = 9.2710 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 124: Generator Loss = 11.8087, Discriminator Loss = 9.9913\n",
      "Epoch 124: Validation Loss = 7.0337\n",
      "Epoch 124: Validation Loss = 7.0337 Generator Loss = 11.8087, Discriminator Loss = 9.9913 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 125: Generator Loss = 8.5186, Discriminator Loss = 10.4737\n",
      "Epoch 125: Validation Loss = 6.1299\n",
      "Epoch 125: Validation Loss = 6.1299 Generator Loss = 8.5186, Discriminator Loss = 10.4737 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 126: Generator Loss = 11.2619, Discriminator Loss = 10.3915\n",
      "Epoch 126: Validation Loss = 6.1468\n",
      "Epoch 126: Validation Loss = 6.1468 Generator Loss = 11.2619, Discriminator Loss = 10.3915 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 127: Generator Loss = 8.9310, Discriminator Loss = 9.4278\n",
      "Epoch 127: Validation Loss = 6.2639\n",
      "Epoch 127: Validation Loss = 6.2639 Generator Loss = 8.9310, Discriminator Loss = 9.4278 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 128: Generator Loss = 10.4690, Discriminator Loss = 9.5479\n",
      "Epoch 128: Validation Loss = 6.4648\n",
      "Epoch 128: Validation Loss = 6.4648 Generator Loss = 10.4690, Discriminator Loss = 9.5479 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 129: Generator Loss = 8.6170, Discriminator Loss = 9.9946\n",
      "Epoch 129: Validation Loss = 6.5845\n",
      "Epoch 129: Validation Loss = 6.5845 Generator Loss = 8.6170, Discriminator Loss = 9.9946 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 130: Generator Loss = 9.9907, Discriminator Loss = 9.7007\n",
      "Epoch 130: Validation Loss = 5.9405\n",
      "Epoch 130: Validation Loss = 5.9405 Generator Loss = 9.9907, Discriminator Loss = 9.7007 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 131: Generator Loss = 10.8045, Discriminator Loss = 8.9507\n",
      "Epoch 131: Validation Loss = 5.8456\n",
      "Epoch 131: Validation Loss = 5.8456 Generator Loss = 10.8045, Discriminator Loss = 8.9507 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 132: Generator Loss = 10.0250, Discriminator Loss = 10.2510\n",
      "Epoch 132: Validation Loss = 10.1834\n",
      "Epoch 132: Validation Loss = 10.1834 Generator Loss = 10.0250, Discriminator Loss = 10.2510 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 133: Generator Loss = 10.7655, Discriminator Loss = 11.3205\n",
      "Epoch 133: Validation Loss = 5.7610\n",
      "Epoch 133: Validation Loss = 5.7610 Generator Loss = 10.7655, Discriminator Loss = 11.3205 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 134: Generator Loss = 10.9277, Discriminator Loss = 8.5951\n",
      "Epoch 134: Validation Loss = 5.5327\n",
      "Epoch 134: Validation Loss = 5.5327 Generator Loss = 10.9277, Discriminator Loss = 8.5951 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 135: Generator Loss = 10.5526, Discriminator Loss = 9.6707\n",
      "Epoch 135: Validation Loss = 7.3366\n",
      "Epoch 135: Validation Loss = 7.3366 Generator Loss = 10.5526, Discriminator Loss = 9.6707 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 136: Generator Loss = 8.4051, Discriminator Loss = 10.1013\n",
      "Epoch 136: Validation Loss = 6.1219\n",
      "Epoch 136: Validation Loss = 6.1219 Generator Loss = 8.4051, Discriminator Loss = 10.1013 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 137: Generator Loss = 9.5574, Discriminator Loss = 9.2914\n",
      "Epoch 137: Validation Loss = 6.3133\n",
      "Epoch 137: Validation Loss = 6.3133 Generator Loss = 9.5574, Discriminator Loss = 9.2914 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 138: Generator Loss = 11.4577, Discriminator Loss = 10.1307\n",
      "Epoch 138: Validation Loss = 6.5769\n",
      "Epoch 138: Validation Loss = 6.5769 Generator Loss = 11.4577, Discriminator Loss = 10.1307 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 139: Generator Loss = 10.8101, Discriminator Loss = 10.2575\n",
      "Epoch 139: Validation Loss = 6.0217\n",
      "Epoch 139: Validation Loss = 6.0217 Generator Loss = 10.8101, Discriminator Loss = 10.2575 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 140: Generator Loss = 10.8147, Discriminator Loss = 10.2944\n",
      "Epoch 140: Validation Loss = 7.4095\n",
      "Epoch 140: Validation Loss = 7.4095 Generator Loss = 10.8147, Discriminator Loss = 10.2944 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 141: Generator Loss = 8.0927, Discriminator Loss = 9.9357\n",
      "Epoch 141: Validation Loss = 6.1744\n",
      "Epoch 141: Validation Loss = 6.1744 Generator Loss = 8.0927, Discriminator Loss = 9.9357 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 142: Generator Loss = 10.2972, Discriminator Loss = 9.4293\n",
      "Epoch 142: Validation Loss = 6.3415\n",
      "Epoch 142: Validation Loss = 6.3415 Generator Loss = 10.2972, Discriminator Loss = 9.4293 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 143: Generator Loss = 9.3270, Discriminator Loss = 10.4330\n",
      "Epoch 143: Validation Loss = 6.6687\n",
      "Epoch 143: Validation Loss = 6.6687 Generator Loss = 9.3270, Discriminator Loss = 10.4330 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 144: Generator Loss = 9.1795, Discriminator Loss = 9.6094\n",
      "Epoch 144: Validation Loss = 5.9423\n",
      "Epoch 144: Validation Loss = 5.9423 Generator Loss = 9.1795, Discriminator Loss = 9.6094 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 145: Generator Loss = 10.6670, Discriminator Loss = 9.1767\n",
      "Epoch 145: Validation Loss = 6.4839\n",
      "Epoch 145: Validation Loss = 6.4839 Generator Loss = 10.6670, Discriminator Loss = 9.1767 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 146: Generator Loss = 11.0873, Discriminator Loss = 10.6987\n",
      "Epoch 146: Validation Loss = 6.5213\n",
      "Epoch 146: Validation Loss = 6.5213 Generator Loss = 11.0873, Discriminator Loss = 10.6987 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 147: Generator Loss = 11.2075, Discriminator Loss = 9.2852\n",
      "Epoch 147: Validation Loss = 5.8315\n",
      "Epoch 147: Validation Loss = 5.8315 Generator Loss = 11.2075, Discriminator Loss = 9.2852 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 148: Generator Loss = 11.1394, Discriminator Loss = 13.1504\n",
      "Epoch 148: Validation Loss = 6.2863\n",
      "Epoch 148: Validation Loss = 6.2863 Generator Loss = 11.1394, Discriminator Loss = 13.1504 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 149: Generator Loss = 10.8798, Discriminator Loss = 9.4590\n",
      "Epoch 149: Validation Loss = 5.9317\n",
      "Epoch 149: Validation Loss = 5.9317 Generator Loss = 10.8798, Discriminator Loss = 9.4590 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 150: Generator Loss = 9.1404, Discriminator Loss = 10.4045\n",
      "Epoch 150: Validation Loss = 6.1147\n",
      "Epoch 150: Validation Loss = 6.1147 Generator Loss = 9.1404, Discriminator Loss = 10.4045 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 151: Generator Loss = 9.0436, Discriminator Loss = 9.3061\n",
      "Epoch 151: Validation Loss = 6.0542\n",
      "Epoch 151: Validation Loss = 6.0542 Generator Loss = 9.0436, Discriminator Loss = 9.3061 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 152: Generator Loss = 9.3012, Discriminator Loss = 9.4501\n",
      "Epoch 152: Validation Loss = 6.1750\n",
      "Epoch 152: Validation Loss = 6.1750 Generator Loss = 9.3012, Discriminator Loss = 9.4501 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 153: Generator Loss = 8.9969, Discriminator Loss = 9.5943\n",
      "Epoch 153: Validation Loss = 6.3832\n",
      "Epoch 153: Validation Loss = 6.3832 Generator Loss = 8.9969, Discriminator Loss = 9.5943 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 154: Generator Loss = 10.1205, Discriminator Loss = 9.8518\n",
      "Epoch 154: Validation Loss = 6.3450\n",
      "Epoch 154: Validation Loss = 6.3450 Generator Loss = 10.1205, Discriminator Loss = 9.8518 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 155: Generator Loss = 9.6942, Discriminator Loss = 9.5932\n",
      "Epoch 155: Validation Loss = 6.1027\n",
      "Epoch 155: Validation Loss = 6.1027 Generator Loss = 9.6942, Discriminator Loss = 9.5932 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 156: Generator Loss = 10.3377, Discriminator Loss = 9.6126\n",
      "Epoch 156: Validation Loss = 6.2477\n",
      "Epoch 156: Validation Loss = 6.2477 Generator Loss = 10.3377, Discriminator Loss = 9.6126 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 157: Generator Loss = 9.7786, Discriminator Loss = 9.8153\n",
      "Epoch 157: Validation Loss = 6.3621\n",
      "Epoch 157: Validation Loss = 6.3621 Generator Loss = 9.7786, Discriminator Loss = 9.8153 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 158: Generator Loss = 9.1373, Discriminator Loss = 9.6456\n",
      "Epoch 158: Validation Loss = 6.2196\n",
      "Epoch 158: Validation Loss = 6.2196 Generator Loss = 9.1373, Discriminator Loss = 9.6456 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 159: Generator Loss = 10.8128, Discriminator Loss = 9.9128\n",
      "Epoch 159: Validation Loss = 6.4683\n",
      "Epoch 159: Validation Loss = 6.4683 Generator Loss = 10.8128, Discriminator Loss = 9.9128 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 160: Generator Loss = 10.1491, Discriminator Loss = 9.8192\n",
      "Epoch 160: Validation Loss = 6.4310\n",
      "Epoch 160: Validation Loss = 6.4310 Generator Loss = 10.1491, Discriminator Loss = 9.8192 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 161: Generator Loss = 8.9461, Discriminator Loss = 9.6579\n",
      "Epoch 161: Validation Loss = 6.3836\n",
      "Epoch 161: Validation Loss = 6.3836 Generator Loss = 8.9461, Discriminator Loss = 9.6579 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 162: Generator Loss = 10.7682, Discriminator Loss = 9.4581\n",
      "Epoch 162: Validation Loss = 6.3200\n",
      "Epoch 162: Validation Loss = 6.3200 Generator Loss = 10.7682, Discriminator Loss = 9.4581 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 163: Generator Loss = 12.0959, Discriminator Loss = 10.7516\n",
      "Epoch 163: Validation Loss = 6.5124\n",
      "Epoch 163: Validation Loss = 6.5124 Generator Loss = 12.0959, Discriminator Loss = 10.7516 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 164: Generator Loss = 9.7345, Discriminator Loss = 9.5959\n",
      "Epoch 164: Validation Loss = 6.1834\n",
      "Epoch 164: Validation Loss = 6.1834 Generator Loss = 9.7345, Discriminator Loss = 9.5959 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 165: Generator Loss = 10.0380, Discriminator Loss = 9.4414\n",
      "Epoch 165: Validation Loss = 6.0888\n",
      "Epoch 165: Validation Loss = 6.0888 Generator Loss = 10.0380, Discriminator Loss = 9.4414 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 166: Generator Loss = 10.7815, Discriminator Loss = 10.1436\n",
      "Epoch 166: Validation Loss = 6.5165\n",
      "Epoch 166: Validation Loss = 6.5165 Generator Loss = 10.7815, Discriminator Loss = 10.1436 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 167: Generator Loss = 12.5655, Discriminator Loss = 9.8232\n",
      "Epoch 167: Validation Loss = 7.1153\n",
      "Epoch 167: Validation Loss = 7.1153 Generator Loss = 12.5655, Discriminator Loss = 9.8232 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 168: Generator Loss = 8.6366, Discriminator Loss = 10.1269\n",
      "Epoch 168: Validation Loss = 6.2398\n",
      "Epoch 168: Validation Loss = 6.2398 Generator Loss = 8.6366, Discriminator Loss = 10.1269 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 169: Generator Loss = 9.4335, Discriminator Loss = 9.5549\n",
      "Epoch 169: Validation Loss = 6.1671\n",
      "Epoch 169: Validation Loss = 6.1671 Generator Loss = 9.4335, Discriminator Loss = 9.5549 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 170: Generator Loss = 10.0514, Discriminator Loss = 9.5950\n",
      "Epoch 170: Validation Loss = 6.4345\n",
      "Epoch 170: Validation Loss = 6.4345 Generator Loss = 10.0514, Discriminator Loss = 9.5950 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 171: Generator Loss = 9.6900, Discriminator Loss = 9.7149\n",
      "Epoch 171: Validation Loss = 6.0834\n",
      "Epoch 171: Validation Loss = 6.0834 Generator Loss = 9.6900, Discriminator Loss = 9.7149 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 172: Generator Loss = 9.4442, Discriminator Loss = 9.5667\n",
      "Epoch 172: Validation Loss = 6.0875\n",
      "Epoch 172: Validation Loss = 6.0875 Generator Loss = 9.4442, Discriminator Loss = 9.5667 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 173: Generator Loss = 14.7770, Discriminator Loss = 12.7080\n",
      "Epoch 173: Validation Loss = 7.1295\n",
      "Epoch 173: Validation Loss = 7.1295 Generator Loss = 14.7770, Discriminator Loss = 12.7080 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 174: Generator Loss = 10.0413, Discriminator Loss = 9.8186\n",
      "Epoch 174: Validation Loss = 6.2204\n",
      "Epoch 174: Validation Loss = 6.2204 Generator Loss = 10.0413, Discriminator Loss = 9.8186 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 175: Generator Loss = 8.7834, Discriminator Loss = 9.4664\n",
      "Epoch 175: Validation Loss = 6.3724\n",
      "Epoch 175: Validation Loss = 6.3724 Generator Loss = 8.7834, Discriminator Loss = 9.4664 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 176: Generator Loss = 10.1831, Discriminator Loss = 9.7447\n",
      "Epoch 176: Validation Loss = 6.1520\n",
      "Epoch 176: Validation Loss = 6.1520 Generator Loss = 10.1831, Discriminator Loss = 9.7447 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 177: Generator Loss = 9.0680, Discriminator Loss = 9.5049\n",
      "Epoch 177: Validation Loss = 6.3486\n",
      "Epoch 177: Validation Loss = 6.3486 Generator Loss = 9.0680, Discriminator Loss = 9.5049 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 178: Generator Loss = 10.1333, Discriminator Loss = 9.8160\n",
      "Epoch 178: Validation Loss = 6.4267\n",
      "Epoch 178: Validation Loss = 6.4267 Generator Loss = 10.1333, Discriminator Loss = 9.8160 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 179: Generator Loss = 9.3744, Discriminator Loss = 9.7814\n",
      "Epoch 179: Validation Loss = 6.1072\n",
      "Epoch 179: Validation Loss = 6.1072 Generator Loss = 9.3744, Discriminator Loss = 9.7814 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 180: Generator Loss = 11.3370, Discriminator Loss = 9.4682\n",
      "Epoch 180: Validation Loss = 6.5468\n",
      "Epoch 180: Validation Loss = 6.5468 Generator Loss = 11.3370, Discriminator Loss = 9.4682 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 181: Generator Loss = 9.1478, Discriminator Loss = 10.3694\n",
      "Epoch 181: Validation Loss = 6.3396\n",
      "Epoch 181: Validation Loss = 6.3396 Generator Loss = 9.1478, Discriminator Loss = 10.3694 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 182: Generator Loss = 9.6399, Discriminator Loss = 9.6228\n",
      "Epoch 182: Validation Loss = 6.2631\n",
      "Epoch 182: Validation Loss = 6.2631 Generator Loss = 9.6399, Discriminator Loss = 9.6228 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 183: Generator Loss = 10.1513, Discriminator Loss = 9.5383\n",
      "Epoch 183: Validation Loss = 6.1944\n",
      "Epoch 183: Validation Loss = 6.1944 Generator Loss = 10.1513, Discriminator Loss = 9.5383 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 184: Generator Loss = 10.2322, Discriminator Loss = 9.7128\n",
      "Epoch 184: Validation Loss = 6.7000\n",
      "Epoch 184: Validation Loss = 6.7000 Generator Loss = 10.2322, Discriminator Loss = 9.7128 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 185: Generator Loss = 9.0918, Discriminator Loss = 10.0502\n",
      "Epoch 185: Validation Loss = 6.3128\n",
      "Epoch 185: Validation Loss = 6.3128 Generator Loss = 9.0918, Discriminator Loss = 10.0502 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 186: Generator Loss = 10.6563, Discriminator Loss = 9.3306\n",
      "Epoch 186: Validation Loss = 5.9799\n",
      "Epoch 186: Validation Loss = 5.9799 Generator Loss = 10.6563, Discriminator Loss = 9.3306 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 187: Generator Loss = 10.0056, Discriminator Loss = 9.4045\n",
      "Epoch 187: Validation Loss = 7.0075\n",
      "Epoch 187: Validation Loss = 7.0075 Generator Loss = 10.0056, Discriminator Loss = 9.4045 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 188: Generator Loss = 9.5662, Discriminator Loss = 10.6266\n",
      "Epoch 188: Validation Loss = 6.5032\n",
      "Epoch 188: Validation Loss = 6.5032 Generator Loss = 9.5662, Discriminator Loss = 10.6266 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 189: Generator Loss = 10.5281, Discriminator Loss = 9.2164\n",
      "Epoch 189: Validation Loss = 5.2050\n",
      "Epoch 189: Validation Loss = 5.2050 Generator Loss = 10.5281, Discriminator Loss = 9.2164 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 190: Generator Loss = 15.5943, Discriminator Loss = 11.5230\n",
      "Epoch 190: Validation Loss = 12.3135\n",
      "Epoch 190: Validation Loss = 12.3135 Generator Loss = 15.5943, Discriminator Loss = 11.5230 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 191: Generator Loss = 10.9681, Discriminator Loss = 11.7913\n",
      "Epoch 191: Validation Loss = 6.1051\n",
      "Epoch 191: Validation Loss = 6.1051 Generator Loss = 10.9681, Discriminator Loss = 11.7913 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 192: Generator Loss = 8.9707, Discriminator Loss = 9.7550\n",
      "Epoch 192: Validation Loss = 5.9182\n",
      "Epoch 192: Validation Loss = 5.9182 Generator Loss = 8.9707, Discriminator Loss = 9.7550 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 193: Generator Loss = 9.2701, Discriminator Loss = 9.2678\n",
      "Epoch 193: Validation Loss = 6.3249\n",
      "Epoch 193: Validation Loss = 6.3249 Generator Loss = 9.2701, Discriminator Loss = 9.2678 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 194: Generator Loss = 9.9150, Discriminator Loss = 10.6299\n",
      "Epoch 194: Validation Loss = 6.6065\n",
      "Epoch 194: Validation Loss = 6.6065 Generator Loss = 9.9150, Discriminator Loss = 10.6299 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 195: Generator Loss = 10.8998, Discriminator Loss = 9.8004\n",
      "Epoch 195: Validation Loss = 6.0992\n",
      "Epoch 195: Validation Loss = 6.0992 Generator Loss = 10.8998, Discriminator Loss = 9.8004 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 196: Generator Loss = 9.4129, Discriminator Loss = 9.4462\n",
      "Epoch 196: Validation Loss = 6.2702\n",
      "Epoch 196: Validation Loss = 6.2702 Generator Loss = 9.4129, Discriminator Loss = 9.4462 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 197: Generator Loss = 9.0469, Discriminator Loss = 9.8941\n",
      "Epoch 197: Validation Loss = 6.4358\n",
      "Epoch 197: Validation Loss = 6.4358 Generator Loss = 9.0469, Discriminator Loss = 9.8941 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 198: Generator Loss = 10.8108, Discriminator Loss = 10.2735\n",
      "Epoch 198: Validation Loss = 6.6335\n",
      "Epoch 198: Validation Loss = 6.6335 Generator Loss = 10.8108, Discriminator Loss = 10.2735 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 199: Generator Loss = 8.9834, Discriminator Loss = 9.8245\n",
      "Epoch 199: Validation Loss = 6.4022\n",
      "Epoch 199: Validation Loss = 6.4022 Generator Loss = 8.9834, Discriminator Loss = 9.8245 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 200: Generator Loss = 9.5529, Discriminator Loss = 10.0012\n",
      "Epoch 200: Validation Loss = 6.5515\n",
      "Epoch 200: Validation Loss = 6.5515 Generator Loss = 9.5529, Discriminator Loss = 10.0012 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generating New Data for each method"
   ],
   "id": "be098fdb6fd7c1b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:50:04.171241Z",
     "start_time": "2025-05-18T11:49:51.031361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gan = GAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/GAN\",load_pretrained=True)\n",
    "cnngan = CNNGAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/CNNGAN\",load_pretrained=True)\n",
    "vae = VAE(device=device,result_dir='results/VAE',load_pretrained=True).to(device)\n",
    "cnnvae = CNNVAE(device=device,result_dir='results/CNNVAE',load_pretrained=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# gan.generate_new_data(num_samples=300)\n",
    "# cnngan.generate_new_data(num_samples=300)\n",
    "\n",
    "vae.generate_similar_data(dataset, num_samples=100)\n",
    "cnnvae.generate_similar_data(dataset, num_samples=100)\n"
   ],
   "id": "4a483222518fbdb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded best generator weights.\n",
      "[INFO] Loaded best discriminator weights.\n",
      "[INFO] Loaded best generator weights.\n",
      "[INFO] Loaded best discriminator weights.\n",
      "[INFO] Loaded best VAE weights.\n",
      "[INFO] Loaded best VAE weights.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f69f811c5a97506"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dirs_to_create = [\n",
    "    'res_net_model/',\n",
    "    'res_net_model/original_dataset/',\n",
    "    'res_net_model/oversampled_dataset/',\n",
    "    'res_net_model/oversampled_dataset/CNNGAN/',\n",
    "    'res_net_model/oversampled_dataset/CNNVAE/',\n",
    "    'res_net_model/oversampled_dataset/',\n",
    "    'res_net_model/synthetic_dataset/CNNGAN/',\n",
    "    'res_net_model/synthetic_dataset/CNNVAE/',\n",
    "    'res_net_model/over_oversampled_dataset/',\n",
    "    'res_net_model/over_oversampled_dataset/CNNGAN/',\n",
    "    'res_net_model/over_oversampled_dataset/CNNVAE/'\n",
    "]\n",
    "\n",
    "for dir_path in dirs_to_create:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=2,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:52:00.687269Z",
     "start_time": "2025-05-18T11:52:00.680921Z"
    }
   },
   "id": "7af14d7d449386a1",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## Full Dataset",
   "metadata": {
    "collapsed": false
   },
   "id": "a6f2363fa3cede6e"
  },
  {
   "cell_type": "code",
   "source": [
    "class CapsuleDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 pos_dir: str,\n",
    "                 neg_dirs,\n",
    "                 transform=None,\n",
    "                 max_per_dir: int = None):\n",
    "        \"\"\"\n",
    "        neg_dirs: either a single path string or a list of paths\n",
    "        max_per_dir: if set, take at most this many files from EACH directory\n",
    "        \"\"\"\n",
    "        if isinstance(neg_dirs, str):\n",
    "            neg_dirs = [neg_dirs]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        def _gather_files(directory):\n",
    "            files = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "            if max_per_dir is not None:\n",
    "                return files[:max_per_dir]\n",
    "            return files\n",
    "\n",
    "        # positives (label=0)\n",
    "        pos_files = sorted(glob.glob(os.path.join(pos_dir, \"*\")))\n",
    "        self.samples += [(p, 0) for p in pos_files]\n",
    "\n",
    "        # negatives (label=1)\n",
    "        for nd in neg_dirs:\n",
    "            neg_files = _gather_files(nd)\n",
    "            self.samples += [(p, 1) for p in neg_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# hyper‚Äëparameters\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "EPOCHS = 25\n",
    "\n",
    "# your augmentations + normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05,0.05), scale=(1.1,1.15), fill=255),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "# download or mount your Kaggle data however you like; suppose:\n",
    "# path = \".../capsule-defects\"\n",
    "pos_folder = os.path.join(path, \"capsule/positive\")\n",
    "neg_folder = os.path.join(path, \"capsule/negative\")\n",
    "print(len(glob.glob(os.path.join(pos_folder, \"*\"))))\n",
    "print(len(glob.glob(os.path.join(neg_folder, \"*\"))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T21:21:51.351753Z",
     "start_time": "2025-05-23T21:21:50.636262Z"
    }
   },
   "id": "f8a2b3c17f730679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "219\n",
      "109\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Original Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef545229f0077afd"
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T21:21:53.568153Z",
     "start_time": "2025-05-23T21:21:53.560393Z"
    }
   },
   "id": "d1168f38bfa0c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Oversampled Dataset (Equality in classes' instances)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21ea064fff490045"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dcc7463c9979bb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T00:53:52.310466Z",
     "start_time": "2025-05-18T00:53:52.303944Z"
    }
   },
   "id": "286d588a171d34b0",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec4fe8e16342fc3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4be0eab9b9ef1744"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:54:19.802839Z",
     "start_time": "2025-05-18T11:54:19.795308Z"
    }
   },
   "id": "e14420ca7060463",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.5752 Acc: 0.7032 | Val Loss: 0.6516 Acc: 0.6758\n",
      "Epoch 2/25 | Train Loss: 0.3923 Acc: 0.8128 | Val Loss: 0.5846 Acc: 0.6484\n",
      "Epoch 3/25 | Train Loss: 0.5497 Acc: 0.7717 | Val Loss: 0.9610 Acc: 0.4977\n",
      "Epoch 4/25 | Train Loss: 0.5461 Acc: 0.7443 | Val Loss: 0.5847 Acc: 0.6301\n",
      "Epoch 5/25 | Train Loss: 0.4664 Acc: 0.7808 | Val Loss: 0.6255 Acc: 0.5890\n",
      "Epoch 6/25 | Train Loss: 0.3942 Acc: 0.8174 | Val Loss: 0.5404 Acc: 0.7215\n",
      "Epoch 7/25 | Train Loss: 0.3537 Acc: 0.8265 | Val Loss: 0.6454 Acc: 0.6347\n",
      "Epoch 8/25 | Train Loss: 0.3135 Acc: 0.8356 | Val Loss: 0.5049 Acc: 0.6849\n",
      "Epoch 9/25 | Train Loss: 0.3048 Acc: 0.8584 | Val Loss: 0.5567 Acc: 0.7306\n",
      "Epoch 10/25 | Train Loss: 0.3468 Acc: 0.8311 | Val Loss: 0.4570 Acc: 0.7489\n",
      "Epoch 11/25 | Train Loss: 0.3034 Acc: 0.8493 | Val Loss: 0.4533 Acc: 0.7534\n",
      "Epoch 12/25 | Train Loss: 0.2887 Acc: 0.8950 | Val Loss: 0.4952 Acc: 0.7260\n",
      "Epoch 13/25 | Train Loss: 0.2772 Acc: 0.8676 | Val Loss: 0.5176 Acc: 0.7306\n",
      "Epoch 14/25 | Train Loss: 0.3283 Acc: 0.8493 | Val Loss: 0.4072 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3062 Acc: 0.8402 | Val Loss: 0.3979 Acc: 0.7854\n",
      "Epoch 16/25 | Train Loss: 0.3232 Acc: 0.8676 | Val Loss: 0.4044 Acc: 0.7854\n",
      "Epoch 17/25 | Train Loss: 0.3117 Acc: 0.8721 | Val Loss: 0.4501 Acc: 0.7534\n",
      "Epoch 18/25 | Train Loss: 0.2664 Acc: 0.8858 | Val Loss: 0.4478 Acc: 0.7626\n",
      "Epoch 19/25 | Train Loss: 0.2549 Acc: 0.8904 | Val Loss: 0.4545 Acc: 0.7489\n",
      "Epoch 20/25 | Train Loss: 0.2978 Acc: 0.8858 | Val Loss: 0.4603 Acc: 0.7808\n",
      "Epoch 21/25 | Train Loss: 0.2997 Acc: 0.8813 | Val Loss: 0.4104 Acc: 0.7900\n",
      "Epoch 22/25 | Train Loss: 0.2856 Acc: 0.8584 | Val Loss: 0.4081 Acc: 0.7900\n",
      "Epoch 23/25 | Train Loss: 0.2973 Acc: 0.8447 | Val Loss: 0.4247 Acc: 0.7991\n",
      "Epoch 24/25 | Train Loss: 0.3354 Acc: 0.8402 | Val Loss: 0.4022 Acc: 0.7580\n",
      "Epoch 25/25 | Train Loss: 0.3189 Acc: 0.8721 | Val Loss: 0.4289 Acc: 0.7626\n",
      "Fold 1 Test Accuracy: 0.8128\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6463 Acc: 0.5982 | Val Loss: 0.5443 Acc: 0.7808\n",
      "Epoch 2/25 | Train Loss: 0.5723 Acc: 0.6895 | Val Loss: 0.5591 Acc: 0.6119\n",
      "Epoch 3/25 | Train Loss: 0.5640 Acc: 0.7260 | Val Loss: 0.5955 Acc: 0.6393\n",
      "Epoch 4/25 | Train Loss: 0.6146 Acc: 0.6986 | Val Loss: 0.5767 Acc: 0.5799\n",
      "Epoch 5/25 | Train Loss: 0.5458 Acc: 0.6895 | Val Loss: 0.3867 Acc: 0.8128\n",
      "Epoch 6/25 | Train Loss: 0.5381 Acc: 0.7306 | Val Loss: 0.7016 Acc: 0.5799\n",
      "Epoch 7/25 | Train Loss: 0.4852 Acc: 0.7671 | Val Loss: 0.5099 Acc: 0.7215\n",
      "Epoch 8/25 | Train Loss: 0.4583 Acc: 0.7945 | Val Loss: 0.2962 Acc: 0.8813\n",
      "Epoch 9/25 | Train Loss: 0.4108 Acc: 0.7945 | Val Loss: 0.2913 Acc: 0.8721\n",
      "Epoch 10/25 | Train Loss: 0.3663 Acc: 0.8265 | Val Loss: 0.3434 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.4072 Acc: 0.8265 | Val Loss: 0.3390 Acc: 0.8174\n",
      "Epoch 12/25 | Train Loss: 0.3496 Acc: 0.8447 | Val Loss: 0.2849 Acc: 0.8950\n",
      "Epoch 13/25 | Train Loss: 0.4199 Acc: 0.8037 | Val Loss: 0.2543 Acc: 0.8813\n",
      "Epoch 14/25 | Train Loss: 0.3831 Acc: 0.8265 | Val Loss: 0.3606 Acc: 0.8356\n",
      "Epoch 15/25 | Train Loss: 0.3656 Acc: 0.8219 | Val Loss: 0.3181 Acc: 0.8584\n",
      "Epoch 16/25 | Train Loss: 0.3160 Acc: 0.8676 | Val Loss: 0.2892 Acc: 0.8676\n",
      "Epoch 17/25 | Train Loss: 0.3446 Acc: 0.8493 | Val Loss: 0.2893 Acc: 0.8904\n",
      "Epoch 18/25 | Train Loss: 0.3484 Acc: 0.8584 | Val Loss: 0.2680 Acc: 0.8995\n",
      "Epoch 19/25 | Train Loss: 0.3306 Acc: 0.8539 | Val Loss: 0.2724 Acc: 0.8995\n",
      "Epoch 20/25 | Train Loss: 0.3199 Acc: 0.8676 | Val Loss: 0.2774 Acc: 0.8539\n",
      "Epoch 21/25 | Train Loss: 0.2979 Acc: 0.8767 | Val Loss: 0.2631 Acc: 0.9087\n",
      "Epoch 22/25 | Train Loss: 0.3390 Acc: 0.8630 | Val Loss: 0.2756 Acc: 0.8676\n",
      "Epoch 23/25 | Train Loss: 0.3008 Acc: 0.8630 | Val Loss: 0.2990 Acc: 0.8813\n",
      "Epoch 24/25 | Train Loss: 0.3429 Acc: 0.8539 | Val Loss: 0.2675 Acc: 0.8950\n",
      "Epoch 25/25 | Train Loss: 0.3567 Acc: 0.8082 | Val Loss: 0.2785 Acc: 0.8813\n",
      "Fold 2 Test Accuracy: 0.8950\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.6242 Acc: 0.6484 | Val Loss: 0.8196 Acc: 0.5342\n",
      "Epoch 2/25 | Train Loss: 0.5786 Acc: 0.6712 | Val Loss: 0.5036 Acc: 0.7306\n",
      "Epoch 3/25 | Train Loss: 0.5309 Acc: 0.7489 | Val Loss: 0.5612 Acc: 0.7489\n",
      "Epoch 4/25 | Train Loss: 0.5004 Acc: 0.7717 | Val Loss: 0.7451 Acc: 0.7580\n",
      "Epoch 5/25 | Train Loss: 0.5166 Acc: 0.7900 | Val Loss: 0.5313 Acc: 0.7626\n",
      "Epoch 6/25 | Train Loss: 0.4487 Acc: 0.7763 | Val Loss: 0.7877 Acc: 0.7671\n",
      "Epoch 7/25 | Train Loss: 0.3977 Acc: 0.7991 | Val Loss: 1.2537 Acc: 0.5068\n",
      "Epoch 8/25 | Train Loss: 0.3618 Acc: 0.8265 | Val Loss: 0.4071 Acc: 0.8082\n",
      "Epoch 9/25 | Train Loss: 0.3525 Acc: 0.8447 | Val Loss: 0.3956 Acc: 0.8219\n",
      "Epoch 10/25 | Train Loss: 0.2477 Acc: 0.8995 | Val Loss: 0.4586 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.3482 Acc: 0.8447 | Val Loss: 0.3782 Acc: 0.7991\n",
      "Epoch 12/25 | Train Loss: 0.2623 Acc: 0.8858 | Val Loss: 0.3781 Acc: 0.8174\n",
      "Epoch 13/25 | Train Loss: 0.2497 Acc: 0.9132 | Val Loss: 0.3402 Acc: 0.8402\n",
      "Epoch 14/25 | Train Loss: 0.3047 Acc: 0.8858 | Val Loss: 0.4605 Acc: 0.7854\n",
      "Epoch 15/25 | Train Loss: 0.3278 Acc: 0.8493 | Val Loss: 0.3700 Acc: 0.8402\n",
      "Epoch 16/25 | Train Loss: 0.2995 Acc: 0.8721 | Val Loss: 0.3662 Acc: 0.8265\n",
      "Epoch 17/25 | Train Loss: 0.2819 Acc: 0.8630 | Val Loss: 0.3545 Acc: 0.8311\n",
      "Epoch 18/25 | Train Loss: 0.3213 Acc: 0.8356 | Val Loss: 0.3863 Acc: 0.8311\n",
      "Epoch 19/25 | Train Loss: 0.2634 Acc: 0.8767 | Val Loss: 0.3363 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.3067 Acc: 0.8858 | Val Loss: 0.3778 Acc: 0.8493\n",
      "Epoch 21/25 | Train Loss: 0.2950 Acc: 0.8493 | Val Loss: 0.3834 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.2341 Acc: 0.8995 | Val Loss: 0.3397 Acc: 0.8356\n",
      "Epoch 23/25 | Train Loss: 0.2845 Acc: 0.8813 | Val Loss: 0.3698 Acc: 0.8356\n",
      "Epoch 24/25 | Train Loss: 0.2800 Acc: 0.8539 | Val Loss: 0.3571 Acc: 0.8265\n",
      "Epoch 25/25 | Train Loss: 0.3048 Acc: 0.8493 | Val Loss: 0.3122 Acc: 0.8402\n",
      "Fold 3 Test Accuracy: 0.8356\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5935 Acc: 0.6895 | Val Loss: 0.5782 Acc: 0.6256\n",
      "Epoch 2/25 | Train Loss: 0.5491 Acc: 0.7306 | Val Loss: 0.5758 Acc: 0.7078\n",
      "Epoch 3/25 | Train Loss: 0.6559 Acc: 0.6804 | Val Loss: 0.6591 Acc: 0.7489\n",
      "Epoch 4/25 | Train Loss: 0.5253 Acc: 0.7352 | Val Loss: 0.4722 Acc: 0.7397\n",
      "Epoch 5/25 | Train Loss: 0.5172 Acc: 0.7671 | Val Loss: 0.5695 Acc: 0.6758\n",
      "Epoch 6/25 | Train Loss: 0.4335 Acc: 0.7900 | Val Loss: 0.4436 Acc: 0.7945\n",
      "Epoch 7/25 | Train Loss: 0.5307 Acc: 0.7397 | Val Loss: 0.5459 Acc: 0.7032\n",
      "Epoch 8/25 | Train Loss: 0.4750 Acc: 0.7626 | Val Loss: 0.4999 Acc: 0.7260\n",
      "Epoch 9/25 | Train Loss: 0.3884 Acc: 0.7945 | Val Loss: 0.4328 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.4075 Acc: 0.7808 | Val Loss: 0.4304 Acc: 0.8082\n",
      "Epoch 11/25 | Train Loss: 0.3994 Acc: 0.8082 | Val Loss: 0.4549 Acc: 0.7945\n",
      "Epoch 12/25 | Train Loss: 0.3864 Acc: 0.8174 | Val Loss: 0.4417 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3986 Acc: 0.8356 | Val Loss: 0.4526 Acc: 0.7854\n",
      "Epoch 14/25 | Train Loss: 0.3645 Acc: 0.8219 | Val Loss: 0.3997 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.4080 Acc: 0.7945 | Val Loss: 0.4262 Acc: 0.7671\n",
      "Epoch 16/25 | Train Loss: 0.3973 Acc: 0.7991 | Val Loss: 0.4088 Acc: 0.7945\n",
      "Epoch 17/25 | Train Loss: 0.3663 Acc: 0.8311 | Val Loss: 0.4438 Acc: 0.7945\n",
      "Epoch 18/25 | Train Loss: 0.3754 Acc: 0.8311 | Val Loss: 0.4210 Acc: 0.7945\n",
      "Epoch 19/25 | Train Loss: 0.3660 Acc: 0.8265 | Val Loss: 0.3797 Acc: 0.8311\n",
      "Epoch 20/25 | Train Loss: 0.3464 Acc: 0.8402 | Val Loss: 0.3923 Acc: 0.7945\n",
      "Epoch 21/25 | Train Loss: 0.4079 Acc: 0.7763 | Val Loss: 0.3891 Acc: 0.7991\n",
      "Epoch 22/25 | Train Loss: 0.3704 Acc: 0.8082 | Val Loss: 0.4301 Acc: 0.7808\n",
      "Epoch 23/25 | Train Loss: 0.3698 Acc: 0.8447 | Val Loss: 0.4004 Acc: 0.7991\n",
      "Epoch 24/25 | Train Loss: 0.3505 Acc: 0.8402 | Val Loss: 0.4126 Acc: 0.7854\n",
      "Epoch 25/25 | Train Loss: 0.3494 Acc: 0.8493 | Val Loss: 0.3888 Acc: 0.8265\n",
      "Fold 4 Test Accuracy: 0.7945\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.7110 Acc: 0.5936 | Val Loss: 0.5292 Acc: 0.7123\n",
      "Epoch 2/25 | Train Loss: 0.5512 Acc: 0.7215 | Val Loss: 0.6018 Acc: 0.6393\n",
      "Epoch 3/25 | Train Loss: 0.5128 Acc: 0.7306 | Val Loss: 0.5324 Acc: 0.7169\n",
      "Epoch 4/25 | Train Loss: 0.5640 Acc: 0.7671 | Val Loss: 0.6799 Acc: 0.5799\n",
      "Epoch 5/25 | Train Loss: 0.4783 Acc: 0.7808 | Val Loss: 0.4982 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.4452 Acc: 0.7900 | Val Loss: 0.4607 Acc: 0.7397\n",
      "Epoch 7/25 | Train Loss: 0.4406 Acc: 0.7900 | Val Loss: 0.6323 Acc: 0.7306\n",
      "Epoch 8/25 | Train Loss: 0.4806 Acc: 0.7900 | Val Loss: 0.5131 Acc: 0.7260\n",
      "Epoch 9/25 | Train Loss: 0.3780 Acc: 0.8174 | Val Loss: 0.4341 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.4170 Acc: 0.7534 | Val Loss: 0.4543 Acc: 0.7626\n",
      "Epoch 11/25 | Train Loss: 0.4215 Acc: 0.7945 | Val Loss: 0.4611 Acc: 0.7534\n",
      "Epoch 12/25 | Train Loss: 0.3717 Acc: 0.8356 | Val Loss: 0.4368 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3782 Acc: 0.7991 | Val Loss: 0.4285 Acc: 0.7945\n",
      "Epoch 14/25 | Train Loss: 0.3907 Acc: 0.7945 | Val Loss: 0.4422 Acc: 0.7580\n",
      "Epoch 15/25 | Train Loss: 0.3960 Acc: 0.8082 | Val Loss: 0.4130 Acc: 0.7717\n",
      "Epoch 16/25 | Train Loss: 0.3785 Acc: 0.8174 | Val Loss: 0.4313 Acc: 0.7854\n",
      "Epoch 17/25 | Train Loss: 0.3866 Acc: 0.7763 | Val Loss: 0.4212 Acc: 0.7717\n",
      "Epoch 18/25 | Train Loss: 0.3660 Acc: 0.8174 | Val Loss: 0.4332 Acc: 0.7534\n",
      "Epoch 19/25 | Train Loss: 0.3912 Acc: 0.8082 | Val Loss: 0.4281 Acc: 0.7671\n",
      "Epoch 20/25 | Train Loss: 0.3767 Acc: 0.8082 | Val Loss: 0.4035 Acc: 0.7945\n",
      "Epoch 21/25 | Train Loss: 0.3943 Acc: 0.7945 | Val Loss: 0.4232 Acc: 0.7671\n",
      "Epoch 22/25 | Train Loss: 0.4114 Acc: 0.7991 | Val Loss: 0.4356 Acc: 0.7854\n",
      "Epoch 23/25 | Train Loss: 0.3724 Acc: 0.8402 | Val Loss: 0.3982 Acc: 0.7900\n",
      "Epoch 24/25 | Train Loss: 0.4313 Acc: 0.7900 | Val Loss: 0.4283 Acc: 0.7854\n",
      "Epoch 25/25 | Train Loss: 0.3960 Acc: 0.8037 | Val Loss: 0.4119 Acc: 0.7763\n",
      "Fold 5 Test Accuracy: 0.7717\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.6295 Acc: 0.6438 | Val Loss: 0.5869 Acc: 0.7717\n",
      "Epoch 2/25 | Train Loss: 0.5707 Acc: 0.6895 | Val Loss: 0.4373 Acc: 0.7945\n",
      "Epoch 3/25 | Train Loss: 0.5946 Acc: 0.7260 | Val Loss: 0.8192 Acc: 0.5708\n",
      "Epoch 4/25 | Train Loss: 0.5153 Acc: 0.7489 | Val Loss: 0.4525 Acc: 0.7763\n",
      "Epoch 5/25 | Train Loss: 0.4796 Acc: 0.7489 | Val Loss: 0.6136 Acc: 0.7808\n",
      "Epoch 6/25 | Train Loss: 0.4653 Acc: 0.7854 | Val Loss: 0.4260 Acc: 0.7808\n",
      "Epoch 7/25 | Train Loss: 0.4216 Acc: 0.8219 | Val Loss: 0.5213 Acc: 0.7991\n",
      "Epoch 8/25 | Train Loss: 0.4151 Acc: 0.8174 | Val Loss: 0.3903 Acc: 0.8174\n",
      "Epoch 9/25 | Train Loss: 0.4120 Acc: 0.7854 | Val Loss: 0.3593 Acc: 0.8311\n",
      "Epoch 10/25 | Train Loss: 0.3613 Acc: 0.8447 | Val Loss: 0.3773 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.3626 Acc: 0.8265 | Val Loss: 0.3633 Acc: 0.8174\n",
      "Epoch 12/25 | Train Loss: 0.3494 Acc: 0.8356 | Val Loss: 0.3655 Acc: 0.8265\n",
      "Epoch 13/25 | Train Loss: 0.3383 Acc: 0.8447 | Val Loss: 0.3366 Acc: 0.8584\n",
      "Epoch 14/25 | Train Loss: 0.2958 Acc: 0.8676 | Val Loss: 0.3437 Acc: 0.8311\n",
      "Epoch 15/25 | Train Loss: 0.2901 Acc: 0.8950 | Val Loss: 0.3544 Acc: 0.8493\n",
      "Epoch 16/25 | Train Loss: 0.2628 Acc: 0.9041 | Val Loss: 0.3561 Acc: 0.8539\n",
      "Epoch 17/25 | Train Loss: 0.3604 Acc: 0.8402 | Val Loss: 0.3078 Acc: 0.8767\n",
      "Epoch 18/25 | Train Loss: 0.3051 Acc: 0.8630 | Val Loss: 0.3319 Acc: 0.8539\n",
      "Epoch 19/25 | Train Loss: 0.3077 Acc: 0.8584 | Val Loss: 0.3426 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.2715 Acc: 0.8858 | Val Loss: 0.3389 Acc: 0.8219\n",
      "Epoch 21/25 | Train Loss: 0.2896 Acc: 0.8767 | Val Loss: 0.3547 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.2656 Acc: 0.8950 | Val Loss: 0.3497 Acc: 0.8676\n",
      "Epoch 23/25 | Train Loss: 0.3203 Acc: 0.8493 | Val Loss: 0.3172 Acc: 0.8630\n",
      "Epoch 24/25 | Train Loss: 0.3410 Acc: 0.8402 | Val Loss: 0.3340 Acc: 0.8356\n",
      "Epoch 25/25 | Train Loss: 0.3235 Acc: 0.8493 | Val Loss: 0.3851 Acc: 0.8219\n",
      "Fold 6 Test Accuracy: 0.8265\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.6419 Acc: 0.6393 | Val Loss: 0.4971 Acc: 0.7534\n",
      "Epoch 2/25 | Train Loss: 0.5408 Acc: 0.7215 | Val Loss: 0.4693 Acc: 0.7991\n",
      "Epoch 3/25 | Train Loss: 0.5476 Acc: 0.7078 | Val Loss: 0.5908 Acc: 0.6164\n",
      "Epoch 4/25 | Train Loss: 0.5801 Acc: 0.7123 | Val Loss: 0.6161 Acc: 0.7717\n",
      "Epoch 5/25 | Train Loss: 0.4395 Acc: 0.8128 | Val Loss: 0.4954 Acc: 0.7808\n",
      "Epoch 6/25 | Train Loss: 0.4696 Acc: 0.7443 | Val Loss: 0.4389 Acc: 0.7945\n",
      "Epoch 7/25 | Train Loss: 0.4821 Acc: 0.7671 | Val Loss: 0.4000 Acc: 0.7945\n",
      "Epoch 8/25 | Train Loss: 0.4241 Acc: 0.7991 | Val Loss: 0.4393 Acc: 0.7580\n",
      "Epoch 9/25 | Train Loss: 0.4298 Acc: 0.7763 | Val Loss: 0.4422 Acc: 0.7489\n",
      "Epoch 10/25 | Train Loss: 0.3933 Acc: 0.8128 | Val Loss: 0.4190 Acc: 0.7534\n",
      "Epoch 11/25 | Train Loss: 0.4043 Acc: 0.8037 | Val Loss: 0.4102 Acc: 0.8128\n",
      "Epoch 12/25 | Train Loss: 0.4226 Acc: 0.7854 | Val Loss: 0.4035 Acc: 0.8265\n",
      "Epoch 13/25 | Train Loss: 0.3448 Acc: 0.8402 | Val Loss: 0.3936 Acc: 0.7808\n",
      "Epoch 14/25 | Train Loss: 0.3648 Acc: 0.8584 | Val Loss: 0.3832 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3657 Acc: 0.8356 | Val Loss: 0.3697 Acc: 0.7900\n",
      "Epoch 16/25 | Train Loss: 0.3517 Acc: 0.8630 | Val Loss: 0.3710 Acc: 0.8356\n",
      "Epoch 17/25 | Train Loss: 0.4368 Acc: 0.7763 | Val Loss: 0.3898 Acc: 0.8037\n",
      "Epoch 18/25 | Train Loss: 0.3454 Acc: 0.8493 | Val Loss: 0.3601 Acc: 0.8356\n",
      "Epoch 19/25 | Train Loss: 0.3740 Acc: 0.8356 | Val Loss: 0.3978 Acc: 0.7945\n",
      "Epoch 20/25 | Train Loss: 0.3623 Acc: 0.8402 | Val Loss: 0.3701 Acc: 0.8037\n",
      "Epoch 21/25 | Train Loss: 0.3505 Acc: 0.8356 | Val Loss: 0.3458 Acc: 0.8311\n",
      "Epoch 22/25 | Train Loss: 0.3715 Acc: 0.8447 | Val Loss: 0.3774 Acc: 0.7900\n",
      "Epoch 23/25 | Train Loss: 0.3529 Acc: 0.8402 | Val Loss: 0.3666 Acc: 0.8311\n",
      "Epoch 24/25 | Train Loss: 0.3532 Acc: 0.8493 | Val Loss: 0.3529 Acc: 0.8356\n",
      "Epoch 25/25 | Train Loss: 0.3289 Acc: 0.8813 | Val Loss: 0.3524 Acc: 0.8219\n",
      "Fold 7 Test Accuracy: 0.8447\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.6299 Acc: 0.6575 | Val Loss: 0.5992 Acc: 0.5890\n",
      "Epoch 2/25 | Train Loss: 0.6312 Acc: 0.7169 | Val Loss: 0.9002 Acc: 0.5160\n",
      "Epoch 3/25 | Train Loss: 0.5342 Acc: 0.7443 | Val Loss: 0.5249 Acc: 0.7352\n",
      "Epoch 4/25 | Train Loss: 0.4500 Acc: 0.7580 | Val Loss: 0.5540 Acc: 0.6804\n",
      "Epoch 5/25 | Train Loss: 0.4804 Acc: 0.7717 | Val Loss: 0.5478 Acc: 0.6895\n",
      "Epoch 6/25 | Train Loss: 0.5154 Acc: 0.7717 | Val Loss: 0.5307 Acc: 0.7352\n",
      "Epoch 7/25 | Train Loss: 0.4626 Acc: 0.7626 | Val Loss: 0.5344 Acc: 0.6758\n",
      "Epoch 8/25 | Train Loss: 0.4260 Acc: 0.7854 | Val Loss: 0.5015 Acc: 0.7489\n",
      "Epoch 9/25 | Train Loss: 0.3895 Acc: 0.8037 | Val Loss: 0.4851 Acc: 0.7352\n",
      "Epoch 10/25 | Train Loss: 0.3640 Acc: 0.8311 | Val Loss: 0.4912 Acc: 0.7352\n",
      "Epoch 11/25 | Train Loss: 0.3577 Acc: 0.8265 | Val Loss: 0.4767 Acc: 0.7260\n",
      "Epoch 12/25 | Train Loss: 0.3426 Acc: 0.8539 | Val Loss: 0.4538 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3568 Acc: 0.8356 | Val Loss: 0.4774 Acc: 0.7626\n",
      "Epoch 14/25 | Train Loss: 0.3649 Acc: 0.8174 | Val Loss: 0.5178 Acc: 0.7534\n",
      "Epoch 15/25 | Train Loss: 0.3296 Acc: 0.8356 | Val Loss: 0.4867 Acc: 0.7626\n",
      "Epoch 16/25 | Train Loss: 0.3322 Acc: 0.8584 | Val Loss: 0.4618 Acc: 0.7763\n",
      "Epoch 17/25 | Train Loss: 0.3770 Acc: 0.8219 | Val Loss: 0.4627 Acc: 0.7626\n",
      "Epoch 18/25 | Train Loss: 0.3529 Acc: 0.8265 | Val Loss: 0.4266 Acc: 0.7717\n",
      "Epoch 19/25 | Train Loss: 0.4076 Acc: 0.8128 | Val Loss: 0.5121 Acc: 0.7123\n",
      "Epoch 20/25 | Train Loss: 0.3427 Acc: 0.8311 | Val Loss: 0.4489 Acc: 0.7854\n",
      "Epoch 21/25 | Train Loss: 0.3587 Acc: 0.8174 | Val Loss: 0.4747 Acc: 0.7808\n",
      "Epoch 22/25 | Train Loss: 0.3807 Acc: 0.8402 | Val Loss: 0.4628 Acc: 0.7489\n",
      "Epoch 23/25 | Train Loss: 0.3357 Acc: 0.8311 | Val Loss: 0.4534 Acc: 0.7717\n",
      "Epoch 24/25 | Train Loss: 0.3440 Acc: 0.8356 | Val Loss: 0.4611 Acc: 0.7763\n",
      "Epoch 25/25 | Train Loss: 0.3458 Acc: 0.8356 | Val Loss: 0.5009 Acc: 0.7626\n",
      "Fold 8 Test Accuracy: 0.7123\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5849 Acc: 0.6667 | Val Loss: 0.5402 Acc: 0.6849\n",
      "Epoch 2/25 | Train Loss: 0.5510 Acc: 0.6758 | Val Loss: 0.4582 Acc: 0.7580\n",
      "Epoch 3/25 | Train Loss: 0.5637 Acc: 0.6804 | Val Loss: 0.5111 Acc: 0.7306\n",
      "Epoch 4/25 | Train Loss: 0.5622 Acc: 0.7078 | Val Loss: 0.4265 Acc: 0.7900\n",
      "Epoch 5/25 | Train Loss: 0.5264 Acc: 0.7032 | Val Loss: 0.5216 Acc: 0.6986\n",
      "Epoch 6/25 | Train Loss: 0.5126 Acc: 0.7397 | Val Loss: 0.5417 Acc: 0.6347\n",
      "Epoch 7/25 | Train Loss: 0.5252 Acc: 0.7260 | Val Loss: 0.9126 Acc: 0.5114\n",
      "Epoch 8/25 | Train Loss: 0.4754 Acc: 0.7352 | Val Loss: 0.4499 Acc: 0.7808\n",
      "Epoch 9/25 | Train Loss: 0.4267 Acc: 0.7945 | Val Loss: 0.4045 Acc: 0.7854\n",
      "Epoch 10/25 | Train Loss: 0.4137 Acc: 0.8082 | Val Loss: 0.3768 Acc: 0.8037\n",
      "Epoch 11/25 | Train Loss: 0.4097 Acc: 0.8219 | Val Loss: 0.4124 Acc: 0.8082\n",
      "Epoch 12/25 | Train Loss: 0.3915 Acc: 0.8356 | Val Loss: 0.4504 Acc: 0.7717\n",
      "Epoch 13/25 | Train Loss: 0.3962 Acc: 0.8037 | Val Loss: 0.3580 Acc: 0.8402\n",
      "Epoch 14/25 | Train Loss: 0.4045 Acc: 0.7900 | Val Loss: 0.3647 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.4083 Acc: 0.8174 | Val Loss: 0.3444 Acc: 0.8584\n",
      "Epoch 16/25 | Train Loss: 0.3877 Acc: 0.8128 | Val Loss: 0.3695 Acc: 0.8219\n",
      "Epoch 17/25 | Train Loss: 0.4038 Acc: 0.8219 | Val Loss: 0.3748 Acc: 0.8265\n",
      "Epoch 18/25 | Train Loss: 0.3915 Acc: 0.8311 | Val Loss: 0.3666 Acc: 0.7945\n",
      "Epoch 19/25 | Train Loss: 0.3617 Acc: 0.8447 | Val Loss: 0.3567 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.3717 Acc: 0.8082 | Val Loss: 0.3383 Acc: 0.8493\n",
      "Epoch 21/25 | Train Loss: 0.3994 Acc: 0.7808 | Val Loss: 0.3719 Acc: 0.8311\n",
      "Epoch 22/25 | Train Loss: 0.4104 Acc: 0.8174 | Val Loss: 0.3649 Acc: 0.8128\n",
      "Epoch 23/25 | Train Loss: 0.4003 Acc: 0.7991 | Val Loss: 0.4008 Acc: 0.8037\n",
      "Epoch 24/25 | Train Loss: 0.3622 Acc: 0.8174 | Val Loss: 0.3258 Acc: 0.8539\n",
      "Epoch 25/25 | Train Loss: 0.4176 Acc: 0.8128 | Val Loss: 0.3596 Acc: 0.8447\n",
      "Fold 9 Test Accuracy: 0.8219\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.6299 Acc: 0.6575 | Val Loss: 0.5763 Acc: 0.6941\n",
      "Epoch 2/25 | Train Loss: 0.5459 Acc: 0.7260 | Val Loss: 0.5108 Acc: 0.7078\n",
      "Epoch 3/25 | Train Loss: 0.5298 Acc: 0.7215 | Val Loss: 0.5680 Acc: 0.6712\n",
      "Epoch 4/25 | Train Loss: 0.4612 Acc: 0.7991 | Val Loss: 0.4860 Acc: 0.7489\n",
      "Epoch 5/25 | Train Loss: 0.4572 Acc: 0.7945 | Val Loss: 0.5424 Acc: 0.7306\n",
      "Epoch 6/25 | Train Loss: 0.3962 Acc: 0.8128 | Val Loss: 0.4690 Acc: 0.7489\n",
      "Epoch 7/25 | Train Loss: 0.3770 Acc: 0.8219 | Val Loss: 0.5861 Acc: 0.7626\n",
      "Epoch 8/25 | Train Loss: 0.3529 Acc: 0.8447 | Val Loss: 0.4440 Acc: 0.7945\n",
      "Epoch 9/25 | Train Loss: 0.3483 Acc: 0.8128 | Val Loss: 0.4977 Acc: 0.7626\n",
      "Epoch 10/25 | Train Loss: 0.2905 Acc: 0.8950 | Val Loss: 0.4349 Acc: 0.8037\n",
      "Epoch 11/25 | Train Loss: 0.3546 Acc: 0.8584 | Val Loss: 0.4582 Acc: 0.7900\n",
      "Epoch 12/25 | Train Loss: 0.3132 Acc: 0.8493 | Val Loss: 0.4388 Acc: 0.7900\n",
      "Epoch 13/25 | Train Loss: 0.2960 Acc: 0.8767 | Val Loss: 0.3873 Acc: 0.7945\n",
      "Epoch 14/25 | Train Loss: 0.3080 Acc: 0.8676 | Val Loss: 0.3959 Acc: 0.8174\n",
      "Epoch 15/25 | Train Loss: 0.2682 Acc: 0.8813 | Val Loss: 0.3733 Acc: 0.8174\n",
      "Epoch 16/25 | Train Loss: 0.3105 Acc: 0.8493 | Val Loss: 0.4022 Acc: 0.7991\n",
      "Epoch 17/25 | Train Loss: 0.2972 Acc: 0.8858 | Val Loss: 0.3904 Acc: 0.8174\n",
      "Epoch 18/25 | Train Loss: 0.2449 Acc: 0.9087 | Val Loss: 0.4194 Acc: 0.8037\n",
      "Epoch 19/25 | Train Loss: 0.2698 Acc: 0.8950 | Val Loss: 0.3960 Acc: 0.8174\n",
      "Epoch 20/25 | Train Loss: 0.2627 Acc: 0.8995 | Val Loss: 0.3871 Acc: 0.8082\n",
      "Epoch 21/25 | Train Loss: 0.2930 Acc: 0.8858 | Val Loss: 0.4255 Acc: 0.7991\n",
      "Epoch 22/25 | Train Loss: 0.2808 Acc: 0.8904 | Val Loss: 0.4067 Acc: 0.8174\n",
      "Epoch 23/25 | Train Loss: 0.2691 Acc: 0.8995 | Val Loss: 0.4094 Acc: 0.8037\n",
      "Epoch 24/25 | Train Loss: 0.2513 Acc: 0.8995 | Val Loss: 0.4379 Acc: 0.8128\n",
      "Epoch 25/25 | Train Loss: 0.2934 Acc: 0.8813 | Val Loss: 0.3856 Acc: 0.8265\n",
      "Fold 10 Test Accuracy: 0.7991\n",
      "\n",
      "5√ó2 CV results: Mean Acc = 0.8114, Std Dev = 0.0481\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T13:18:38.335751Z",
     "start_time": "2025-05-18T11:54:41.552675Z"
    }
   },
   "id": "af21f1d23069c568",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Syntethic dataset (negative class only made from generated images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d584694220a9823"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ab0e479319d2a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T05:30:01.540388Z",
     "start_time": "2025-05-18T05:30:01.525220Z"
    }
   },
   "id": "4236292aeee6eb5a",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.2726 Acc: 0.8767 | Val Loss: 0.0956 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0280 Acc: 1.0000 | Val Loss: 0.0093 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0031 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0140 Acc: 0.9954 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0132 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0007 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.2200 Acc: 0.9132 | Val Loss: 0.0676 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0247 Acc: 0.9954 | Val Loss: 0.0085 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0264 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0198 Acc: 0.9909 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0060 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0312 Acc: 0.9817 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0088 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 1.0000\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.3141 Acc: 0.8858 | Val Loss: 0.2268 Acc: 0.9498\n",
      "Epoch 2/25 | Train Loss: 0.0270 Acc: 1.0000 | Val Loss: 0.0121 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0073 Acc: 1.0000 | Val Loss: 0.0040 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0213 Acc: 0.9909 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0158 Acc: 1.0000 | Val Loss: 0.0044 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0149 Acc: 0.9954 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0123 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0074 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 1.0000\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.3270 Acc: 0.8447 | Val Loss: 0.0958 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0064 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0324 Acc: 0.9954 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0023 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0076 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0122 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0242 Acc: 0.9863 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0090 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0127 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0313 Acc: 0.9909 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.3339 Acc: 0.8265 | Val Loss: 0.0685 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0241 Acc: 1.0000 | Val Loss: 0.0107 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0282 Acc: 0.9954 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0082 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0097 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 1.0000\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.2898 Acc: 0.8539 | Val Loss: 0.1650 Acc: 0.9909\n",
      "Epoch 2/25 | Train Loss: 0.0170 Acc: 1.0000 | Val Loss: 0.0212 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0284 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0102 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0153 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 1.0000\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.3460 Acc: 0.8356 | Val Loss: 0.0851 Acc: 0.9954\n",
      "Epoch 2/25 | Train Loss: 0.0191 Acc: 1.0000 | Val Loss: 0.0044 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0206 Acc: 0.9909 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0108 Acc: 1.0000 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0103 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0090 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0137 Acc: 0.9954 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.3080 Acc: 0.8676 | Val Loss: 0.0785 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0404 Acc: 0.9954 | Val Loss: 0.0031 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3343 Acc: 0.8493 | Val Loss: 0.0785 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0347 Acc: 0.9954 | Val Loss: 0.0085 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0109 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0255 Acc: 0.9863 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0146 Acc: 0.9954 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0073 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0100 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0305 Acc: 0.9863 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 1.0000\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.3599 Acc: 0.8082 | Val Loss: 0.0908 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0109 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0146 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0104 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0092 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0069 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5√ó2 CV results: Mean Acc = 1.0000, Std Dev = 0.0000\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T06:23:32.513883Z",
     "start_time": "2025-05-18T05:30:01.542396Z"
    }
   },
   "id": "1da07ad0daad1554",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80a67814c1f2e3e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T14:34:54.574802Z",
     "start_time": "2025-05-18T14:34:54.565027Z"
    }
   },
   "id": "f7b7b351d2e2435",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.3896 Acc: 0.8037 | Val Loss: 0.0933 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0291 Acc: 1.0000 | Val Loss: 0.0050 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0356 Acc: 0.9909 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0183 Acc: 0.9954 | Val Loss: 0.0037 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0164 Acc: 0.9954 | Val Loss: 0.0191 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0077 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0091 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0056 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0086 Acc: 0.9954 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0131 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.3403 Acc: 0.8493 | Val Loss: 0.0729 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0352 Acc: 1.0000 | Val Loss: 0.0149 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0078 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0501 Acc: 0.9726 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0155 Acc: 0.9909 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0241 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0191 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0104 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 0.9863\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.3590 Acc: 0.8447 | Val Loss: 0.1136 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0366 Acc: 1.0000 | Val Loss: 0.0069 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0117 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0701 Acc: 0.9680 | Val Loss: 0.0059 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0170 Acc: 0.9954 | Val Loss: 0.0029 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0059 Acc: 1.0000 | Val Loss: 0.0051 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 0.9954\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.3403 Acc: 0.8630 | Val Loss: 0.0945 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0314 Acc: 1.0000 | Val Loss: 0.0088 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0025 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0019 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0238 Acc: 0.9954 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0090 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0273 Acc: 0.9863 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0260 Acc: 0.9909 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0100 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.3381 Acc: 0.8630 | Val Loss: 0.1809 Acc: 0.9863\n",
      "Epoch 2/25 | Train Loss: 0.0221 Acc: 1.0000 | Val Loss: 0.0273 Acc: 0.9954\n",
      "Epoch 3/25 | Train Loss: 0.0228 Acc: 0.9954 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0082 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0096 Acc: 0.9954 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0090 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0141 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 1.0000\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.3743 Acc: 0.8402 | Val Loss: 0.1135 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0520 Acc: 0.9909 | Val Loss: 0.0096 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0237 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0144 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0124 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0059 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 1.0000\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.3053 Acc: 0.8995 | Val Loss: 0.0741 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0912 Acc: 0.9772 | Val Loss: 0.0039 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0160 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0070 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0117 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0218 Acc: 0.9863 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0102 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0143 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0120 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.3281 Acc: 0.8767 | Val Loss: 0.0789 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0788 Acc: 0.9726 | Val Loss: 0.0076 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0093 Acc: 1.0000 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0023 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0151 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0116 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0074 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0087 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0092 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0087 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0391 Acc: 0.9863 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0192 Acc: 0.9954 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0143 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3810 Acc: 0.8311 | Val Loss: 0.0821 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0403 Acc: 0.9954 | Val Loss: 0.0037 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0119 Acc: 1.0000 | Val Loss: 0.0026 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0181 Acc: 0.9954 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0106 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0153 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 1.0000\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.3095 Acc: 0.8767 | Val Loss: 0.0919 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0415 Acc: 1.0000 | Val Loss: 0.0122 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0123 Acc: 1.0000 | Val Loss: 0.0036 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0212 Acc: 0.9954 | Val Loss: 0.0409 Acc: 0.9863\n",
      "Epoch 7/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0086 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0156 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0088 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0070 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0220 Acc: 0.9954 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0118 Acc: 0.9909 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0045 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0056 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5√ó2 CV results: Mean Acc = 0.9982, Std Dev = 0.0044\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T15:28:48.417453Z",
     "start_time": "2025-05-18T14:34:54.576809Z"
    }
   },
   "id": "2e0987154c90646c",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Over-oversampled dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a22bc78da939e67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ab2e653f59225c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T09:03:00.001267Z",
     "start_time": "2025-05-18T09:02:59.986146Z"
    }
   },
   "id": "7e11a8a1d8aeb8cc",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.4942 Acc: 0.7482 | Val Loss: 0.4767 Acc: 0.7993\n",
      "Epoch 2/25 | Train Loss: 0.4869 Acc: 0.7409 | Val Loss: 0.4522 Acc: 0.7153\n",
      "Epoch 3/25 | Train Loss: 0.3837 Acc: 0.8066 | Val Loss: 0.5400 Acc: 0.6642\n",
      "Epoch 4/25 | Train Loss: 0.4839 Acc: 0.7263 | Val Loss: 0.4458 Acc: 0.6642\n",
      "Epoch 5/25 | Train Loss: 0.4508 Acc: 0.7591 | Val Loss: 0.3456 Acc: 0.8394\n",
      "Epoch 6/25 | Train Loss: 0.3910 Acc: 0.8248 | Val Loss: 0.3947 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.3735 Acc: 0.8431 | Val Loss: 0.4566 Acc: 0.6861\n",
      "Epoch 8/25 | Train Loss: 0.4287 Acc: 0.7190 | Val Loss: 0.3816 Acc: 0.7956\n",
      "Epoch 9/25 | Train Loss: 0.3653 Acc: 0.8139 | Val Loss: 0.3381 Acc: 0.8358\n",
      "Epoch 10/25 | Train Loss: 0.3448 Acc: 0.8066 | Val Loss: 0.3607 Acc: 0.7956\n",
      "Epoch 11/25 | Train Loss: 0.3717 Acc: 0.8175 | Val Loss: 0.3289 Acc: 0.8358\n",
      "Epoch 12/25 | Train Loss: 0.3529 Acc: 0.8139 | Val Loss: 0.3549 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3260 Acc: 0.8504 | Val Loss: 0.3346 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.3185 Acc: 0.8613 | Val Loss: 0.3549 Acc: 0.8321\n",
      "Epoch 15/25 | Train Loss: 0.3066 Acc: 0.8796 | Val Loss: 0.3540 Acc: 0.8175\n",
      "Epoch 16/25 | Train Loss: 0.2965 Acc: 0.8759 | Val Loss: 0.3346 Acc: 0.8577\n",
      "Epoch 17/25 | Train Loss: 0.3293 Acc: 0.8577 | Val Loss: 0.3476 Acc: 0.8321\n",
      "Epoch 18/25 | Train Loss: 0.3386 Acc: 0.8467 | Val Loss: 0.3273 Acc: 0.8212\n",
      "Epoch 19/25 | Train Loss: 0.3483 Acc: 0.8504 | Val Loss: 0.3380 Acc: 0.8066\n",
      "Epoch 20/25 | Train Loss: 0.3233 Acc: 0.8723 | Val Loss: 0.3176 Acc: 0.8504\n",
      "Epoch 21/25 | Train Loss: 0.2945 Acc: 0.8832 | Val Loss: 0.3120 Acc: 0.8504\n",
      "Epoch 22/25 | Train Loss: 0.3333 Acc: 0.8431 | Val Loss: 0.3403 Acc: 0.8431\n",
      "Epoch 23/25 | Train Loss: 0.3025 Acc: 0.8723 | Val Loss: 0.3473 Acc: 0.8394\n",
      "Epoch 24/25 | Train Loss: 0.3156 Acc: 0.8504 | Val Loss: 0.3301 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.3449 Acc: 0.8394 | Val Loss: 0.3309 Acc: 0.8431\n",
      "Fold 1 Test Accuracy: 0.8467\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6129 Acc: 0.6569 | Val Loss: 0.5563 Acc: 0.6350\n",
      "Epoch 2/25 | Train Loss: 0.5788 Acc: 0.6898 | Val Loss: 0.3772 Acc: 0.8248\n",
      "Epoch 3/25 | Train Loss: 0.6517 Acc: 0.7299 | Val Loss: 0.3700 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.5849 Acc: 0.7737 | Val Loss: 0.4922 Acc: 0.7701\n",
      "Epoch 5/25 | Train Loss: 0.4477 Acc: 0.7993 | Val Loss: 0.3836 Acc: 0.8139\n",
      "Epoch 6/25 | Train Loss: 0.4050 Acc: 0.7847 | Val Loss: 0.3952 Acc: 0.8029\n",
      "Epoch 7/25 | Train Loss: 0.4747 Acc: 0.7591 | Val Loss: 0.3933 Acc: 0.8029\n",
      "Epoch 8/25 | Train Loss: 0.3918 Acc: 0.8175 | Val Loss: 0.3942 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3827 Acc: 0.7956 | Val Loss: 0.4005 Acc: 0.8029\n",
      "Epoch 10/25 | Train Loss: 0.3549 Acc: 0.8285 | Val Loss: 0.3541 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3778 Acc: 0.8029 | Val Loss: 0.3681 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3617 Acc: 0.8212 | Val Loss: 0.3910 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3853 Acc: 0.7956 | Val Loss: 0.3646 Acc: 0.8066\n",
      "Epoch 14/25 | Train Loss: 0.3675 Acc: 0.8175 | Val Loss: 0.3640 Acc: 0.8139\n",
      "Epoch 15/25 | Train Loss: 0.3470 Acc: 0.8248 | Val Loss: 0.3750 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.3724 Acc: 0.7993 | Val Loss: 0.3732 Acc: 0.8139\n",
      "Epoch 17/25 | Train Loss: 0.3843 Acc: 0.8066 | Val Loss: 0.3558 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3608 Acc: 0.8212 | Val Loss: 0.3591 Acc: 0.8175\n",
      "Epoch 19/25 | Train Loss: 0.4010 Acc: 0.7883 | Val Loss: 0.3521 Acc: 0.8102\n",
      "Epoch 20/25 | Train Loss: 0.3781 Acc: 0.8175 | Val Loss: 0.3660 Acc: 0.8139\n",
      "Epoch 21/25 | Train Loss: 0.3702 Acc: 0.8066 | Val Loss: 0.3673 Acc: 0.8139\n",
      "Epoch 22/25 | Train Loss: 0.3792 Acc: 0.8029 | Val Loss: 0.3547 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3728 Acc: 0.8212 | Val Loss: 0.3585 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.3733 Acc: 0.8066 | Val Loss: 0.3686 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.3619 Acc: 0.8066 | Val Loss: 0.3722 Acc: 0.8066\n",
      "Fold 2 Test Accuracy: 0.8029\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5548 Acc: 0.7044 | Val Loss: 0.4236 Acc: 0.8102\n",
      "Epoch 2/25 | Train Loss: 0.4508 Acc: 0.7774 | Val Loss: 0.6651 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4113 Acc: 0.7883 | Val Loss: 0.4188 Acc: 0.7701\n",
      "Epoch 4/25 | Train Loss: 0.3829 Acc: 0.8102 | Val Loss: 0.4539 Acc: 0.7007\n",
      "Epoch 5/25 | Train Loss: 0.3765 Acc: 0.8248 | Val Loss: 0.3927 Acc: 0.7445\n",
      "Epoch 6/25 | Train Loss: 0.3537 Acc: 0.8175 | Val Loss: 0.5170 Acc: 0.8066\n",
      "Epoch 7/25 | Train Loss: 0.4060 Acc: 0.7737 | Val Loss: 0.4173 Acc: 0.8102\n",
      "Epoch 8/25 | Train Loss: 0.4211 Acc: 0.8029 | Val Loss: 0.3732 Acc: 0.8066\n",
      "Epoch 9/25 | Train Loss: 0.3745 Acc: 0.8358 | Val Loss: 0.3693 Acc: 0.7993\n",
      "Epoch 10/25 | Train Loss: 0.3398 Acc: 0.8285 | Val Loss: 0.3505 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3138 Acc: 0.8540 | Val Loss: 0.3431 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3295 Acc: 0.8540 | Val Loss: 0.3692 Acc: 0.8175\n",
      "Epoch 13/25 | Train Loss: 0.2821 Acc: 0.8650 | Val Loss: 0.3389 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.2719 Acc: 0.8796 | Val Loss: 0.3336 Acc: 0.8212\n",
      "Epoch 15/25 | Train Loss: 0.2727 Acc: 0.8759 | Val Loss: 0.3123 Acc: 0.8394\n",
      "Epoch 16/25 | Train Loss: 0.2770 Acc: 0.8686 | Val Loss: 0.3225 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.2839 Acc: 0.8869 | Val Loss: 0.3396 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.2782 Acc: 0.8759 | Val Loss: 0.3356 Acc: 0.8358\n",
      "Epoch 19/25 | Train Loss: 0.3244 Acc: 0.8467 | Val Loss: 0.3299 Acc: 0.8504\n",
      "Epoch 20/25 | Train Loss: 0.2715 Acc: 0.8832 | Val Loss: 0.3105 Acc: 0.8431\n",
      "Epoch 21/25 | Train Loss: 0.2841 Acc: 0.8686 | Val Loss: 0.3231 Acc: 0.8175\n",
      "Epoch 22/25 | Train Loss: 0.2787 Acc: 0.8796 | Val Loss: 0.3270 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.2919 Acc: 0.8832 | Val Loss: 0.3182 Acc: 0.8358\n",
      "Epoch 24/25 | Train Loss: 0.3016 Acc: 0.8504 | Val Loss: 0.3056 Acc: 0.8504\n",
      "Epoch 25/25 | Train Loss: 0.3050 Acc: 0.8540 | Val Loss: 0.3189 Acc: 0.8504\n",
      "Fold 3 Test Accuracy: 0.8467\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5348 Acc: 0.7153 | Val Loss: 0.4500 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.5139 Acc: 0.7153 | Val Loss: 0.4264 Acc: 0.7628\n",
      "Epoch 3/25 | Train Loss: 0.3751 Acc: 0.7956 | Val Loss: 0.3781 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.4094 Acc: 0.7883 | Val Loss: 0.3708 Acc: 0.8102\n",
      "Epoch 5/25 | Train Loss: 0.5036 Acc: 0.7993 | Val Loss: 0.5336 Acc: 0.6788\n",
      "Epoch 6/25 | Train Loss: 0.3633 Acc: 0.8358 | Val Loss: 0.4084 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.3440 Acc: 0.8358 | Val Loss: 0.3586 Acc: 0.8139\n",
      "Epoch 8/25 | Train Loss: 0.3608 Acc: 0.8285 | Val Loss: 0.3328 Acc: 0.8321\n",
      "Epoch 9/25 | Train Loss: 0.3723 Acc: 0.8358 | Val Loss: 0.3247 Acc: 0.8504\n",
      "Epoch 10/25 | Train Loss: 0.3694 Acc: 0.8394 | Val Loss: 0.3351 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3181 Acc: 0.8431 | Val Loss: 0.3720 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.3305 Acc: 0.8467 | Val Loss: 0.3205 Acc: 0.8431\n",
      "Epoch 13/25 | Train Loss: 0.3211 Acc: 0.8467 | Val Loss: 0.3041 Acc: 0.8650\n",
      "Epoch 14/25 | Train Loss: 0.3229 Acc: 0.8394 | Val Loss: 0.3360 Acc: 0.8139\n",
      "Epoch 15/25 | Train Loss: 0.3138 Acc: 0.8577 | Val Loss: 0.3332 Acc: 0.8321\n",
      "Epoch 16/25 | Train Loss: 0.3369 Acc: 0.8358 | Val Loss: 0.3046 Acc: 0.8613\n",
      "Epoch 17/25 | Train Loss: 0.3275 Acc: 0.8504 | Val Loss: 0.3210 Acc: 0.8613\n",
      "Epoch 18/25 | Train Loss: 0.2857 Acc: 0.8686 | Val Loss: 0.3257 Acc: 0.8321\n",
      "Epoch 19/25 | Train Loss: 0.2834 Acc: 0.8796 | Val Loss: 0.3209 Acc: 0.8431\n",
      "Epoch 20/25 | Train Loss: 0.2908 Acc: 0.8577 | Val Loss: 0.3109 Acc: 0.8540\n",
      "Epoch 21/25 | Train Loss: 0.3291 Acc: 0.8431 | Val Loss: 0.3219 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.2911 Acc: 0.8650 | Val Loss: 0.3286 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.2802 Acc: 0.8723 | Val Loss: 0.3318 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.2884 Acc: 0.8613 | Val Loss: 0.3314 Acc: 0.8394\n",
      "Epoch 25/25 | Train Loss: 0.2968 Acc: 0.8540 | Val Loss: 0.3145 Acc: 0.8394\n",
      "Fold 4 Test Accuracy: 0.8577\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.6469 Acc: 0.6496 | Val Loss: 0.4629 Acc: 0.8029\n",
      "Epoch 2/25 | Train Loss: 0.4287 Acc: 0.7555 | Val Loss: 0.4223 Acc: 0.8066\n",
      "Epoch 3/25 | Train Loss: 0.4282 Acc: 0.7664 | Val Loss: 0.5532 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4849 Acc: 0.7591 | Val Loss: 0.4124 Acc: 0.7847\n",
      "Epoch 5/25 | Train Loss: 0.5204 Acc: 0.8102 | Val Loss: 0.6646 Acc: 0.6387\n",
      "Epoch 6/25 | Train Loss: 0.4537 Acc: 0.7737 | Val Loss: 0.4421 Acc: 0.7336\n",
      "Epoch 7/25 | Train Loss: 0.4210 Acc: 0.7847 | Val Loss: 0.4154 Acc: 0.7810\n",
      "Epoch 8/25 | Train Loss: 0.3688 Acc: 0.8175 | Val Loss: 0.3853 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3566 Acc: 0.8212 | Val Loss: 0.3386 Acc: 0.8285\n",
      "Epoch 10/25 | Train Loss: 0.3413 Acc: 0.8358 | Val Loss: 0.3402 Acc: 0.8248\n",
      "Epoch 11/25 | Train Loss: 0.3484 Acc: 0.8102 | Val Loss: 0.3662 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.3342 Acc: 0.8248 | Val Loss: 0.3513 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.3254 Acc: 0.8431 | Val Loss: 0.3581 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.3515 Acc: 0.8394 | Val Loss: 0.3574 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.3250 Acc: 0.8394 | Val Loss: 0.3784 Acc: 0.7883\n",
      "Epoch 16/25 | Train Loss: 0.3379 Acc: 0.8285 | Val Loss: 0.3555 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.3718 Acc: 0.8139 | Val Loss: 0.3597 Acc: 0.8248\n",
      "Epoch 18/25 | Train Loss: 0.3426 Acc: 0.8394 | Val Loss: 0.3683 Acc: 0.8066\n",
      "Epoch 19/25 | Train Loss: 0.3128 Acc: 0.8577 | Val Loss: 0.3596 Acc: 0.8285\n",
      "Epoch 20/25 | Train Loss: 0.3358 Acc: 0.8285 | Val Loss: 0.3639 Acc: 0.8358\n",
      "Epoch 21/25 | Train Loss: 0.3358 Acc: 0.8212 | Val Loss: 0.3467 Acc: 0.8248\n",
      "Epoch 22/25 | Train Loss: 0.3381 Acc: 0.8285 | Val Loss: 0.3471 Acc: 0.8504\n",
      "Epoch 23/25 | Train Loss: 0.3305 Acc: 0.8394 | Val Loss: 0.3376 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.3470 Acc: 0.8285 | Val Loss: 0.3273 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3406 Acc: 0.8358 | Val Loss: 0.3626 Acc: 0.8248\n",
      "Fold 5 Test Accuracy: 0.7993\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5378 Acc: 0.7226 | Val Loss: 0.4185 Acc: 0.8066\n",
      "Epoch 2/25 | Train Loss: 0.4670 Acc: 0.7701 | Val Loss: 0.4385 Acc: 0.7993\n",
      "Epoch 3/25 | Train Loss: 0.4807 Acc: 0.7555 | Val Loss: 0.3775 Acc: 0.8212\n",
      "Epoch 4/25 | Train Loss: 0.4626 Acc: 0.7737 | Val Loss: 0.3440 Acc: 0.8212\n",
      "Epoch 5/25 | Train Loss: 0.3817 Acc: 0.8102 | Val Loss: 0.6593 Acc: 0.6569\n",
      "Epoch 6/25 | Train Loss: 0.4254 Acc: 0.7847 | Val Loss: 0.3887 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.4272 Acc: 0.8029 | Val Loss: 0.4658 Acc: 0.7044\n",
      "Epoch 8/25 | Train Loss: 0.4568 Acc: 0.7409 | Val Loss: 0.3743 Acc: 0.8212\n",
      "Epoch 9/25 | Train Loss: 0.3859 Acc: 0.7920 | Val Loss: 0.3616 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3370 Acc: 0.8212 | Val Loss: 0.3445 Acc: 0.8248\n",
      "Epoch 11/25 | Train Loss: 0.3427 Acc: 0.8248 | Val Loss: 0.3665 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3287 Acc: 0.8577 | Val Loss: 0.3871 Acc: 0.7883\n",
      "Epoch 13/25 | Train Loss: 0.3185 Acc: 0.8540 | Val Loss: 0.3615 Acc: 0.8139\n",
      "Epoch 14/25 | Train Loss: 0.3393 Acc: 0.8504 | Val Loss: 0.4028 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3412 Acc: 0.8285 | Val Loss: 0.3857 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3110 Acc: 0.8504 | Val Loss: 0.3910 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3121 Acc: 0.8723 | Val Loss: 0.3527 Acc: 0.8285\n",
      "Epoch 18/25 | Train Loss: 0.3129 Acc: 0.8686 | Val Loss: 0.3492 Acc: 0.8394\n",
      "Epoch 19/25 | Train Loss: 0.3474 Acc: 0.8358 | Val Loss: 0.3681 Acc: 0.8175\n",
      "Epoch 20/25 | Train Loss: 0.3007 Acc: 0.8650 | Val Loss: 0.3719 Acc: 0.8139\n",
      "Epoch 21/25 | Train Loss: 0.3269 Acc: 0.8613 | Val Loss: 0.3585 Acc: 0.7993\n",
      "Epoch 22/25 | Train Loss: 0.3176 Acc: 0.8358 | Val Loss: 0.3506 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3174 Acc: 0.8431 | Val Loss: 0.3763 Acc: 0.8212\n",
      "Epoch 24/25 | Train Loss: 0.3281 Acc: 0.8431 | Val Loss: 0.3666 Acc: 0.8175\n",
      "Epoch 25/25 | Train Loss: 0.3424 Acc: 0.8431 | Val Loss: 0.3650 Acc: 0.8212\n",
      "Fold 6 Test Accuracy: 0.8285\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.5621 Acc: 0.6898 | Val Loss: 0.3681 Acc: 0.8212\n",
      "Epoch 2/25 | Train Loss: 0.4888 Acc: 0.7336 | Val Loss: 0.4843 Acc: 0.6861\n",
      "Epoch 3/25 | Train Loss: 0.4197 Acc: 0.7993 | Val Loss: 0.4540 Acc: 0.7153\n",
      "Epoch 4/25 | Train Loss: 0.4103 Acc: 0.7847 | Val Loss: 0.6431 Acc: 0.7299\n",
      "Epoch 5/25 | Train Loss: 0.6105 Acc: 0.7044 | Val Loss: 0.3520 Acc: 0.8175\n",
      "Epoch 6/25 | Train Loss: 0.4199 Acc: 0.7701 | Val Loss: 0.3155 Acc: 0.8431\n",
      "Epoch 7/25 | Train Loss: 0.4219 Acc: 0.8248 | Val Loss: 0.4505 Acc: 0.7737\n",
      "Epoch 8/25 | Train Loss: 0.4162 Acc: 0.7737 | Val Loss: 0.3147 Acc: 0.8540\n",
      "Epoch 9/25 | Train Loss: 0.3518 Acc: 0.8358 | Val Loss: 0.2863 Acc: 0.8723\n",
      "Epoch 10/25 | Train Loss: 0.3270 Acc: 0.8504 | Val Loss: 0.3067 Acc: 0.8613\n",
      "Epoch 11/25 | Train Loss: 0.3053 Acc: 0.8650 | Val Loss: 0.3203 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3293 Acc: 0.8467 | Val Loss: 0.3062 Acc: 0.8540\n",
      "Epoch 13/25 | Train Loss: 0.2851 Acc: 0.8796 | Val Loss: 0.2900 Acc: 0.8540\n",
      "Epoch 14/25 | Train Loss: 0.3103 Acc: 0.8248 | Val Loss: 0.2686 Acc: 0.8650\n",
      "Epoch 15/25 | Train Loss: 0.2993 Acc: 0.8796 | Val Loss: 0.2830 Acc: 0.8613\n",
      "Epoch 16/25 | Train Loss: 0.2774 Acc: 0.8650 | Val Loss: 0.2896 Acc: 0.8577\n",
      "Epoch 17/25 | Train Loss: 0.3147 Acc: 0.8540 | Val Loss: 0.2778 Acc: 0.8686\n",
      "Epoch 18/25 | Train Loss: 0.2937 Acc: 0.8759 | Val Loss: 0.2769 Acc: 0.8686\n",
      "Epoch 19/25 | Train Loss: 0.2651 Acc: 0.8942 | Val Loss: 0.2827 Acc: 0.8613\n",
      "Epoch 20/25 | Train Loss: 0.2981 Acc: 0.8686 | Val Loss: 0.2836 Acc: 0.8650\n",
      "Epoch 21/25 | Train Loss: 0.2971 Acc: 0.8467 | Val Loss: 0.2712 Acc: 0.8577\n",
      "Epoch 22/25 | Train Loss: 0.3298 Acc: 0.8394 | Val Loss: 0.2761 Acc: 0.8796\n",
      "Epoch 23/25 | Train Loss: 0.3022 Acc: 0.8686 | Val Loss: 0.2829 Acc: 0.8613\n",
      "Epoch 24/25 | Train Loss: 0.2746 Acc: 0.8650 | Val Loss: 0.2898 Acc: 0.8540\n",
      "Epoch 25/25 | Train Loss: 0.2827 Acc: 0.8686 | Val Loss: 0.2846 Acc: 0.8577\n",
      "Fold 7 Test Accuracy: 0.8613\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.4539 Acc: 0.7555 | Val Loss: 0.4482 Acc: 0.7409\n",
      "Epoch 2/25 | Train Loss: 0.4158 Acc: 0.7883 | Val Loss: 0.4524 Acc: 0.7445\n",
      "Epoch 3/25 | Train Loss: 0.3814 Acc: 0.7956 | Val Loss: 0.4504 Acc: 0.7810\n",
      "Epoch 4/25 | Train Loss: 0.3530 Acc: 0.8321 | Val Loss: 0.5834 Acc: 0.7664\n",
      "Epoch 5/25 | Train Loss: 0.3678 Acc: 0.8139 | Val Loss: 0.5455 Acc: 0.7774\n",
      "Epoch 6/25 | Train Loss: 0.3862 Acc: 0.8321 | Val Loss: 0.4145 Acc: 0.7701\n",
      "Epoch 7/25 | Train Loss: 0.3367 Acc: 0.8102 | Val Loss: 0.4037 Acc: 0.8029\n",
      "Epoch 8/25 | Train Loss: 0.3452 Acc: 0.8540 | Val Loss: 0.3619 Acc: 0.8102\n",
      "Epoch 9/25 | Train Loss: 0.3218 Acc: 0.8394 | Val Loss: 0.3680 Acc: 0.7883\n",
      "Epoch 10/25 | Train Loss: 0.3276 Acc: 0.8467 | Val Loss: 0.3918 Acc: 0.7847\n",
      "Epoch 11/25 | Train Loss: 0.2821 Acc: 0.8650 | Val Loss: 0.3595 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.2785 Acc: 0.8686 | Val Loss: 0.3510 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.2958 Acc: 0.8759 | Val Loss: 0.3684 Acc: 0.8029\n",
      "Epoch 14/25 | Train Loss: 0.2693 Acc: 0.8905 | Val Loss: 0.3710 Acc: 0.7920\n",
      "Epoch 15/25 | Train Loss: 0.2772 Acc: 0.8686 | Val Loss: 0.3680 Acc: 0.7920\n",
      "Epoch 16/25 | Train Loss: 0.2826 Acc: 0.8869 | Val Loss: 0.3333 Acc: 0.8212\n",
      "Epoch 17/25 | Train Loss: 0.2531 Acc: 0.8723 | Val Loss: 0.3540 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.2702 Acc: 0.8759 | Val Loss: 0.3461 Acc: 0.8285\n",
      "Epoch 19/25 | Train Loss: 0.2831 Acc: 0.8504 | Val Loss: 0.3764 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.2843 Acc: 0.8686 | Val Loss: 0.3555 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.2626 Acc: 0.8942 | Val Loss: 0.3338 Acc: 0.8394\n",
      "Epoch 22/25 | Train Loss: 0.2607 Acc: 0.8869 | Val Loss: 0.3310 Acc: 0.8321\n",
      "Epoch 23/25 | Train Loss: 0.2976 Acc: 0.8577 | Val Loss: 0.3288 Acc: 0.8248\n",
      "Epoch 24/25 | Train Loss: 0.2683 Acc: 0.8650 | Val Loss: 0.3224 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.2633 Acc: 0.8759 | Val Loss: 0.3573 Acc: 0.8212\n",
      "Fold 8 Test Accuracy: 0.8248\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5400 Acc: 0.6898 | Val Loss: 0.4228 Acc: 0.7701\n",
      "Epoch 2/25 | Train Loss: 0.5173 Acc: 0.7701 | Val Loss: 0.5215 Acc: 0.7883\n",
      "Epoch 3/25 | Train Loss: 0.5479 Acc: 0.7518 | Val Loss: 0.4687 Acc: 0.8139\n",
      "Epoch 4/25 | Train Loss: 0.4255 Acc: 0.7737 | Val Loss: 0.4033 Acc: 0.7628\n",
      "Epoch 5/25 | Train Loss: 0.4900 Acc: 0.7482 | Val Loss: 0.5480 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.5169 Acc: 0.7299 | Val Loss: 0.5328 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.5508 Acc: 0.7993 | Val Loss: 0.5325 Acc: 0.6387\n",
      "Epoch 8/25 | Train Loss: 0.4309 Acc: 0.7445 | Val Loss: 0.3763 Acc: 0.7956\n",
      "Epoch 9/25 | Train Loss: 0.3574 Acc: 0.8285 | Val Loss: 0.3699 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3246 Acc: 0.8540 | Val Loss: 0.3604 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3418 Acc: 0.8285 | Val Loss: 0.3448 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3181 Acc: 0.8650 | Val Loss: 0.3747 Acc: 0.7956\n",
      "Epoch 13/25 | Train Loss: 0.3009 Acc: 0.8431 | Val Loss: 0.3715 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3074 Acc: 0.8759 | Val Loss: 0.3282 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3128 Acc: 0.8650 | Val Loss: 0.3380 Acc: 0.8285\n",
      "Epoch 16/25 | Train Loss: 0.3140 Acc: 0.8686 | Val Loss: 0.3536 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.2979 Acc: 0.8723 | Val Loss: 0.3528 Acc: 0.7956\n",
      "Epoch 18/25 | Train Loss: 0.2910 Acc: 0.8796 | Val Loss: 0.3560 Acc: 0.8467\n",
      "Epoch 19/25 | Train Loss: 0.3054 Acc: 0.8613 | Val Loss: 0.3617 Acc: 0.8066\n",
      "Epoch 20/25 | Train Loss: 0.3029 Acc: 0.8723 | Val Loss: 0.3512 Acc: 0.8431\n",
      "Epoch 21/25 | Train Loss: 0.2990 Acc: 0.8613 | Val Loss: 0.3290 Acc: 0.8431\n",
      "Epoch 22/25 | Train Loss: 0.2935 Acc: 0.8577 | Val Loss: 0.3573 Acc: 0.8139\n",
      "Epoch 23/25 | Train Loss: 0.2967 Acc: 0.8504 | Val Loss: 0.3400 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2824 Acc: 0.8905 | Val Loss: 0.3463 Acc: 0.8139\n",
      "Epoch 25/25 | Train Loss: 0.3043 Acc: 0.8577 | Val Loss: 0.3434 Acc: 0.8175\n",
      "Fold 9 Test Accuracy: 0.8431\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5525 Acc: 0.7190 | Val Loss: 0.4245 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.4325 Acc: 0.7883 | Val Loss: 0.5856 Acc: 0.7993\n",
      "Epoch 3/25 | Train Loss: 0.4503 Acc: 0.7701 | Val Loss: 0.4044 Acc: 0.7737\n",
      "Epoch 4/25 | Train Loss: 0.4250 Acc: 0.7847 | Val Loss: 0.4339 Acc: 0.8029\n",
      "Epoch 5/25 | Train Loss: 0.3976 Acc: 0.8102 | Val Loss: 0.3707 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.3626 Acc: 0.8102 | Val Loss: 0.5161 Acc: 0.7007\n",
      "Epoch 7/25 | Train Loss: 0.3709 Acc: 0.7883 | Val Loss: 0.5132 Acc: 0.7993\n",
      "Epoch 8/25 | Train Loss: 0.4229 Acc: 0.8066 | Val Loss: 0.4478 Acc: 0.7993\n",
      "Epoch 9/25 | Train Loss: 0.3639 Acc: 0.8248 | Val Loss: 0.3568 Acc: 0.8321\n",
      "Epoch 10/25 | Train Loss: 0.3321 Acc: 0.8650 | Val Loss: 0.3360 Acc: 0.8358\n",
      "Epoch 11/25 | Train Loss: 0.3386 Acc: 0.8431 | Val Loss: 0.3441 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3127 Acc: 0.8650 | Val Loss: 0.3346 Acc: 0.8431\n",
      "Epoch 13/25 | Train Loss: 0.3282 Acc: 0.8504 | Val Loss: 0.3363 Acc: 0.8358\n",
      "Epoch 14/25 | Train Loss: 0.3298 Acc: 0.8540 | Val Loss: 0.3264 Acc: 0.8358\n",
      "Epoch 15/25 | Train Loss: 0.3035 Acc: 0.8540 | Val Loss: 0.3315 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3035 Acc: 0.8613 | Val Loss: 0.3333 Acc: 0.8358\n",
      "Epoch 17/25 | Train Loss: 0.3187 Acc: 0.8504 | Val Loss: 0.3620 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.3072 Acc: 0.8467 | Val Loss: 0.3346 Acc: 0.8321\n",
      "Epoch 19/25 | Train Loss: 0.3024 Acc: 0.8796 | Val Loss: 0.3551 Acc: 0.8248\n",
      "Epoch 20/25 | Train Loss: 0.3087 Acc: 0.8504 | Val Loss: 0.3307 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3040 Acc: 0.8723 | Val Loss: 0.3337 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.3143 Acc: 0.8431 | Val Loss: 0.3389 Acc: 0.8394\n",
      "Epoch 23/25 | Train Loss: 0.3003 Acc: 0.8504 | Val Loss: 0.3246 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.3004 Acc: 0.8577 | Val Loss: 0.3384 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3038 Acc: 0.8650 | Val Loss: 0.3249 Acc: 0.8467\n",
      "Fold 10 Test Accuracy: 0.8467\n",
      "\n",
      "5√ó2 CV results: Mean Acc = 0.8358, Std Dev = 0.0214\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T10:20:45.010596Z",
     "start_time": "2025-05-18T09:03:00.002273Z"
    }
   },
   "id": "b366861a88b8de03",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f700fbe891b185be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T16:22:26.804131Z",
     "start_time": "2025-05-18T16:22:26.790843Z"
    }
   },
   "id": "314d5ad019d8954c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.5229 Acc: 0.7226 | Val Loss: 0.4320 Acc: 0.7774\n",
      "Epoch 2/25 | Train Loss: 0.4052 Acc: 0.7664 | Val Loss: 0.6136 Acc: 0.6496\n",
      "Epoch 3/25 | Train Loss: 0.4967 Acc: 0.7482 | Val Loss: 0.4206 Acc: 0.7956\n",
      "Epoch 4/25 | Train Loss: 0.4338 Acc: 0.8066 | Val Loss: 0.4717 Acc: 0.7628\n",
      "Epoch 5/25 | Train Loss: 0.5014 Acc: 0.7664 | Val Loss: 0.4394 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.3703 Acc: 0.8321 | Val Loss: 0.4600 Acc: 0.7117\n",
      "Epoch 7/25 | Train Loss: 0.3997 Acc: 0.7956 | Val Loss: 0.4127 Acc: 0.7664\n",
      "Epoch 8/25 | Train Loss: 0.3319 Acc: 0.8577 | Val Loss: 0.4073 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3398 Acc: 0.8248 | Val Loss: 0.3912 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3524 Acc: 0.8285 | Val Loss: 0.3782 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3281 Acc: 0.8686 | Val Loss: 0.3679 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3496 Acc: 0.8248 | Val Loss: 0.3618 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.3210 Acc: 0.8504 | Val Loss: 0.3708 Acc: 0.8139\n",
      "Epoch 14/25 | Train Loss: 0.3331 Acc: 0.8431 | Val Loss: 0.3843 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.3232 Acc: 0.8358 | Val Loss: 0.3639 Acc: 0.8066\n",
      "Epoch 16/25 | Train Loss: 0.3336 Acc: 0.8394 | Val Loss: 0.3417 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3415 Acc: 0.8321 | Val Loss: 0.3772 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3103 Acc: 0.8540 | Val Loss: 0.3849 Acc: 0.8102\n",
      "Epoch 19/25 | Train Loss: 0.3421 Acc: 0.8285 | Val Loss: 0.3614 Acc: 0.8175\n",
      "Epoch 20/25 | Train Loss: 0.2997 Acc: 0.8650 | Val Loss: 0.3573 Acc: 0.8066\n",
      "Epoch 21/25 | Train Loss: 0.3119 Acc: 0.8577 | Val Loss: 0.3666 Acc: 0.8029\n",
      "Epoch 22/25 | Train Loss: 0.3321 Acc: 0.8285 | Val Loss: 0.3591 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3584 Acc: 0.8577 | Val Loss: 0.3413 Acc: 0.8248\n",
      "Epoch 24/25 | Train Loss: 0.3465 Acc: 0.8431 | Val Loss: 0.3826 Acc: 0.7993\n",
      "Epoch 25/25 | Train Loss: 0.3201 Acc: 0.8467 | Val Loss: 0.3510 Acc: 0.8175\n",
      "Fold 1 Test Accuracy: 0.8212\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.5238 Acc: 0.7226 | Val Loss: 0.4456 Acc: 0.8139\n",
      "Epoch 2/25 | Train Loss: 0.4925 Acc: 0.7518 | Val Loss: 0.5085 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4618 Acc: 0.7774 | Val Loss: 0.3852 Acc: 0.8212\n",
      "Epoch 4/25 | Train Loss: 0.4341 Acc: 0.7555 | Val Loss: 0.3645 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4432 Acc: 0.7956 | Val Loss: 0.3751 Acc: 0.8102\n",
      "Epoch 6/25 | Train Loss: 0.4215 Acc: 0.7883 | Val Loss: 0.4046 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.4396 Acc: 0.7336 | Val Loss: 0.3377 Acc: 0.8285\n",
      "Epoch 8/25 | Train Loss: 0.3428 Acc: 0.8285 | Val Loss: 0.3281 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3482 Acc: 0.7920 | Val Loss: 0.3308 Acc: 0.8248\n",
      "Epoch 10/25 | Train Loss: 0.3337 Acc: 0.8321 | Val Loss: 0.3374 Acc: 0.8394\n",
      "Epoch 11/25 | Train Loss: 0.3166 Acc: 0.8321 | Val Loss: 0.3256 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.3194 Acc: 0.8431 | Val Loss: 0.3058 Acc: 0.8577\n",
      "Epoch 13/25 | Train Loss: 0.3286 Acc: 0.8285 | Val Loss: 0.3201 Acc: 0.8467\n",
      "Epoch 14/25 | Train Loss: 0.3192 Acc: 0.8431 | Val Loss: 0.3378 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3043 Acc: 0.8540 | Val Loss: 0.3187 Acc: 0.8321\n",
      "Epoch 16/25 | Train Loss: 0.3062 Acc: 0.8650 | Val Loss: 0.2980 Acc: 0.8723\n",
      "Epoch 17/25 | Train Loss: 0.2968 Acc: 0.8577 | Val Loss: 0.3127 Acc: 0.8358\n",
      "Epoch 18/25 | Train Loss: 0.3132 Acc: 0.8467 | Val Loss: 0.3141 Acc: 0.8358\n",
      "Epoch 19/25 | Train Loss: 0.3104 Acc: 0.8613 | Val Loss: 0.3100 Acc: 0.8540\n",
      "Epoch 20/25 | Train Loss: 0.3088 Acc: 0.8431 | Val Loss: 0.2977 Acc: 0.8650\n",
      "Epoch 21/25 | Train Loss: 0.2926 Acc: 0.8431 | Val Loss: 0.3205 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3256 Acc: 0.8650 | Val Loss: 0.3163 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.2960 Acc: 0.8759 | Val Loss: 0.2886 Acc: 0.8759\n",
      "Epoch 24/25 | Train Loss: 0.2857 Acc: 0.8869 | Val Loss: 0.3299 Acc: 0.8504\n",
      "Epoch 25/25 | Train Loss: 0.3241 Acc: 0.8394 | Val Loss: 0.3024 Acc: 0.8577\n",
      "Fold 2 Test Accuracy: 0.8540\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5157 Acc: 0.7409 | Val Loss: 0.7756 Acc: 0.6204\n",
      "Epoch 2/25 | Train Loss: 0.4508 Acc: 0.7628 | Val Loss: 0.4272 Acc: 0.7920\n",
      "Epoch 3/25 | Train Loss: 0.4949 Acc: 0.7810 | Val Loss: 0.4345 Acc: 0.7847\n",
      "Epoch 4/25 | Train Loss: 0.5009 Acc: 0.7226 | Val Loss: 0.5409 Acc: 0.8066\n",
      "Epoch 5/25 | Train Loss: 0.4699 Acc: 0.7810 | Val Loss: 0.3786 Acc: 0.7847\n",
      "Epoch 6/25 | Train Loss: 0.3717 Acc: 0.8102 | Val Loss: 0.3572 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.3755 Acc: 0.7920 | Val Loss: 0.3737 Acc: 0.7956\n",
      "Epoch 8/25 | Train Loss: 0.3611 Acc: 0.8102 | Val Loss: 0.3658 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.3450 Acc: 0.8394 | Val Loss: 0.3497 Acc: 0.8175\n",
      "Epoch 10/25 | Train Loss: 0.3554 Acc: 0.8029 | Val Loss: 0.3419 Acc: 0.8321\n",
      "Epoch 11/25 | Train Loss: 0.3387 Acc: 0.8431 | Val Loss: 0.3770 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3320 Acc: 0.8248 | Val Loss: 0.3479 Acc: 0.8175\n",
      "Epoch 13/25 | Train Loss: 0.3197 Acc: 0.8540 | Val Loss: 0.3379 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3414 Acc: 0.8248 | Val Loss: 0.3372 Acc: 0.8431\n",
      "Epoch 15/25 | Train Loss: 0.3229 Acc: 0.8358 | Val Loss: 0.3581 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.3058 Acc: 0.8650 | Val Loss: 0.3312 Acc: 0.8431\n",
      "Epoch 17/25 | Train Loss: 0.3083 Acc: 0.8504 | Val Loss: 0.3457 Acc: 0.8212\n",
      "Epoch 18/25 | Train Loss: 0.3200 Acc: 0.8285 | Val Loss: 0.3370 Acc: 0.8431\n",
      "Epoch 19/25 | Train Loss: 0.3254 Acc: 0.8504 | Val Loss: 0.3443 Acc: 0.8248\n",
      "Epoch 20/25 | Train Loss: 0.3490 Acc: 0.8248 | Val Loss: 0.3361 Acc: 0.8467\n",
      "Epoch 21/25 | Train Loss: 0.3389 Acc: 0.8358 | Val Loss: 0.3400 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3287 Acc: 0.8394 | Val Loss: 0.3273 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.3356 Acc: 0.8248 | Val Loss: 0.3433 Acc: 0.8285\n",
      "Epoch 24/25 | Train Loss: 0.3419 Acc: 0.8212 | Val Loss: 0.3441 Acc: 0.8394\n",
      "Epoch 25/25 | Train Loss: 0.3011 Acc: 0.8577 | Val Loss: 0.3243 Acc: 0.8248\n",
      "Fold 3 Test Accuracy: 0.8212\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5115 Acc: 0.7080 | Val Loss: 0.5320 Acc: 0.6387\n",
      "Epoch 2/25 | Train Loss: 0.4424 Acc: 0.7737 | Val Loss: 0.4811 Acc: 0.7117\n",
      "Epoch 3/25 | Train Loss: 0.4079 Acc: 0.7993 | Val Loss: 0.4420 Acc: 0.7409\n",
      "Epoch 4/25 | Train Loss: 0.4349 Acc: 0.7993 | Val Loss: 0.5259 Acc: 0.6934\n",
      "Epoch 5/25 | Train Loss: 0.4259 Acc: 0.7810 | Val Loss: 0.4306 Acc: 0.7555\n",
      "Epoch 6/25 | Train Loss: 0.4826 Acc: 0.7409 | Val Loss: 0.3876 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.4065 Acc: 0.7883 | Val Loss: 0.4090 Acc: 0.7591\n",
      "Epoch 8/25 | Train Loss: 0.3746 Acc: 0.8102 | Val Loss: 0.3880 Acc: 0.7883\n",
      "Epoch 9/25 | Train Loss: 0.3922 Acc: 0.7920 | Val Loss: 0.3825 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3838 Acc: 0.8029 | Val Loss: 0.3772 Acc: 0.8102\n",
      "Epoch 11/25 | Train Loss: 0.3715 Acc: 0.8139 | Val Loss: 0.3589 Acc: 0.8175\n",
      "Epoch 12/25 | Train Loss: 0.3597 Acc: 0.8212 | Val Loss: 0.3701 Acc: 0.8066\n",
      "Epoch 13/25 | Train Loss: 0.3575 Acc: 0.8285 | Val Loss: 0.3748 Acc: 0.7956\n",
      "Epoch 14/25 | Train Loss: 0.3476 Acc: 0.8139 | Val Loss: 0.3675 Acc: 0.8102\n",
      "Epoch 15/25 | Train Loss: 0.3508 Acc: 0.8394 | Val Loss: 0.3650 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.3557 Acc: 0.8248 | Val Loss: 0.3722 Acc: 0.8029\n",
      "Epoch 17/25 | Train Loss: 0.3563 Acc: 0.8321 | Val Loss: 0.3660 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.3506 Acc: 0.8540 | Val Loss: 0.3433 Acc: 0.8139\n",
      "Epoch 19/25 | Train Loss: 0.3631 Acc: 0.8066 | Val Loss: 0.3795 Acc: 0.8029\n",
      "Epoch 20/25 | Train Loss: 0.3527 Acc: 0.8285 | Val Loss: 0.3472 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3671 Acc: 0.8175 | Val Loss: 0.3713 Acc: 0.8066\n",
      "Epoch 22/25 | Train Loss: 0.3842 Acc: 0.8066 | Val Loss: 0.3572 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.3510 Acc: 0.8321 | Val Loss: 0.3411 Acc: 0.8066\n",
      "Epoch 24/25 | Train Loss: 0.3729 Acc: 0.8102 | Val Loss: 0.3548 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3392 Acc: 0.8321 | Val Loss: 0.3773 Acc: 0.8102\n",
      "Fold 4 Test Accuracy: 0.8139\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.5943 Acc: 0.6642 | Val Loss: 0.4295 Acc: 0.8102\n",
      "Epoch 2/25 | Train Loss: 0.4297 Acc: 0.7810 | Val Loss: 0.7049 Acc: 0.6022\n",
      "Epoch 3/25 | Train Loss: 0.5107 Acc: 0.7372 | Val Loss: 0.4157 Acc: 0.8175\n",
      "Epoch 4/25 | Train Loss: 0.4736 Acc: 0.7117 | Val Loss: 0.4361 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4499 Acc: 0.7263 | Val Loss: 0.5513 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.4705 Acc: 0.7555 | Val Loss: 0.5007 Acc: 0.6752\n",
      "Epoch 7/25 | Train Loss: 0.3592 Acc: 0.8066 | Val Loss: 0.3842 Acc: 0.8066\n",
      "Epoch 8/25 | Train Loss: 0.3598 Acc: 0.8029 | Val Loss: 0.3507 Acc: 0.8102\n",
      "Epoch 9/25 | Train Loss: 0.3280 Acc: 0.8394 | Val Loss: 0.3507 Acc: 0.8321\n",
      "Epoch 10/25 | Train Loss: 0.3253 Acc: 0.8467 | Val Loss: 0.3565 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3474 Acc: 0.8394 | Val Loss: 0.3517 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3651 Acc: 0.8029 | Val Loss: 0.3759 Acc: 0.8102\n",
      "Epoch 13/25 | Train Loss: 0.3361 Acc: 0.8321 | Val Loss: 0.3724 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3193 Acc: 0.8285 | Val Loss: 0.3564 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3267 Acc: 0.8431 | Val Loss: 0.3463 Acc: 0.8139\n",
      "Epoch 16/25 | Train Loss: 0.2944 Acc: 0.8613 | Val Loss: 0.3352 Acc: 0.8139\n",
      "Epoch 17/25 | Train Loss: 0.3381 Acc: 0.8358 | Val Loss: 0.3397 Acc: 0.8066\n",
      "Epoch 18/25 | Train Loss: 0.3237 Acc: 0.8577 | Val Loss: 0.3248 Acc: 0.8285\n",
      "Epoch 19/25 | Train Loss: 0.3210 Acc: 0.8394 | Val Loss: 0.3500 Acc: 0.8139\n",
      "Epoch 20/25 | Train Loss: 0.3211 Acc: 0.8394 | Val Loss: 0.3425 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3113 Acc: 0.8431 | Val Loss: 0.3613 Acc: 0.7956\n",
      "Epoch 22/25 | Train Loss: 0.3235 Acc: 0.8394 | Val Loss: 0.3320 Acc: 0.8321\n",
      "Epoch 23/25 | Train Loss: 0.3198 Acc: 0.8358 | Val Loss: 0.3516 Acc: 0.8212\n",
      "Epoch 24/25 | Train Loss: 0.3112 Acc: 0.8358 | Val Loss: 0.3533 Acc: 0.8102\n",
      "Epoch 25/25 | Train Loss: 0.3204 Acc: 0.8431 | Val Loss: 0.3492 Acc: 0.8139\n",
      "Fold 5 Test Accuracy: 0.8066\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5850 Acc: 0.6825 | Val Loss: 0.4141 Acc: 0.8248\n",
      "Epoch 2/25 | Train Loss: 0.4522 Acc: 0.7774 | Val Loss: 0.4737 Acc: 0.6971\n",
      "Epoch 3/25 | Train Loss: 0.4603 Acc: 0.7555 | Val Loss: 0.5238 Acc: 0.6241\n",
      "Epoch 4/25 | Train Loss: 0.4423 Acc: 0.7701 | Val Loss: 0.4372 Acc: 0.7445\n",
      "Epoch 5/25 | Train Loss: 0.4244 Acc: 0.7883 | Val Loss: 0.4805 Acc: 0.7956\n",
      "Epoch 6/25 | Train Loss: 0.4403 Acc: 0.7847 | Val Loss: 0.3781 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.4184 Acc: 0.7810 | Val Loss: 0.4153 Acc: 0.8212\n",
      "Epoch 8/25 | Train Loss: 0.3749 Acc: 0.8139 | Val Loss: 0.4412 Acc: 0.7810\n",
      "Epoch 9/25 | Train Loss: 0.3540 Acc: 0.8321 | Val Loss: 0.3736 Acc: 0.7883\n",
      "Epoch 10/25 | Train Loss: 0.3661 Acc: 0.8066 | Val Loss: 0.3907 Acc: 0.7920\n",
      "Epoch 11/25 | Train Loss: 0.3589 Acc: 0.8212 | Val Loss: 0.3938 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3809 Acc: 0.7993 | Val Loss: 0.3839 Acc: 0.7737\n",
      "Epoch 13/25 | Train Loss: 0.3813 Acc: 0.8102 | Val Loss: 0.3720 Acc: 0.8066\n",
      "Epoch 14/25 | Train Loss: 0.3537 Acc: 0.8358 | Val Loss: 0.3925 Acc: 0.8029\n",
      "Epoch 15/25 | Train Loss: 0.3451 Acc: 0.8467 | Val Loss: 0.4034 Acc: 0.8066\n",
      "Epoch 16/25 | Train Loss: 0.3495 Acc: 0.8175 | Val Loss: 0.4028 Acc: 0.7920\n",
      "Epoch 17/25 | Train Loss: 0.3434 Acc: 0.8248 | Val Loss: 0.3830 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3538 Acc: 0.8321 | Val Loss: 0.3741 Acc: 0.7993\n",
      "Epoch 19/25 | Train Loss: 0.3419 Acc: 0.8285 | Val Loss: 0.3805 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.3440 Acc: 0.8139 | Val Loss: 0.3973 Acc: 0.8066\n",
      "Epoch 21/25 | Train Loss: 0.3291 Acc: 0.8248 | Val Loss: 0.3707 Acc: 0.8066\n",
      "Epoch 22/25 | Train Loss: 0.3478 Acc: 0.8212 | Val Loss: 0.3943 Acc: 0.7847\n",
      "Epoch 23/25 | Train Loss: 0.3361 Acc: 0.8394 | Val Loss: 0.3632 Acc: 0.8175\n",
      "Epoch 24/25 | Train Loss: 0.3407 Acc: 0.8394 | Val Loss: 0.3841 Acc: 0.7883\n",
      "Epoch 25/25 | Train Loss: 0.3338 Acc: 0.8358 | Val Loss: 0.3768 Acc: 0.8066\n",
      "Fold 6 Test Accuracy: 0.7993\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.5899 Acc: 0.6861 | Val Loss: 0.8139 Acc: 0.5985\n",
      "Epoch 2/25 | Train Loss: 0.5595 Acc: 0.7372 | Val Loss: 0.5833 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4568 Acc: 0.7409 | Val Loss: 0.3507 Acc: 0.8066\n",
      "Epoch 4/25 | Train Loss: 0.4189 Acc: 0.7628 | Val Loss: 0.3870 Acc: 0.8358\n",
      "Epoch 5/25 | Train Loss: 0.4402 Acc: 0.7628 | Val Loss: 0.3695 Acc: 0.7956\n",
      "Epoch 6/25 | Train Loss: 0.4108 Acc: 0.8139 | Val Loss: 0.3993 Acc: 0.7664\n",
      "Epoch 7/25 | Train Loss: 0.4351 Acc: 0.7518 | Val Loss: 0.4483 Acc: 0.7226\n",
      "Epoch 8/25 | Train Loss: 0.3571 Acc: 0.8394 | Val Loss: 0.3289 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3176 Acc: 0.8467 | Val Loss: 0.2929 Acc: 0.8686\n",
      "Epoch 10/25 | Train Loss: 0.3136 Acc: 0.8394 | Val Loss: 0.2701 Acc: 0.8723\n",
      "Epoch 11/25 | Train Loss: 0.3156 Acc: 0.8285 | Val Loss: 0.3069 Acc: 0.8394\n",
      "Epoch 12/25 | Train Loss: 0.3048 Acc: 0.8577 | Val Loss: 0.2563 Acc: 0.8759\n",
      "Epoch 13/25 | Train Loss: 0.2953 Acc: 0.8759 | Val Loss: 0.2947 Acc: 0.8504\n",
      "Epoch 14/25 | Train Loss: 0.2637 Acc: 0.9015 | Val Loss: 0.2402 Acc: 0.8905\n",
      "Epoch 15/25 | Train Loss: 0.3054 Acc: 0.8394 | Val Loss: 0.2833 Acc: 0.8613\n",
      "Epoch 16/25 | Train Loss: 0.2736 Acc: 0.8796 | Val Loss: 0.2727 Acc: 0.8723\n",
      "Epoch 17/25 | Train Loss: 0.2797 Acc: 0.8796 | Val Loss: 0.2652 Acc: 0.8723\n",
      "Epoch 18/25 | Train Loss: 0.2731 Acc: 0.8869 | Val Loss: 0.2552 Acc: 0.8978\n",
      "Epoch 19/25 | Train Loss: 0.2673 Acc: 0.8759 | Val Loss: 0.2517 Acc: 0.8723\n",
      "Epoch 20/25 | Train Loss: 0.2799 Acc: 0.8759 | Val Loss: 0.2385 Acc: 0.8978\n",
      "Epoch 21/25 | Train Loss: 0.2725 Acc: 0.8686 | Val Loss: 0.2621 Acc: 0.8942\n",
      "Epoch 22/25 | Train Loss: 0.2636 Acc: 0.8869 | Val Loss: 0.2450 Acc: 0.8832\n",
      "Epoch 23/25 | Train Loss: 0.2962 Acc: 0.8796 | Val Loss: 0.2645 Acc: 0.8832\n",
      "Epoch 24/25 | Train Loss: 0.2915 Acc: 0.8686 | Val Loss: 0.2588 Acc: 0.8650\n",
      "Epoch 25/25 | Train Loss: 0.2799 Acc: 0.8613 | Val Loss: 0.2582 Acc: 0.8832\n",
      "Fold 7 Test Accuracy: 0.8796\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.5088 Acc: 0.7080 | Val Loss: 0.4851 Acc: 0.6934\n",
      "Epoch 2/25 | Train Loss: 0.3794 Acc: 0.7883 | Val Loss: 0.5258 Acc: 0.7372\n",
      "Epoch 3/25 | Train Loss: 0.3981 Acc: 0.8102 | Val Loss: 0.4378 Acc: 0.7263\n",
      "Epoch 4/25 | Train Loss: 0.3766 Acc: 0.8175 | Val Loss: 0.4820 Acc: 0.7591\n",
      "Epoch 5/25 | Train Loss: 0.4222 Acc: 0.8139 | Val Loss: 0.3946 Acc: 0.8212\n",
      "Epoch 6/25 | Train Loss: 0.3668 Acc: 0.8467 | Val Loss: 0.3669 Acc: 0.8175\n",
      "Epoch 7/25 | Train Loss: 0.4022 Acc: 0.8029 | Val Loss: 0.3997 Acc: 0.7810\n",
      "Epoch 8/25 | Train Loss: 0.3098 Acc: 0.8467 | Val Loss: 0.3772 Acc: 0.8066\n",
      "Epoch 9/25 | Train Loss: 0.3420 Acc: 0.8321 | Val Loss: 0.3644 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3054 Acc: 0.8650 | Val Loss: 0.4246 Acc: 0.7847\n",
      "Epoch 11/25 | Train Loss: 0.3362 Acc: 0.8540 | Val Loss: 0.3571 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3084 Acc: 0.8686 | Val Loss: 0.4016 Acc: 0.7847\n",
      "Epoch 13/25 | Train Loss: 0.2869 Acc: 0.8650 | Val Loss: 0.3934 Acc: 0.7810\n",
      "Epoch 14/25 | Train Loss: 0.2995 Acc: 0.8577 | Val Loss: 0.3843 Acc: 0.7993\n",
      "Epoch 15/25 | Train Loss: 0.2754 Acc: 0.8723 | Val Loss: 0.3480 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.2762 Acc: 0.8869 | Val Loss: 0.3543 Acc: 0.8175\n",
      "Epoch 17/25 | Train Loss: 0.2708 Acc: 0.8686 | Val Loss: 0.3582 Acc: 0.8102\n",
      "Epoch 18/25 | Train Loss: 0.2752 Acc: 0.8759 | Val Loss: 0.3836 Acc: 0.7956\n",
      "Epoch 19/25 | Train Loss: 0.2695 Acc: 0.8942 | Val Loss: 0.3773 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.2702 Acc: 0.8978 | Val Loss: 0.3812 Acc: 0.8212\n",
      "Epoch 21/25 | Train Loss: 0.3040 Acc: 0.8540 | Val Loss: 0.3722 Acc: 0.8029\n",
      "Epoch 22/25 | Train Loss: 0.2756 Acc: 0.8723 | Val Loss: 0.3673 Acc: 0.7810\n",
      "Epoch 23/25 | Train Loss: 0.2685 Acc: 0.8869 | Val Loss: 0.3905 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2693 Acc: 0.8869 | Val Loss: 0.3832 Acc: 0.7920\n",
      "Epoch 25/25 | Train Loss: 0.2825 Acc: 0.8759 | Val Loss: 0.3576 Acc: 0.8102\n",
      "Fold 8 Test Accuracy: 0.7993\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5337 Acc: 0.7153 | Val Loss: 0.7934 Acc: 0.5949\n",
      "Epoch 2/25 | Train Loss: 0.5189 Acc: 0.7482 | Val Loss: 0.4222 Acc: 0.8102\n",
      "Epoch 3/25 | Train Loss: 0.4232 Acc: 0.8029 | Val Loss: 0.3766 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4676 Acc: 0.7591 | Val Loss: 0.4072 Acc: 0.7883\n",
      "Epoch 5/25 | Train Loss: 0.4112 Acc: 0.7810 | Val Loss: 0.3781 Acc: 0.8175\n",
      "Epoch 6/25 | Train Loss: 0.3486 Acc: 0.8248 | Val Loss: 0.3807 Acc: 0.8248\n",
      "Epoch 7/25 | Train Loss: 0.3780 Acc: 0.7993 | Val Loss: 0.4476 Acc: 0.7482\n",
      "Epoch 8/25 | Train Loss: 0.3287 Acc: 0.8394 | Val Loss: 0.3444 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.2971 Acc: 0.8577 | Val Loss: 0.3317 Acc: 0.8504\n",
      "Epoch 10/25 | Train Loss: 0.3227 Acc: 0.8358 | Val Loss: 0.3271 Acc: 0.8175\n",
      "Epoch 11/25 | Train Loss: 0.2798 Acc: 0.8723 | Val Loss: 0.3134 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.2800 Acc: 0.8613 | Val Loss: 0.3138 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.2755 Acc: 0.8759 | Val Loss: 0.3506 Acc: 0.8175\n",
      "Epoch 14/25 | Train Loss: 0.2900 Acc: 0.8650 | Val Loss: 0.3233 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.2717 Acc: 0.8869 | Val Loss: 0.3236 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.2757 Acc: 0.8796 | Val Loss: 0.3215 Acc: 0.8285\n",
      "Epoch 17/25 | Train Loss: 0.2685 Acc: 0.8723 | Val Loss: 0.3214 Acc: 0.8504\n",
      "Epoch 18/25 | Train Loss: 0.2910 Acc: 0.8504 | Val Loss: 0.3093 Acc: 0.8577\n",
      "Epoch 19/25 | Train Loss: 0.2628 Acc: 0.8942 | Val Loss: 0.3099 Acc: 0.8577\n",
      "Epoch 20/25 | Train Loss: 0.2680 Acc: 0.8686 | Val Loss: 0.3284 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.2481 Acc: 0.8942 | Val Loss: 0.3250 Acc: 0.8358\n",
      "Epoch 22/25 | Train Loss: 0.2963 Acc: 0.8686 | Val Loss: 0.3098 Acc: 0.8394\n",
      "Epoch 23/25 | Train Loss: 0.2375 Acc: 0.8978 | Val Loss: 0.3404 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.2685 Acc: 0.8905 | Val Loss: 0.2906 Acc: 0.8540\n",
      "Epoch 25/25 | Train Loss: 0.2957 Acc: 0.8759 | Val Loss: 0.3627 Acc: 0.8139\n",
      "Fold 9 Test Accuracy: 0.8394\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5259 Acc: 0.7044 | Val Loss: 0.5077 Acc: 0.6679\n",
      "Epoch 2/25 | Train Loss: 0.4946 Acc: 0.7810 | Val Loss: 0.4711 Acc: 0.7080\n",
      "Epoch 3/25 | Train Loss: 0.4242 Acc: 0.7810 | Val Loss: 0.4685 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4849 Acc: 0.7810 | Val Loss: 0.4490 Acc: 0.8102\n",
      "Epoch 5/25 | Train Loss: 0.4172 Acc: 0.7956 | Val Loss: 0.4375 Acc: 0.7263\n",
      "Epoch 6/25 | Train Loss: 0.4006 Acc: 0.7993 | Val Loss: 0.4210 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.4832 Acc: 0.7591 | Val Loss: 0.5054 Acc: 0.7993\n",
      "Epoch 8/25 | Train Loss: 0.4492 Acc: 0.8139 | Val Loss: 0.4413 Acc: 0.7993\n",
      "Epoch 9/25 | Train Loss: 0.3814 Acc: 0.8285 | Val Loss: 0.3844 Acc: 0.8139\n",
      "Epoch 10/25 | Train Loss: 0.3345 Acc: 0.8431 | Val Loss: 0.3622 Acc: 0.8139\n",
      "Epoch 11/25 | Train Loss: 0.3212 Acc: 0.8577 | Val Loss: 0.3889 Acc: 0.8066\n",
      "Epoch 12/25 | Train Loss: 0.3227 Acc: 0.8613 | Val Loss: 0.3688 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3236 Acc: 0.8504 | Val Loss: 0.3605 Acc: 0.8248\n",
      "Epoch 14/25 | Train Loss: 0.3278 Acc: 0.8139 | Val Loss: 0.3547 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3243 Acc: 0.8358 | Val Loss: 0.3378 Acc: 0.8358\n",
      "Epoch 16/25 | Train Loss: 0.3157 Acc: 0.8431 | Val Loss: 0.3553 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3227 Acc: 0.8504 | Val Loss: 0.3808 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3091 Acc: 0.8467 | Val Loss: 0.3588 Acc: 0.8066\n",
      "Epoch 19/25 | Train Loss: 0.3462 Acc: 0.8321 | Val Loss: 0.3449 Acc: 0.8321\n",
      "Epoch 20/25 | Train Loss: 0.3421 Acc: 0.8358 | Val Loss: 0.3282 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.3192 Acc: 0.8540 | Val Loss: 0.3322 Acc: 0.8212\n",
      "Epoch 22/25 | Train Loss: 0.3285 Acc: 0.8212 | Val Loss: 0.3745 Acc: 0.8066\n",
      "Epoch 23/25 | Train Loss: 0.3219 Acc: 0.8431 | Val Loss: 0.3541 Acc: 0.8285\n",
      "Epoch 24/25 | Train Loss: 0.3104 Acc: 0.8431 | Val Loss: 0.3317 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3381 Acc: 0.8358 | Val Loss: 0.3655 Acc: 0.8175\n",
      "Fold 10 Test Accuracy: 0.8066\n",
      "\n",
      "5√ó2 CV results: Mean Acc = 0.8241, Std Dev = 0.0262\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T17:42:01.006555Z",
     "start_time": "2025-05-18T16:22:26.805137Z"
    }
   },
   "id": "17aece9678433731",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffeb30fe25b0420b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def res_net_training(output_dir, oversampler=None, dataset_type='original', epochs=25, batch_size=16, n_splits=2, n_repeats=5, random_state=42):\n",
    "    path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "    # download or mount your Kaggle data however you like; suppose:\n",
    "    # path = \".../capsule-defects\"\n",
    "    pos_folder = os.path.join(path, \"capsule/positive\")\n",
    "    neg_folder = os.path.join(path, \"capsule/negative\")\n",
    "    \n",
    "    max_per_dir = None\n",
    "\n",
    "    if oversampler==None:\n",
    "        neg_dirs=[neg_folder]\n",
    "        save_to=\"\"\n",
    "    else:\n",
    "        sample_folder = f'generated_images/{oversampler}'\n",
    "        save_to=f\"{oversampler}/\"\n",
    "        if dataset_type == 'oversampled':\n",
    "            neg_dirs=[neg_folder, sample_folder]\n",
    "            max_per_dir=110\n",
    "        elif dataset_type == 'over_oversampled':\n",
    "            neg_dirs=[neg_folder, sample_folder]\n",
    "            max_per_dir=220\n",
    "        elif dataset_type == 'synthetic':\n",
    "            neg_dirs=[sample_folder]\n",
    "            max_per_dir=219\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset type.\")\n",
    "    \n",
    "    dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_dirs, max_per_dir=max_per_dir, transform=transform)\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "    rskf = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    all_fold_results = []\n",
    "    \n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "        print(f\"===== Fold {fold_idx} =====\")\n",
    "    \n",
    "        # Subset + DataLoader\n",
    "        train_ds = Subset(dataset, train_idx)\n",
    "        test_ds  = Subset(dataset, test_idx)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        trainer = ResNetTrainer()\n",
    "    \n",
    "        trainer.train(\n",
    "            train_loader,\n",
    "            val_loader=test_loader,\n",
    "            num_epochs=epochs,\n",
    "            save_best_to=f\"{output_dir}{dataset_type}_dataset/{save_to}best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "        )\n",
    "    \n",
    "        trainer.load_model(f\"{output_dir}{dataset_type}_dataset/{save_to}best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "        _, acc = trainer.validate(test_loader)\n",
    "        print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "    \n",
    "        all_fold_results.append(acc)\n",
    "    \n",
    "    mean_acc = np.mean(all_fold_results)\n",
    "    std_acc  = np.std(all_fold_results, ddof=1)\n",
    "    print(f\"\\n5√ó2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef2038393f71cf3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "oversamplers = [\n",
    "    'CNNGAN',\n",
    "    'GAN',\n",
    "    'CNNVAE',\n",
    "    'VAE'\n",
    "]\n",
    "\n",
    "dataset_types = [\n",
    "    'oversampled',\n",
    "    'synthetic',\n",
    "    'over_oversampled',\n",
    "]\n",
    "\n",
    "# original dataset\n",
    "res_net_training('res_net_model/')\n",
    "\n",
    "# other datasets\n",
    "for oversampler in oversamplers:\n",
    "    for dataset_type in dataset_types:\n",
    "        res_net_training('res_net_model/', oversampler=oversampler, dataset_type=dataset_type)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5cd4f801f099b02",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
