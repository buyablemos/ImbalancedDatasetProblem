{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:51:42.908605Z",
     "start_time": "2025-05-18T11:51:38.694982Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from torchvision.utils import save_image\n",
    "from PIL import ImageFilter\n",
    "from torch.optim.lr_scheduler import CyclicLR,ReduceLROnPlateau\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from CNNVAE import CNNVAE\n",
    "from ResNet34 import ResNetTrainer\n",
    "from VAE import VAE\n",
    "\n",
    "from GAN import GAN\n",
    "from CNNGAN import CNNGAN"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:51:42.939222Z",
     "start_time": "2025-05-18T11:51:42.909609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parametry modelu\n",
    "IMG_SIZE = 128\n",
    "CHANNELS = 3\n",
    "LATENT_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 1000  # Liczba epok bez poprawy, po których trening się zatrzyma\n",
    "# Konfiguracja\n",
    "result_dir = 'results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/GAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNGAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/VAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNVAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "name = 'cnnvae'\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(f\"Training device: {device}\")"
   ],
   "id": "62222097ab218943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "id": "45054a3b231c03ba"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.2.2+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp312-cp312-win_amd64.whl (2454.8 MB)\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB 21.6 MB/s eta 0:01:54\n",
      "     ---------------------------------------- 0.0/2.5 GB 35.2 MB/s eta 0:01:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 44.4 MB/s eta 0:00:55\n",
      "      --------------------------------------- 0.0/2.5 GB 46.4 MB/s eta 0:00:53\n",
      "      --------------------------------------- 0.0/2.5 GB 48.4 MB/s eta 0:00:50\n",
      "      --------------------------------------- 0.1/2.5 GB 49.7 MB/s eta 0:00:49\n",
      "     - -------------------------------------- 0.1/2.5 GB 50.3 MB/s eta 0:00:48\n",
      "     - -------------------------------------- 0.1/2.5 GB 50.7 MB/s eta 0:00:47\n",
      "     - -------------------------------------- 0.1/2.5 GB 51.2 MB/s eta 0:00:47\n",
      "     - -------------------------------------- 0.1/2.5 GB 51.2 MB/s eta 0:00:46\n",
      "     - -------------------------------------- 0.1/2.5 GB 51.2 MB/s eta 0:00:46\n",
      "     -- ------------------------------------- 0.1/2.5 GB 51.7 MB/s eta 0:00:46\n",
      "     -- ------------------------------------- 0.1/2.5 GB 52.1 MB/s eta 0:00:45\n",
      "     -- ------------------------------------- 0.2/2.5 GB 52.8 MB/s eta 0:00:44\n",
      "     -- ------------------------------------- 0.2/2.5 GB 53.9 MB/s eta 0:00:43\n",
      "     -- ------------------------------------- 0.2/2.5 GB 54.5 MB/s eta 0:00:42\n",
      "     --- ------------------------------------ 0.2/2.5 GB 54.5 MB/s eta 0:00:42\n",
      "     --- ------------------------------------ 0.2/2.5 GB 55.2 MB/s eta 0:00:41\n",
      "     --- ------------------------------------ 0.2/2.5 GB 55.9 MB/s eta 0:00:41\n",
      "     --- ------------------------------------ 0.2/2.5 GB 56.3 MB/s eta 0:00:40\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 56.8 MB/s eta 0:00:39\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 57.2 MB/s eta 0:00:39\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 59.0 MB/s eta 0:00:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 59.7 MB/s eta 0:00:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 60.7 MB/s eta 0:00:36\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 60.1 MB/s eta 0:00:36\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 61.0 MB/s eta 0:00:35\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 61.4 MB/s eta 0:00:35\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 61.6 MB/s eta 0:00:35\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 61.8 MB/s eta 0:00:34\n",
      "     ------ --------------------------------- 0.4/2.5 GB 62.8 MB/s eta 0:00:34\n",
      "     ------ --------------------------------- 0.4/2.5 GB 63.3 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 0.4/2.5 GB 63.7 MB/s eta 0:00:33\n",
      "     ------ --------------------------------- 0.4/2.5 GB 63.7 MB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.4/2.5 GB 63.7 MB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.5 GB 64.7 MB/s eta 0:00:31\n",
      "     ------- -------------------------------- 0.5/2.5 GB 65.0 MB/s eta 0:00:31\n",
      "     ------- -------------------------------- 0.5/2.5 GB 65.0 MB/s eta 0:00:31\n",
      "     -------- ------------------------------- 0.5/2.5 GB 65.2 MB/s eta 0:00:31\n",
      "     -------- ------------------------------- 0.5/2.5 GB 65.5 MB/s eta 0:00:30\n",
      "     -------- ------------------------------- 0.5/2.5 GB 66.0 MB/s eta 0:00:30\n",
      "     -------- ------------------------------- 0.5/2.5 GB 66.2 MB/s eta 0:00:29\n",
      "     --------- ------------------------------ 0.6/2.5 GB 66.0 MB/s eta 0:00:29\n",
      "     --------- ------------------------------ 0.6/2.5 GB 67.0 MB/s eta 0:00:29\n",
      "     --------- ------------------------------ 0.6/2.5 GB 67.0 MB/s eta 0:00:28\n",
      "     --------- ------------------------------ 0.6/2.5 GB 66.8 MB/s eta 0:00:28\n",
      "     --------- ------------------------------ 0.6/2.5 GB 67.3 MB/s eta 0:00:28\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 67.3 MB/s eta 0:00:28\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 67.3 MB/s eta 0:00:28\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 62.3 MB/s eta 0:00:30\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 62.3 MB/s eta 0:00:30\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 62.3 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 62.3 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 62.3 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 62.3 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 61.4 MB/s eta 0:00:29\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 61.2 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 0.7/2.5 GB 60.3 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 0.8/2.5 GB 59.6 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 0.8/2.5 GB 58.8 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 0.8/2.5 GB 58.0 MB/s eta 0:00:29\n",
      "     ------------ --------------------------- 0.8/2.5 GB 57.4 MB/s eta 0:00:30\n",
      "     ------------ --------------------------- 0.8/2.5 GB 56.6 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 0.8/2.5 GB 56.1 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 0.8/2.5 GB 55.5 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 0.8/2.5 GB 55.5 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 0.9/2.5 GB 55.1 MB/s eta 0:00:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 55.5 MB/s eta 0:00:29\n",
      "     -------------- ------------------------- 0.9/2.5 GB 55.1 MB/s eta 0:00:29\n",
      "     -------------- ------------------------- 0.9/2.5 GB 58.8 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 0.9/2.5 GB 58.6 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 0.9/2.5 GB 58.4 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 0.9/2.5 GB 58.6 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 0.9/2.5 GB 58.4 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 1.0/2.5 GB 58.6 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 1.0/2.5 GB 59.0 MB/s eta 0:00:26\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 59.6 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 61.2 MB/s eta 0:00:24\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 61.6 MB/s eta 0:00:24\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 62.5 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 63.5 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 64.7 MB/s eta 0:00:22\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 65.5 MB/s eta 0:00:21\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 70.4 MB/s eta 0:00:20\n",
      "     ------------------ --------------------- 1.1/2.5 GB 67.6 MB/s eta 0:00:20\n",
      "     ------------------ --------------------- 1.1/2.5 GB 67.6 MB/s eta 0:00:20\n",
      "     ------------------ --------------------- 1.1/2.5 GB 69.3 MB/s eta 0:00:19\n",
      "     ------------------ --------------------- 1.2/2.5 GB 69.8 MB/s eta 0:00:19\n",
      "     ------------------- -------------------- 1.2/2.5 GB 69.5 MB/s eta 0:00:19\n",
      "     ------------------- -------------------- 1.2/2.5 GB 69.5 MB/s eta 0:00:19\n",
      "     ------------------- -------------------- 1.2/2.5 GB 69.0 MB/s eta 0:00:19\n",
      "     ------------------- -------------------- 1.2/2.5 GB 69.0 MB/s eta 0:00:19\n",
      "     ------------------- -------------------- 1.2/2.5 GB 69.0 MB/s eta 0:00:18\n",
      "     -------------------- ------------------- 1.2/2.5 GB 69.0 MB/s eta 0:00:18\n",
      "     -------------------- ------------------- 1.3/2.5 GB 69.3 MB/s eta 0:00:18\n",
      "     -------------------- ------------------- 1.3/2.5 GB 69.0 MB/s eta 0:00:18\n",
      "     -------------------- ------------------- 1.3/2.5 GB 69.3 MB/s eta 0:00:18\n",
      "     -------------------- ------------------- 1.3/2.5 GB 65.0 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 1.3/2.5 GB 65.2 MB/s eta 0:00:18\n",
      "     --------------------- ------------------ 1.3/2.5 GB 63.5 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 1.3/2.5 GB 62.3 MB/s eta 0:00:19\n",
      "     --------------------- ------------------ 1.3/2.5 GB 62.8 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 63.0 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 63.0 MB/s eta 0:00:18\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 63.5 MB/s eta 0:00:17\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 63.0 MB/s eta 0:00:17\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 62.1 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 63.0 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 62.8 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 62.5 MB/s eta 0:00:17\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 63.0 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 1.5/2.5 GB 63.5 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 1.5/2.5 GB 63.5 MB/s eta 0:00:16\n",
      "     ------------------------ --------------- 1.5/2.5 GB 63.5 MB/s eta 0:00:15\n",
      "     ------------------------ --------------- 1.5/2.5 GB 64.0 MB/s eta 0:00:15\n",
      "     ------------------------- -------------- 1.5/2.5 GB 67.6 MB/s eta 0:00:14\n",
      "     ------------------------- -------------- 1.6/2.5 GB 67.0 MB/s eta 0:00:14\n",
      "     ------------------------- -------------- 1.6/2.5 GB 69.6 MB/s eta 0:00:13\n",
      "     ------------------------- -------------- 1.6/2.5 GB 69.3 MB/s eta 0:00:13\n",
      "     -------------------------- ------------- 1.6/2.5 GB 69.2 MB/s eta 0:00:13\n",
      "     -------------------------- ------------- 1.6/2.5 GB 69.3 MB/s eta 0:00:13\n",
      "     -------------------------- ------------- 1.6/2.5 GB 70.7 MB/s eta 0:00:12\n",
      "     -------------------------- ------------- 1.6/2.5 GB 69.8 MB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.7/2.5 GB 70.7 MB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.7/2.5 GB 70.1 MB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.7/2.5 GB 69.8 MB/s eta 0:00:11\n",
      "     --------------------------- ------------ 1.7/2.5 GB 70.7 MB/s eta 0:00:11\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 70.7 MB/s eta 0:00:11\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 70.4 MB/s eta 0:00:11\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 70.4 MB/s eta 0:00:10\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 69.8 MB/s eta 0:00:10\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 67.9 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 69.3 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 67.0 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 67.0 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 65.0 MB/s eta 0:00:10\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 63.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.8/2.5 GB 64.5 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.9/2.5 GB 62.3 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.9/2.5 GB 61.4 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.9/2.5 GB 60.3 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.9/2.5 GB 59.7 MB/s eta 0:00:10\n",
      "     ------------------------------ --------- 1.9/2.5 GB 58.8 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 1.9/2.5 GB 59.6 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 1.9/2.5 GB 58.4 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 1.9/2.5 GB 54.4 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 1.9/2.5 GB 54.2 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 1.9/2.5 GB 53.9 MB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.0/2.5 GB 53.2 MB/s eta 0:00:10\n",
      "     -------------------------------- ------- 2.0/2.5 GB 53.4 MB/s eta 0:00:10\n",
      "     -------------------------------- ------- 2.0/2.5 GB 52.9 MB/s eta 0:00:09\n",
      "     -------------------------------- ------- 2.0/2.5 GB 52.5 MB/s eta 0:00:09\n",
      "     -------------------------------- ------- 2.0/2.5 GB 52.5 MB/s eta 0:00:09\n",
      "     --------------------------------- ------ 2.0/2.5 GB 52.9 MB/s eta 0:00:09\n",
      "     --------------------------------- ------ 2.0/2.5 GB 53.4 MB/s eta 0:00:08\n",
      "     --------------------------------- ------ 2.1/2.5 GB 54.4 MB/s eta 0:00:08\n",
      "     --------------------------------- ------ 2.1/2.5 GB 54.6 MB/s eta 0:00:08\n",
      "     --------------------------------- ------ 2.1/2.5 GB 55.1 MB/s eta 0:00:07\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 55.7 MB/s eta 0:00:07\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 56.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 56.8 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 57.6 MB/s eta 0:00:06\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 57.6 MB/s eta 0:00:06\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 58.2 MB/s eta 0:00:06\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 57.6 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 56.1 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 59.4 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 58.8 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 58.2 MB/s eta 0:00:05\n",
      "     ------------------------------------ --- 2.2/2.5 GB 58.4 MB/s eta 0:00:05\n",
      "     ------------------------------------ --- 2.2/2.5 GB 58.0 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 2.2/2.5 GB 57.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 2.3/2.5 GB 56.8 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 2.3/2.5 GB 56.2 MB/s eta 0:00:04\n",
      "     ------------------------------------- -- 2.3/2.5 GB 55.7 MB/s eta 0:00:04\n",
      "     ------------------------------------- -- 2.3/2.5 GB 55.3 MB/s eta 0:00:04\n",
      "     ------------------------------------- -- 2.3/2.5 GB 54.9 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 2.3/2.5 GB 54.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 2.3/2.5 GB 53.7 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 2.3/2.5 GB 53.0 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.3/2.5 GB 52.1 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.3/2.5 GB 52.2 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 51.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.5 GB 51.1 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.5 GB 50.8 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.5 GB 50.3 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 49.7 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 49.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 49.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 51.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 52.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 GB 14.5 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.17.2+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp312-cp312-win_amd64.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     --------------------------------- ------ 4.7/5.7 MB 25.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 21.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2+cu121) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.17.2+cu121) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.17.2+cu121) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.2.2+cu121) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.2.2+cu121) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "Successfully installed torch-2.2.2+cu121 torchvision-0.17.2+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~-rch'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~-rchvision'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.2.2+cu121 torchvision==0.17.2+cu121 --index-url https://download.pytorch.org/whl/cu121"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T22:35:30.535005Z",
     "start_time": "2025-05-17T22:33:17.451202Z"
    }
   },
   "id": "fb11c78706a074a0",
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:51:51.702196Z",
     "start_time": "2025-05-18T11:51:49.405390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CapsuleNegativeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "if CHANNELS==3:\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "else:\n",
    "    normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(1.1, 1.15),fill=255),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Ścieżka do danych \n",
    "\n",
    "path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "\n",
    "dataset = CapsuleNegativeDataset(\n",
    "    root_dir=os.path.join(path, 'capsule/negative'),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Podział danych\n",
    "full_indices = np.arange(len(dataset))\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    full_indices, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_val_indices, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#Print first image\n",
    "data_iter = iter(train_loader)\n",
    "images = next(data_iter)  # \n",
    "\n",
    "image = images[0]\n",
    "\n",
    "image = (image+1)/2\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "image = np.array(image)\n",
    "print(image.shape)\n",
    "\n",
    "plt.imshow(image)\n"
   ],
   "id": "c2cbc3f89ac195e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x21616c70140>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GANLogger:\n",
    "    def __init__(self, save_dir: str, filename: str = \"history\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.filename = filename\n",
    "        # Initialize empty history\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"epoch\": [],\n",
    "            \"gen_loss\": [],\n",
    "            \"disc_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "        }\n",
    "\n",
    "    def log(self, epoch: int, gen_loss: float, disc_loss: float, val_loss: float):\n",
    "        \"\"\"Append one epoch’s metrics.\"\"\"\n",
    "        self.history[\"epoch\"].append(epoch)\n",
    "        self.history[\"gen_loss\"].append(gen_loss)\n",
    "        self.history[\"disc_loss\"].append(disc_loss)\n",
    "        self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    def save_json(self):\n",
    "        \"\"\"Dump the history to a JSON file.\"\"\"\n",
    "        json_path = self.save_dir / f\"{self.filename}.json\"\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T18:00:08.151145Z",
     "start_time": "2025-05-17T18:00:08.145646Z"
    }
   },
   "id": "577d6982f00cf188",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VAELogger:\n",
    "    def __init__(self, save_dir: str, filename: str = \"history\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.filename = filename\n",
    "        # Initialize empty history\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"epoch\": [],\n",
    "            \"train_loss\": [],\n",
    "            \"validation_loss\": [],\n",
    "        }\n",
    "\n",
    "    def log(self, epoch: int, train_loss: float, val_loss: float):\n",
    "        \"\"\"Append one epoch’s metrics.\"\"\"\n",
    "        self.history[\"epoch\"].append(epoch)\n",
    "        self.history[\"train_loss\"].append(train_loss)\n",
    "        self.history[\"validation_loss\"].append(val_loss)\n",
    "\n",
    "    def save_json(self):\n",
    "        \"\"\"Dump the history to a JSON file.\"\"\"\n",
    "        json_path = self.save_dir / f\"{self.filename}.json\"\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T18:00:08.158292Z",
     "start_time": "2025-05-17T18:00:08.152264Z"
    }
   },
   "id": "3a2c57e2a592d6f5",
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# VAE"
   ],
   "id": "175252c98d87c348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:00:08.167910Z",
     "start_time": "2025-05-17T18:00:08.159482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BCE_loss = nn.BCELoss(reduction='sum')\n",
    "MSE_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Funkcja strat z modelem, ktory porownuje ekstaktowane cechy \n",
    "def CNN_loss(self,recon_x, x, mu, logvar, perceptual_loss):\n",
    "    # Perceptual Loss dla cech\n",
    "    percep_loss = perceptual_loss(recon_x, x)\n",
    "\n",
    "    # Standardowa strata VAE (BCE + KLD)\n",
    "    bce = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Ważone sumowanie (można dostosować wagi)\n",
    "    return 0.7 * percep_loss + 0.3 * bce + 0.1* kld\n",
    "\n",
    "\n",
    "#Do testowania\n",
    "def loss_tmp(recon_x, x, mu, logvar):\n",
    "    MSE=0\n",
    "    BCE=0\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x,reduction='sum')\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    #recon_x_normalized = (recon_x + 1) / 2  # Przeskalowanie z [-1, 1] na [0, 1]\n",
    "    #x_normalized = (x + 1) / 2\n",
    "    BCE = BCE_loss(recon_x, x)/x.size(0)\n",
    "    #MSE = MSE_loss(recon_x, x)/x.size(0)\n",
    "    \n",
    "    #MSE=MSE/x.size(0)\n",
    "    \n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "\n",
    "    return BCE + MSE + KLD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_from_article(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n"
   ],
   "id": "a2b046eb60913df",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:33:54.773137Z",
     "start_time": "2025-05-14T14:33:54.511587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicjalizacja\n",
    "\n",
    "model = VAE(device=device,result_dir='results/VAE',load_pretrained=True).to(device)\n",
    "\n",
    "#model = CNNVAE(device=device,result_dir='results/CNNVAE',load_pretrained=True).to(device)\n",
    "\n",
    "\n",
    "if isinstance(model,CNNVAE):\n",
    "    result_dir = 'results/CNNVAE'\n",
    "    name = 'cnnvae'\n",
    "\n",
    "\n",
    "if isinstance(model,VAE):\n",
    "    result_dir = 'results/VAE'\n",
    "    name = 'vae'\n",
    "\n",
    "dummy_input = torch.zeros(1, CHANNELS, IMG_SIZE, IMG_SIZE).to(device)  \n",
    "summary(model, input_data=dummy_input)"
   ],
   "id": "882bc931ff30fb55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No VAE checkpoint found.\n"
     ]
    },
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVAE                                      [1, 3, 128, 128]          --\n├─Sequential: 1-1                        [1, 256]                  --\n│    └─Linear: 2-1                       [1, 512]                  25,166,336\n│    └─ReLU: 2-2                         [1, 512]                  --\n│    └─Dropout: 2-3                      [1, 512]                  --\n│    └─Linear: 2-4                       [1, 358]                  183,654\n│    └─ReLU: 2-5                         [1, 358]                  --\n│    └─Dropout: 2-6                      [1, 358]                  --\n│    └─Linear: 2-7                       [1, 256]                  91,904\n│    └─ReLU: 2-8                         [1, 256]                  --\n├─Linear: 1-2                            [1, 64]                   16,448\n├─Linear: 1-3                            [1, 64]                   16,448\n├─Sequential: 1-4                        [1, 49152]                --\n│    └─Linear: 2-9                       [1, 512]                  33,280\n│    └─ReLU: 2-10                        [1, 512]                  --\n│    └─Linear: 2-11                      [1, 49152]                25,214,976\n│    └─Sigmoid: 2-12                     [1, 49152]                --\n==========================================================================================\nTotal params: 50,723,046\nTrainable params: 50,723,046\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 50.72\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 0.41\nParams size (MB): 202.89\nEstimated Total Size (MB): 203.50\n=========================================================================================="
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pętla treningowa VAE"
   ],
   "id": "aff417ff6963f777"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:57:14.129678Z",
     "start_time": "2025-05-14T14:34:41.743373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Główna pętla\n",
    "best_loss = float('inf')\n",
    "no_improve = 0  # Licznik epok bez poprawy\n",
    "\n",
    "logger = VAELogger(save_dir=\"results/VAE\", filename=\"vae_metrics\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss=model.train_vae(epoch=epoch,dataloader=train_loader,loss_fn=loss_tmp)\n",
    "    validate_loss = model.validate(epoch=epoch,dataloader=train_loader,loss_fn=loss_tmp)\n",
    "    \n",
    "    logger.log(epoch, train_loss, validate_loss)\n",
    "    \n",
    "    # Zapisywanie najlepszego modelu\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        model.visualize_reconstruction(epoch=epoch,dataloader=train_loader)\n",
    "        torch.save(model.state_dict(), f'{result_dir}/{name}_best.pth')\n",
    "        no_improve = 0  \n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f'\\nEarly stopping after {PATIENCE} epochs without improvement')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss = {train_loss:.4f} (Best: {best_loss:.4f})')\n",
    "    \n",
    "logger.save_json()    "
   ],
   "id": "5cf3ac85ec700e8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Loss: 2244.6702\n",
      "Validate Epoch: 1 | Loss: 1967.4310\n",
      "Epoch 1: Train Loss = 2244.6702 (Best: 2244.6702)\n",
      "Train Epoch: 2 | Loss: 1973.8148\n",
      "Validate Epoch: 2 | Loss: 1935.6707\n",
      "Epoch 2: Train Loss = 1973.8148 (Best: 1973.8148)\n",
      "Train Epoch: 3 | Loss: 1925.2640\n",
      "Validate Epoch: 3 | Loss: 1965.9591\n",
      "Epoch 3: Train Loss = 1925.2640 (Best: 1925.2640)\n",
      "Train Epoch: 4 | Loss: 1944.6928\n",
      "Validate Epoch: 4 | Loss: 2001.7264\n",
      "Epoch 4: Train Loss = 1944.6928 (Best: 1925.2640)\n",
      "Train Epoch: 5 | Loss: 1968.3888\n",
      "Validate Epoch: 5 | Loss: 1861.5697\n",
      "Epoch 5: Train Loss = 1968.3888 (Best: 1925.2640)\n",
      "Train Epoch: 6 | Loss: 1900.4682\n",
      "Validate Epoch: 6 | Loss: 1943.3280\n",
      "Epoch 6: Train Loss = 1900.4682 (Best: 1900.4682)\n",
      "Train Epoch: 7 | Loss: 1953.1483\n",
      "Validate Epoch: 7 | Loss: 1945.0877\n",
      "Epoch 7: Train Loss = 1953.1483 (Best: 1900.4682)\n",
      "Train Epoch: 8 | Loss: 1843.3382\n",
      "Validate Epoch: 8 | Loss: 1885.5591\n",
      "Epoch 8: Train Loss = 1843.3382 (Best: 1843.3382)\n",
      "Train Epoch: 9 | Loss: 1921.7343\n",
      "Validate Epoch: 9 | Loss: 1931.1152\n",
      "Epoch 9: Train Loss = 1921.7343 (Best: 1843.3382)\n",
      "Train Epoch: 10 | Loss: 1844.2552\n",
      "Validate Epoch: 10 | Loss: 1980.2396\n",
      "Epoch 10: Train Loss = 1844.2552 (Best: 1843.3382)\n",
      "Train Epoch: 11 | Loss: 1876.2313\n",
      "Validate Epoch: 11 | Loss: 1884.1282\n",
      "Epoch 11: Train Loss = 1876.2313 (Best: 1843.3382)\n",
      "Train Epoch: 12 | Loss: 1938.5365\n",
      "Validate Epoch: 12 | Loss: 1911.7531\n",
      "Epoch 12: Train Loss = 1938.5365 (Best: 1843.3382)\n",
      "Train Epoch: 13 | Loss: 1846.8775\n",
      "Validate Epoch: 13 | Loss: 1908.2012\n",
      "Epoch 13: Train Loss = 1846.8775 (Best: 1843.3382)\n",
      "Train Epoch: 14 | Loss: 1852.5198\n",
      "Validate Epoch: 14 | Loss: 1880.4372\n",
      "Epoch 14: Train Loss = 1852.5198 (Best: 1843.3382)\n",
      "Train Epoch: 15 | Loss: 1942.2339\n",
      "Validate Epoch: 15 | Loss: 1853.2783\n",
      "Epoch 15: Train Loss = 1942.2339 (Best: 1843.3382)\n",
      "Train Epoch: 16 | Loss: 1847.8250\n",
      "Validate Epoch: 16 | Loss: 1883.9680\n",
      "Epoch 16: Train Loss = 1847.8250 (Best: 1843.3382)\n",
      "Train Epoch: 17 | Loss: 1858.5013\n",
      "Validate Epoch: 17 | Loss: 1879.3778\n",
      "Epoch 17: Train Loss = 1858.5013 (Best: 1843.3382)\n",
      "Train Epoch: 18 | Loss: 1890.0542\n",
      "Validate Epoch: 18 | Loss: 1835.5639\n",
      "Epoch 18: Train Loss = 1890.0542 (Best: 1843.3382)\n",
      "Train Epoch: 19 | Loss: 1905.9009\n",
      "Validate Epoch: 19 | Loss: 1960.2360\n",
      "Epoch 19: Train Loss = 1905.9009 (Best: 1843.3382)\n",
      "Train Epoch: 20 | Loss: 1887.0960\n",
      "Validate Epoch: 20 | Loss: 1827.6995\n",
      "Epoch 20: Train Loss = 1887.0960 (Best: 1843.3382)\n",
      "Train Epoch: 21 | Loss: 1942.5163\n",
      "Validate Epoch: 21 | Loss: 1893.0157\n",
      "Epoch 21: Train Loss = 1942.5163 (Best: 1843.3382)\n",
      "Train Epoch: 22 | Loss: 1871.7219\n",
      "Validate Epoch: 22 | Loss: 1896.8968\n",
      "Epoch 22: Train Loss = 1871.7219 (Best: 1843.3382)\n",
      "Train Epoch: 23 | Loss: 1936.4809\n",
      "Validate Epoch: 23 | Loss: 1877.2455\n",
      "Epoch 23: Train Loss = 1936.4809 (Best: 1843.3382)\n",
      "Train Epoch: 24 | Loss: 1942.0384\n",
      "Validate Epoch: 24 | Loss: 1859.9645\n",
      "Epoch 24: Train Loss = 1942.0384 (Best: 1843.3382)\n",
      "Train Epoch: 25 | Loss: 1922.7041\n",
      "Validate Epoch: 25 | Loss: 1981.1987\n",
      "Epoch 25: Train Loss = 1922.7041 (Best: 1843.3382)\n",
      "Train Epoch: 26 | Loss: 1944.0388\n",
      "Validate Epoch: 26 | Loss: 1863.4320\n",
      "Epoch 26: Train Loss = 1944.0388 (Best: 1843.3382)\n",
      "Train Epoch: 27 | Loss: 1903.4125\n",
      "Validate Epoch: 27 | Loss: 1891.6063\n",
      "Epoch 27: Train Loss = 1903.4125 (Best: 1843.3382)\n",
      "Train Epoch: 28 | Loss: 1818.6956\n",
      "Validate Epoch: 28 | Loss: 1902.9715\n",
      "Epoch 28: Train Loss = 1818.6956 (Best: 1818.6956)\n",
      "Train Epoch: 29 | Loss: 1879.5805\n",
      "Validate Epoch: 29 | Loss: 1878.9857\n",
      "Epoch 29: Train Loss = 1879.5805 (Best: 1818.6956)\n",
      "Train Epoch: 30 | Loss: 1933.6276\n",
      "Validate Epoch: 30 | Loss: 1808.4309\n",
      "Epoch 30: Train Loss = 1933.6276 (Best: 1818.6956)\n",
      "Train Epoch: 31 | Loss: 1941.6030\n",
      "Validate Epoch: 31 | Loss: 1950.1868\n",
      "Epoch 31: Train Loss = 1941.6030 (Best: 1818.6956)\n",
      "Train Epoch: 32 | Loss: 1830.4213\n",
      "Validate Epoch: 32 | Loss: 1910.4326\n",
      "Epoch 32: Train Loss = 1830.4213 (Best: 1818.6956)\n",
      "Train Epoch: 33 | Loss: 1900.0085\n",
      "Validate Epoch: 33 | Loss: 1941.9497\n",
      "Epoch 33: Train Loss = 1900.0085 (Best: 1818.6956)\n",
      "Train Epoch: 34 | Loss: 1862.0276\n",
      "Validate Epoch: 34 | Loss: 1910.2904\n",
      "Epoch 34: Train Loss = 1862.0276 (Best: 1818.6956)\n",
      "Train Epoch: 35 | Loss: 1810.9758\n",
      "Validate Epoch: 35 | Loss: 1948.1984\n",
      "Epoch 35: Train Loss = 1810.9758 (Best: 1810.9758)\n",
      "Train Epoch: 36 | Loss: 1910.1921\n",
      "Validate Epoch: 36 | Loss: 1851.1325\n",
      "Epoch 36: Train Loss = 1910.1921 (Best: 1810.9758)\n",
      "Train Epoch: 37 | Loss: 1875.5953\n",
      "Validate Epoch: 37 | Loss: 1879.9127\n",
      "Epoch 37: Train Loss = 1875.5953 (Best: 1810.9758)\n",
      "Train Epoch: 38 | Loss: 1884.5269\n",
      "Validate Epoch: 38 | Loss: 2018.9508\n",
      "Epoch 38: Train Loss = 1884.5269 (Best: 1810.9758)\n",
      "Train Epoch: 39 | Loss: 1844.5885\n",
      "Validate Epoch: 39 | Loss: 1808.9783\n",
      "Epoch 39: Train Loss = 1844.5885 (Best: 1810.9758)\n",
      "Train Epoch: 40 | Loss: 1878.8655\n",
      "Validate Epoch: 40 | Loss: 1925.0944\n",
      "Epoch 40: Train Loss = 1878.8655 (Best: 1810.9758)\n",
      "Train Epoch: 41 | Loss: 1897.4247\n",
      "Validate Epoch: 41 | Loss: 1902.5722\n",
      "Epoch 41: Train Loss = 1897.4247 (Best: 1810.9758)\n",
      "Train Epoch: 42 | Loss: 1864.5286\n",
      "Validate Epoch: 42 | Loss: 1858.3524\n",
      "Epoch 42: Train Loss = 1864.5286 (Best: 1810.9758)\n",
      "Train Epoch: 43 | Loss: 1909.9889\n",
      "Validate Epoch: 43 | Loss: 1907.5926\n",
      "Epoch 43: Train Loss = 1909.9889 (Best: 1810.9758)\n",
      "Train Epoch: 44 | Loss: 1917.5857\n",
      "Validate Epoch: 44 | Loss: 1911.6297\n",
      "Epoch 44: Train Loss = 1917.5857 (Best: 1810.9758)\n",
      "Train Epoch: 45 | Loss: 1831.6542\n",
      "Validate Epoch: 45 | Loss: 1864.6536\n",
      "Epoch 45: Train Loss = 1831.6542 (Best: 1810.9758)\n",
      "Train Epoch: 46 | Loss: 1840.2374\n",
      "Validate Epoch: 46 | Loss: 1857.1950\n",
      "Epoch 46: Train Loss = 1840.2374 (Best: 1810.9758)\n",
      "Train Epoch: 47 | Loss: 1916.0164\n",
      "Validate Epoch: 47 | Loss: 1928.5437\n",
      "Epoch 47: Train Loss = 1916.0164 (Best: 1810.9758)\n",
      "Train Epoch: 48 | Loss: 1861.1119\n",
      "Validate Epoch: 48 | Loss: 1807.3254\n",
      "Epoch 48: Train Loss = 1861.1119 (Best: 1810.9758)\n",
      "Train Epoch: 49 | Loss: 1802.9168\n",
      "Validate Epoch: 49 | Loss: 1892.1776\n",
      "Epoch 49: Train Loss = 1802.9168 (Best: 1802.9168)\n",
      "Train Epoch: 50 | Loss: 1801.5006\n",
      "Validate Epoch: 50 | Loss: 1872.5891\n",
      "Epoch 50: Train Loss = 1801.5006 (Best: 1801.5006)\n",
      "Train Epoch: 51 | Loss: 1819.2813\n",
      "Validate Epoch: 51 | Loss: 1868.4813\n",
      "Epoch 51: Train Loss = 1819.2813 (Best: 1801.5006)\n",
      "Train Epoch: 52 | Loss: 1823.4342\n",
      "Validate Epoch: 52 | Loss: 1843.5010\n",
      "Epoch 52: Train Loss = 1823.4342 (Best: 1801.5006)\n",
      "Train Epoch: 53 | Loss: 1931.3000\n",
      "Validate Epoch: 53 | Loss: 1957.1349\n",
      "Epoch 53: Train Loss = 1931.3000 (Best: 1801.5006)\n",
      "Train Epoch: 54 | Loss: 1890.2000\n",
      "Validate Epoch: 54 | Loss: 1815.5692\n",
      "Epoch 54: Train Loss = 1890.2000 (Best: 1801.5006)\n",
      "Train Epoch: 55 | Loss: 1906.9402\n",
      "Validate Epoch: 55 | Loss: 1906.4810\n",
      "Epoch 55: Train Loss = 1906.9402 (Best: 1801.5006)\n",
      "Train Epoch: 56 | Loss: 1844.0406\n",
      "Validate Epoch: 56 | Loss: 1857.0460\n",
      "Epoch 56: Train Loss = 1844.0406 (Best: 1801.5006)\n",
      "Train Epoch: 57 | Loss: 1832.3350\n",
      "Validate Epoch: 57 | Loss: 1898.1720\n",
      "Epoch 57: Train Loss = 1832.3350 (Best: 1801.5006)\n",
      "Train Epoch: 58 | Loss: 1868.1312\n",
      "Validate Epoch: 58 | Loss: 1864.0965\n",
      "Epoch 58: Train Loss = 1868.1312 (Best: 1801.5006)\n",
      "Train Epoch: 59 | Loss: 1845.6418\n",
      "Validate Epoch: 59 | Loss: 1864.8828\n",
      "Epoch 59: Train Loss = 1845.6418 (Best: 1801.5006)\n",
      "Train Epoch: 60 | Loss: 1840.9908\n",
      "Validate Epoch: 60 | Loss: 1883.0239\n",
      "Epoch 60: Train Loss = 1840.9908 (Best: 1801.5006)\n",
      "Train Epoch: 61 | Loss: 1783.7543\n",
      "Validate Epoch: 61 | Loss: 1877.4098\n",
      "Epoch 61: Train Loss = 1783.7543 (Best: 1783.7543)\n",
      "Train Epoch: 62 | Loss: 1822.2012\n",
      "Validate Epoch: 62 | Loss: 1868.6435\n",
      "Epoch 62: Train Loss = 1822.2012 (Best: 1783.7543)\n",
      "Train Epoch: 63 | Loss: 1839.2601\n",
      "Validate Epoch: 63 | Loss: 1855.2359\n",
      "Epoch 63: Train Loss = 1839.2601 (Best: 1783.7543)\n",
      "Train Epoch: 64 | Loss: 1867.7072\n",
      "Validate Epoch: 64 | Loss: 1860.8758\n",
      "Epoch 64: Train Loss = 1867.7072 (Best: 1783.7543)\n",
      "Train Epoch: 65 | Loss: 1865.9021\n",
      "Validate Epoch: 65 | Loss: 1918.4372\n",
      "Epoch 65: Train Loss = 1865.9021 (Best: 1783.7543)\n",
      "Train Epoch: 66 | Loss: 1879.8838\n",
      "Validate Epoch: 66 | Loss: 1826.6384\n",
      "Epoch 66: Train Loss = 1879.8838 (Best: 1783.7543)\n",
      "Train Epoch: 67 | Loss: 1863.5694\n",
      "Validate Epoch: 67 | Loss: 1908.4541\n",
      "Epoch 67: Train Loss = 1863.5694 (Best: 1783.7543)\n",
      "Train Epoch: 68 | Loss: 1793.1670\n",
      "Validate Epoch: 68 | Loss: 1889.9959\n",
      "Epoch 68: Train Loss = 1793.1670 (Best: 1783.7543)\n",
      "Train Epoch: 69 | Loss: 1779.6439\n",
      "Validate Epoch: 69 | Loss: 1902.1202\n",
      "Epoch 69: Train Loss = 1779.6439 (Best: 1779.6439)\n",
      "Train Epoch: 70 | Loss: 1901.4692\n",
      "Validate Epoch: 70 | Loss: 1950.7384\n",
      "Epoch 70: Train Loss = 1901.4692 (Best: 1779.6439)\n",
      "Train Epoch: 71 | Loss: 1827.6037\n",
      "Validate Epoch: 71 | Loss: 1790.6298\n",
      "Epoch 71: Train Loss = 1827.6037 (Best: 1779.6439)\n",
      "Train Epoch: 72 | Loss: 1907.4560\n",
      "Validate Epoch: 72 | Loss: 1962.0512\n",
      "Epoch 72: Train Loss = 1907.4560 (Best: 1779.6439)\n",
      "Train Epoch: 73 | Loss: 1929.7478\n",
      "Validate Epoch: 73 | Loss: 1851.4104\n",
      "Epoch 73: Train Loss = 1929.7478 (Best: 1779.6439)\n",
      "Train Epoch: 74 | Loss: 1908.6633\n",
      "Validate Epoch: 74 | Loss: 1859.8366\n",
      "Epoch 74: Train Loss = 1908.6633 (Best: 1779.6439)\n",
      "Train Epoch: 75 | Loss: 1855.7563\n",
      "Validate Epoch: 75 | Loss: 1909.8303\n",
      "Epoch 75: Train Loss = 1855.7563 (Best: 1779.6439)\n",
      "Train Epoch: 76 | Loss: 1820.5181\n",
      "Validate Epoch: 76 | Loss: 1905.1150\n",
      "Epoch 76: Train Loss = 1820.5181 (Best: 1779.6439)\n",
      "Train Epoch: 77 | Loss: 1828.3921\n",
      "Validate Epoch: 77 | Loss: 1893.4104\n",
      "Epoch 77: Train Loss = 1828.3921 (Best: 1779.6439)\n",
      "Train Epoch: 78 | Loss: 1865.9528\n",
      "Validate Epoch: 78 | Loss: 1801.0403\n",
      "Epoch 78: Train Loss = 1865.9528 (Best: 1779.6439)\n",
      "Train Epoch: 79 | Loss: 1904.4216\n",
      "Validate Epoch: 79 | Loss: 1949.6414\n",
      "Epoch 79: Train Loss = 1904.4216 (Best: 1779.6439)\n",
      "Train Epoch: 80 | Loss: 1875.1756\n",
      "Validate Epoch: 80 | Loss: 1895.9840\n",
      "Epoch 80: Train Loss = 1875.1756 (Best: 1779.6439)\n",
      "Train Epoch: 81 | Loss: 1855.7095\n",
      "Validate Epoch: 81 | Loss: 1859.7339\n",
      "Epoch 81: Train Loss = 1855.7095 (Best: 1779.6439)\n",
      "Train Epoch: 82 | Loss: 1880.6924\n",
      "Validate Epoch: 82 | Loss: 1916.3883\n",
      "Epoch 82: Train Loss = 1880.6924 (Best: 1779.6439)\n",
      "Train Epoch: 83 | Loss: 1940.3726\n",
      "Validate Epoch: 83 | Loss: 1868.7370\n",
      "Epoch 83: Train Loss = 1940.3726 (Best: 1779.6439)\n",
      "Train Epoch: 84 | Loss: 1895.5984\n",
      "Validate Epoch: 84 | Loss: 1872.0806\n",
      "Epoch 84: Train Loss = 1895.5984 (Best: 1779.6439)\n",
      "Train Epoch: 85 | Loss: 1769.6568\n",
      "Validate Epoch: 85 | Loss: 1787.3255\n",
      "Epoch 85: Train Loss = 1769.6568 (Best: 1769.6568)\n",
      "Train Epoch: 86 | Loss: 1813.5888\n",
      "Validate Epoch: 86 | Loss: 1934.0052\n",
      "Epoch 86: Train Loss = 1813.5888 (Best: 1769.6568)\n",
      "Train Epoch: 87 | Loss: 1852.4285\n",
      "Validate Epoch: 87 | Loss: 1818.5769\n",
      "Epoch 87: Train Loss = 1852.4285 (Best: 1769.6568)\n",
      "Train Epoch: 88 | Loss: 1883.0068\n",
      "Validate Epoch: 88 | Loss: 1931.8666\n",
      "Epoch 88: Train Loss = 1883.0068 (Best: 1769.6568)\n",
      "Train Epoch: 89 | Loss: 1845.3191\n",
      "Validate Epoch: 89 | Loss: 1904.7062\n",
      "Epoch 89: Train Loss = 1845.3191 (Best: 1769.6568)\n",
      "Train Epoch: 90 | Loss: 1829.7434\n",
      "Validate Epoch: 90 | Loss: 1889.0590\n",
      "Epoch 90: Train Loss = 1829.7434 (Best: 1769.6568)\n",
      "Train Epoch: 91 | Loss: 1863.0846\n",
      "Validate Epoch: 91 | Loss: 1878.6922\n",
      "Epoch 91: Train Loss = 1863.0846 (Best: 1769.6568)\n",
      "Train Epoch: 92 | Loss: 1800.0886\n",
      "Validate Epoch: 92 | Loss: 1864.1563\n",
      "Epoch 92: Train Loss = 1800.0886 (Best: 1769.6568)\n",
      "Train Epoch: 93 | Loss: 1906.0693\n",
      "Validate Epoch: 93 | Loss: 1924.1325\n",
      "Epoch 93: Train Loss = 1906.0693 (Best: 1769.6568)\n",
      "Train Epoch: 94 | Loss: 1801.8081\n",
      "Validate Epoch: 94 | Loss: 1853.6608\n",
      "Epoch 94: Train Loss = 1801.8081 (Best: 1769.6568)\n",
      "Train Epoch: 95 | Loss: 1875.4004\n",
      "Validate Epoch: 95 | Loss: 1966.9363\n",
      "Epoch 95: Train Loss = 1875.4004 (Best: 1769.6568)\n",
      "Train Epoch: 96 | Loss: 1899.4215\n",
      "Validate Epoch: 96 | Loss: 1871.3358\n",
      "Epoch 96: Train Loss = 1899.4215 (Best: 1769.6568)\n",
      "Train Epoch: 97 | Loss: 1863.0878\n",
      "Validate Epoch: 97 | Loss: 1922.3406\n",
      "Epoch 97: Train Loss = 1863.0878 (Best: 1769.6568)\n",
      "Train Epoch: 98 | Loss: 1799.3704\n",
      "Validate Epoch: 98 | Loss: 1883.0520\n",
      "Epoch 98: Train Loss = 1799.3704 (Best: 1769.6568)\n",
      "Train Epoch: 99 | Loss: 1820.5898\n",
      "Validate Epoch: 99 | Loss: 1891.7681\n",
      "Epoch 99: Train Loss = 1820.5898 (Best: 1769.6568)\n",
      "Train Epoch: 100 | Loss: 1850.8854\n",
      "Validate Epoch: 100 | Loss: 1878.7061\n",
      "Epoch 100: Train Loss = 1850.8854 (Best: 1769.6568)\n",
      "Train Epoch: 101 | Loss: 1859.6890\n",
      "Validate Epoch: 101 | Loss: 1837.6121\n",
      "Epoch 101: Train Loss = 1859.6890 (Best: 1769.6568)\n",
      "Train Epoch: 102 | Loss: 1830.0988\n",
      "Validate Epoch: 102 | Loss: 1804.5602\n",
      "Epoch 102: Train Loss = 1830.0988 (Best: 1769.6568)\n",
      "Train Epoch: 103 | Loss: 1835.1838\n",
      "Validate Epoch: 103 | Loss: 1870.2748\n",
      "Epoch 103: Train Loss = 1835.1838 (Best: 1769.6568)\n",
      "Train Epoch: 104 | Loss: 1841.3739\n",
      "Validate Epoch: 104 | Loss: 1801.1564\n",
      "Epoch 104: Train Loss = 1841.3739 (Best: 1769.6568)\n",
      "Train Epoch: 105 | Loss: 1889.6517\n",
      "Validate Epoch: 105 | Loss: 1903.6480\n",
      "Epoch 105: Train Loss = 1889.6517 (Best: 1769.6568)\n",
      "Train Epoch: 106 | Loss: 1859.4763\n",
      "Validate Epoch: 106 | Loss: 1865.9456\n",
      "Epoch 106: Train Loss = 1859.4763 (Best: 1769.6568)\n",
      "Train Epoch: 107 | Loss: 1853.9769\n",
      "Validate Epoch: 107 | Loss: 1866.6486\n",
      "Epoch 107: Train Loss = 1853.9769 (Best: 1769.6568)\n",
      "Train Epoch: 108 | Loss: 1828.5985\n",
      "Validate Epoch: 108 | Loss: 1849.9908\n",
      "Epoch 108: Train Loss = 1828.5985 (Best: 1769.6568)\n",
      "Train Epoch: 109 | Loss: 1803.1011\n",
      "Validate Epoch: 109 | Loss: 1878.1666\n",
      "Epoch 109: Train Loss = 1803.1011 (Best: 1769.6568)\n",
      "Train Epoch: 110 | Loss: 1821.4322\n",
      "Validate Epoch: 110 | Loss: 1873.4211\n",
      "Epoch 110: Train Loss = 1821.4322 (Best: 1769.6568)\n",
      "Train Epoch: 111 | Loss: 1895.9954\n",
      "Validate Epoch: 111 | Loss: 1865.2359\n",
      "Epoch 111: Train Loss = 1895.9954 (Best: 1769.6568)\n",
      "Train Epoch: 112 | Loss: 1840.4654\n",
      "Validate Epoch: 112 | Loss: 1883.1391\n",
      "Epoch 112: Train Loss = 1840.4654 (Best: 1769.6568)\n",
      "Train Epoch: 113 | Loss: 1759.9006\n",
      "Validate Epoch: 113 | Loss: 1840.3719\n",
      "Epoch 113: Train Loss = 1759.9006 (Best: 1759.9006)\n",
      "Train Epoch: 114 | Loss: 1877.3698\n",
      "Validate Epoch: 114 | Loss: 1941.9684\n",
      "Epoch 114: Train Loss = 1877.3698 (Best: 1759.9006)\n",
      "Train Epoch: 115 | Loss: 1875.0464\n",
      "Validate Epoch: 115 | Loss: 1889.2255\n",
      "Epoch 115: Train Loss = 1875.0464 (Best: 1759.9006)\n",
      "Train Epoch: 116 | Loss: 1824.1100\n",
      "Validate Epoch: 116 | Loss: 1872.2175\n",
      "Epoch 116: Train Loss = 1824.1100 (Best: 1759.9006)\n",
      "Train Epoch: 117 | Loss: 1854.8288\n",
      "Validate Epoch: 117 | Loss: 1928.8378\n",
      "Epoch 117: Train Loss = 1854.8288 (Best: 1759.9006)\n",
      "Train Epoch: 118 | Loss: 1825.7743\n",
      "Validate Epoch: 118 | Loss: 1821.2443\n",
      "Epoch 118: Train Loss = 1825.7743 (Best: 1759.9006)\n",
      "Train Epoch: 119 | Loss: 1817.2704\n",
      "Validate Epoch: 119 | Loss: 1796.3705\n",
      "Epoch 119: Train Loss = 1817.2704 (Best: 1759.9006)\n",
      "Train Epoch: 120 | Loss: 1790.3502\n",
      "Validate Epoch: 120 | Loss: 1898.2502\n",
      "Epoch 120: Train Loss = 1790.3502 (Best: 1759.9006)\n",
      "Train Epoch: 121 | Loss: 1832.2989\n",
      "Validate Epoch: 121 | Loss: 1887.8890\n",
      "Epoch 121: Train Loss = 1832.2989 (Best: 1759.9006)\n",
      "Train Epoch: 122 | Loss: 1819.8185\n",
      "Validate Epoch: 122 | Loss: 1839.6511\n",
      "Epoch 122: Train Loss = 1819.8185 (Best: 1759.9006)\n",
      "Train Epoch: 123 | Loss: 1856.2527\n",
      "Validate Epoch: 123 | Loss: 1913.6228\n",
      "Epoch 123: Train Loss = 1856.2527 (Best: 1759.9006)\n",
      "Train Epoch: 124 | Loss: 1824.2188\n",
      "Validate Epoch: 124 | Loss: 1828.4871\n",
      "Epoch 124: Train Loss = 1824.2188 (Best: 1759.9006)\n",
      "Train Epoch: 125 | Loss: 1812.1043\n",
      "Validate Epoch: 125 | Loss: 1833.2091\n",
      "Epoch 125: Train Loss = 1812.1043 (Best: 1759.9006)\n",
      "Train Epoch: 126 | Loss: 1817.4467\n",
      "Validate Epoch: 126 | Loss: 1884.1834\n",
      "Epoch 126: Train Loss = 1817.4467 (Best: 1759.9006)\n",
      "Train Epoch: 127 | Loss: 1882.0395\n",
      "Validate Epoch: 127 | Loss: 1910.0036\n",
      "Epoch 127: Train Loss = 1882.0395 (Best: 1759.9006)\n",
      "Train Epoch: 128 | Loss: 1820.1887\n",
      "Validate Epoch: 128 | Loss: 1914.1313\n",
      "Epoch 128: Train Loss = 1820.1887 (Best: 1759.9006)\n",
      "Train Epoch: 129 | Loss: 1882.9074\n",
      "Validate Epoch: 129 | Loss: 1905.3280\n",
      "Epoch 129: Train Loss = 1882.9074 (Best: 1759.9006)\n",
      "Train Epoch: 130 | Loss: 1837.9243\n",
      "Validate Epoch: 130 | Loss: 1871.6765\n",
      "Epoch 130: Train Loss = 1837.9243 (Best: 1759.9006)\n",
      "Train Epoch: 131 | Loss: 1855.8555\n",
      "Validate Epoch: 131 | Loss: 1870.4968\n",
      "Epoch 131: Train Loss = 1855.8555 (Best: 1759.9006)\n",
      "Train Epoch: 132 | Loss: 1860.7885\n",
      "Validate Epoch: 132 | Loss: 1856.1913\n",
      "Epoch 132: Train Loss = 1860.7885 (Best: 1759.9006)\n",
      "Train Epoch: 133 | Loss: 1812.3859\n",
      "Validate Epoch: 133 | Loss: 1854.1780\n",
      "Epoch 133: Train Loss = 1812.3859 (Best: 1759.9006)\n",
      "Train Epoch: 134 | Loss: 1807.1285\n",
      "Validate Epoch: 134 | Loss: 1886.3134\n",
      "Epoch 134: Train Loss = 1807.1285 (Best: 1759.9006)\n",
      "Train Epoch: 135 | Loss: 1928.6373\n",
      "Validate Epoch: 135 | Loss: 1887.7653\n",
      "Epoch 135: Train Loss = 1928.6373 (Best: 1759.9006)\n",
      "Train Epoch: 136 | Loss: 1813.0996\n",
      "Validate Epoch: 136 | Loss: 1864.6932\n",
      "Epoch 136: Train Loss = 1813.0996 (Best: 1759.9006)\n",
      "Train Epoch: 137 | Loss: 1840.6260\n",
      "Validate Epoch: 137 | Loss: 1871.7978\n",
      "Epoch 137: Train Loss = 1840.6260 (Best: 1759.9006)\n",
      "Train Epoch: 138 | Loss: 1897.6395\n",
      "Validate Epoch: 138 | Loss: 1794.8525\n",
      "Epoch 138: Train Loss = 1897.6395 (Best: 1759.9006)\n",
      "Train Epoch: 139 | Loss: 1851.4944\n",
      "Validate Epoch: 139 | Loss: 1944.9349\n",
      "Epoch 139: Train Loss = 1851.4944 (Best: 1759.9006)\n",
      "Train Epoch: 140 | Loss: 1888.5438\n",
      "Validate Epoch: 140 | Loss: 1831.1225\n",
      "Epoch 140: Train Loss = 1888.5438 (Best: 1759.9006)\n",
      "Train Epoch: 141 | Loss: 1887.2589\n",
      "Validate Epoch: 141 | Loss: 1875.6146\n",
      "Epoch 141: Train Loss = 1887.2589 (Best: 1759.9006)\n",
      "Train Epoch: 142 | Loss: 1883.9042\n",
      "Validate Epoch: 142 | Loss: 1841.2854\n",
      "Epoch 142: Train Loss = 1883.9042 (Best: 1759.9006)\n",
      "Train Epoch: 143 | Loss: 1796.1995\n",
      "Validate Epoch: 143 | Loss: 1898.0261\n",
      "Epoch 143: Train Loss = 1796.1995 (Best: 1759.9006)\n",
      "Train Epoch: 144 | Loss: 1857.9352\n",
      "Validate Epoch: 144 | Loss: 1824.8145\n",
      "Epoch 144: Train Loss = 1857.9352 (Best: 1759.9006)\n",
      "Train Epoch: 145 | Loss: 1836.6390\n",
      "Validate Epoch: 145 | Loss: 1809.6089\n",
      "Epoch 145: Train Loss = 1836.6390 (Best: 1759.9006)\n",
      "Train Epoch: 146 | Loss: 1818.3518\n",
      "Validate Epoch: 146 | Loss: 1910.9494\n",
      "Epoch 146: Train Loss = 1818.3518 (Best: 1759.9006)\n",
      "Train Epoch: 147 | Loss: 1867.4849\n",
      "Validate Epoch: 147 | Loss: 1920.5391\n",
      "Epoch 147: Train Loss = 1867.4849 (Best: 1759.9006)\n",
      "Train Epoch: 148 | Loss: 1857.3105\n",
      "Validate Epoch: 148 | Loss: 1847.5340\n",
      "Epoch 148: Train Loss = 1857.3105 (Best: 1759.9006)\n",
      "Train Epoch: 149 | Loss: 1863.2979\n",
      "Validate Epoch: 149 | Loss: 1886.5622\n",
      "Epoch 149: Train Loss = 1863.2979 (Best: 1759.9006)\n",
      "Train Epoch: 150 | Loss: 1877.7937\n",
      "Validate Epoch: 150 | Loss: 1838.8935\n",
      "Epoch 150: Train Loss = 1877.7937 (Best: 1759.9006)\n",
      "Train Epoch: 151 | Loss: 1880.8935\n",
      "Validate Epoch: 151 | Loss: 1805.8779\n",
      "Epoch 151: Train Loss = 1880.8935 (Best: 1759.9006)\n",
      "Train Epoch: 152 | Loss: 1860.1000\n",
      "Validate Epoch: 152 | Loss: 1886.0701\n",
      "Epoch 152: Train Loss = 1860.1000 (Best: 1759.9006)\n",
      "Train Epoch: 153 | Loss: 1853.8392\n",
      "Validate Epoch: 153 | Loss: 1922.4895\n",
      "Epoch 153: Train Loss = 1853.8392 (Best: 1759.9006)\n",
      "Train Epoch: 154 | Loss: 1798.2032\n",
      "Validate Epoch: 154 | Loss: 1828.6069\n",
      "Epoch 154: Train Loss = 1798.2032 (Best: 1759.9006)\n",
      "Train Epoch: 155 | Loss: 1870.6049\n",
      "Validate Epoch: 155 | Loss: 1900.3550\n",
      "Epoch 155: Train Loss = 1870.6049 (Best: 1759.9006)\n",
      "Train Epoch: 156 | Loss: 1837.6724\n",
      "Validate Epoch: 156 | Loss: 1804.2381\n",
      "Epoch 156: Train Loss = 1837.6724 (Best: 1759.9006)\n",
      "Train Epoch: 157 | Loss: 1837.7097\n",
      "Validate Epoch: 157 | Loss: 1887.6225\n",
      "Epoch 157: Train Loss = 1837.7097 (Best: 1759.9006)\n",
      "Train Epoch: 158 | Loss: 1839.4677\n",
      "Validate Epoch: 158 | Loss: 1884.4634\n",
      "Epoch 158: Train Loss = 1839.4677 (Best: 1759.9006)\n",
      "Train Epoch: 159 | Loss: 1836.8338\n",
      "Validate Epoch: 159 | Loss: 1832.5256\n",
      "Epoch 159: Train Loss = 1836.8338 (Best: 1759.9006)\n",
      "Train Epoch: 160 | Loss: 1798.8373\n",
      "Validate Epoch: 160 | Loss: 1858.7897\n",
      "Epoch 160: Train Loss = 1798.8373 (Best: 1759.9006)\n",
      "Train Epoch: 161 | Loss: 1875.4877\n",
      "Validate Epoch: 161 | Loss: 1870.7727\n",
      "Epoch 161: Train Loss = 1875.4877 (Best: 1759.9006)\n",
      "Train Epoch: 162 | Loss: 1827.0137\n",
      "Validate Epoch: 162 | Loss: 1865.6729\n",
      "Epoch 162: Train Loss = 1827.0137 (Best: 1759.9006)\n",
      "Train Epoch: 163 | Loss: 1762.8024\n",
      "Validate Epoch: 163 | Loss: 1876.2928\n",
      "Epoch 163: Train Loss = 1762.8024 (Best: 1759.9006)\n",
      "Train Epoch: 164 | Loss: 1832.8127\n",
      "Validate Epoch: 164 | Loss: 1850.1319\n",
      "Epoch 164: Train Loss = 1832.8127 (Best: 1759.9006)\n",
      "Train Epoch: 165 | Loss: 1862.4858\n",
      "Validate Epoch: 165 | Loss: 1815.6537\n",
      "Epoch 165: Train Loss = 1862.4858 (Best: 1759.9006)\n",
      "Train Epoch: 166 | Loss: 1839.6945\n",
      "Validate Epoch: 166 | Loss: 1852.0613\n",
      "Epoch 166: Train Loss = 1839.6945 (Best: 1759.9006)\n",
      "Train Epoch: 167 | Loss: 1794.1527\n",
      "Validate Epoch: 167 | Loss: 1882.7823\n",
      "Epoch 167: Train Loss = 1794.1527 (Best: 1759.9006)\n",
      "Train Epoch: 168 | Loss: 1824.3493\n",
      "Validate Epoch: 168 | Loss: 1813.0289\n",
      "Epoch 168: Train Loss = 1824.3493 (Best: 1759.9006)\n",
      "Train Epoch: 169 | Loss: 1824.8085\n",
      "Validate Epoch: 169 | Loss: 1866.1391\n",
      "Epoch 169: Train Loss = 1824.8085 (Best: 1759.9006)\n",
      "Train Epoch: 170 | Loss: 1824.8770\n",
      "Validate Epoch: 170 | Loss: 1887.1032\n",
      "Epoch 170: Train Loss = 1824.8770 (Best: 1759.9006)\n",
      "Train Epoch: 171 | Loss: 1792.3394\n",
      "Validate Epoch: 171 | Loss: 1811.4158\n",
      "Epoch 171: Train Loss = 1792.3394 (Best: 1759.9006)\n",
      "Train Epoch: 172 | Loss: 1811.8992\n",
      "Validate Epoch: 172 | Loss: 1823.7021\n",
      "Epoch 172: Train Loss = 1811.8992 (Best: 1759.9006)\n",
      "Train Epoch: 173 | Loss: 1931.0643\n",
      "Validate Epoch: 173 | Loss: 1886.2630\n",
      "Epoch 173: Train Loss = 1931.0643 (Best: 1759.9006)\n",
      "Train Epoch: 174 | Loss: 1780.4950\n",
      "Validate Epoch: 174 | Loss: 1837.4123\n",
      "Epoch 174: Train Loss = 1780.4950 (Best: 1759.9006)\n",
      "Train Epoch: 175 | Loss: 1869.6917\n",
      "Validate Epoch: 175 | Loss: 1931.6047\n",
      "Epoch 175: Train Loss = 1869.6917 (Best: 1759.9006)\n",
      "Train Epoch: 176 | Loss: 1809.4075\n",
      "Validate Epoch: 176 | Loss: 1823.6320\n",
      "Epoch 176: Train Loss = 1809.4075 (Best: 1759.9006)\n",
      "Train Epoch: 177 | Loss: 1901.2731\n",
      "Validate Epoch: 177 | Loss: 1871.4314\n",
      "Epoch 177: Train Loss = 1901.2731 (Best: 1759.9006)\n",
      "Train Epoch: 178 | Loss: 1898.1371\n",
      "Validate Epoch: 178 | Loss: 1864.8314\n",
      "Epoch 178: Train Loss = 1898.1371 (Best: 1759.9006)\n",
      "Train Epoch: 179 | Loss: 1877.0206\n",
      "Validate Epoch: 179 | Loss: 1860.3173\n",
      "Epoch 179: Train Loss = 1877.0206 (Best: 1759.9006)\n",
      "Train Epoch: 180 | Loss: 1760.2154\n",
      "Validate Epoch: 180 | Loss: 1878.4371\n",
      "Epoch 180: Train Loss = 1760.2154 (Best: 1759.9006)\n",
      "Train Epoch: 181 | Loss: 1892.0502\n",
      "Validate Epoch: 181 | Loss: 1858.1172\n",
      "Epoch 181: Train Loss = 1892.0502 (Best: 1759.9006)\n",
      "Train Epoch: 182 | Loss: 1840.4730\n",
      "Validate Epoch: 182 | Loss: 1891.1581\n",
      "Epoch 182: Train Loss = 1840.4730 (Best: 1759.9006)\n",
      "Train Epoch: 183 | Loss: 1835.0657\n",
      "Validate Epoch: 183 | Loss: 1868.0305\n",
      "Epoch 183: Train Loss = 1835.0657 (Best: 1759.9006)\n",
      "Train Epoch: 184 | Loss: 1829.3184\n",
      "Validate Epoch: 184 | Loss: 1859.7717\n",
      "Epoch 184: Train Loss = 1829.3184 (Best: 1759.9006)\n",
      "Train Epoch: 185 | Loss: 1895.6140\n",
      "Validate Epoch: 185 | Loss: 1858.4098\n",
      "Epoch 185: Train Loss = 1895.6140 (Best: 1759.9006)\n",
      "Train Epoch: 186 | Loss: 1817.2750\n",
      "Validate Epoch: 186 | Loss: 1858.5774\n",
      "Epoch 186: Train Loss = 1817.2750 (Best: 1759.9006)\n",
      "Train Epoch: 187 | Loss: 1821.6131\n",
      "Validate Epoch: 187 | Loss: 1837.8925\n",
      "Epoch 187: Train Loss = 1821.6131 (Best: 1759.9006)\n",
      "Train Epoch: 188 | Loss: 1738.3732\n",
      "Validate Epoch: 188 | Loss: 1748.7067\n",
      "Epoch 188: Train Loss = 1738.3732 (Best: 1738.3732)\n",
      "Train Epoch: 189 | Loss: 1913.9327\n",
      "Validate Epoch: 189 | Loss: 1910.3203\n",
      "Epoch 189: Train Loss = 1913.9327 (Best: 1738.3732)\n",
      "Train Epoch: 190 | Loss: 1893.6310\n",
      "Validate Epoch: 190 | Loss: 1895.1751\n",
      "Epoch 190: Train Loss = 1893.6310 (Best: 1738.3732)\n",
      "Train Epoch: 191 | Loss: 1771.7333\n",
      "Validate Epoch: 191 | Loss: 1911.4754\n",
      "Epoch 191: Train Loss = 1771.7333 (Best: 1738.3732)\n",
      "Train Epoch: 192 | Loss: 1886.5833\n",
      "Validate Epoch: 192 | Loss: 1911.5394\n",
      "Epoch 192: Train Loss = 1886.5833 (Best: 1738.3732)\n",
      "Train Epoch: 193 | Loss: 1827.7481\n",
      "Validate Epoch: 193 | Loss: 1846.8189\n",
      "Epoch 193: Train Loss = 1827.7481 (Best: 1738.3732)\n",
      "Train Epoch: 194 | Loss: 1876.9240\n",
      "Validate Epoch: 194 | Loss: 1902.0238\n",
      "Epoch 194: Train Loss = 1876.9240 (Best: 1738.3732)\n",
      "Train Epoch: 195 | Loss: 1811.6830\n",
      "Validate Epoch: 195 | Loss: 1857.0560\n",
      "Epoch 195: Train Loss = 1811.6830 (Best: 1738.3732)\n",
      "Train Epoch: 196 | Loss: 1774.5776\n",
      "Validate Epoch: 196 | Loss: 1856.6227\n",
      "Epoch 196: Train Loss = 1774.5776 (Best: 1738.3732)\n",
      "Train Epoch: 197 | Loss: 1883.1260\n",
      "Validate Epoch: 197 | Loss: 1787.9881\n",
      "Epoch 197: Train Loss = 1883.1260 (Best: 1738.3732)\n",
      "Train Epoch: 198 | Loss: 1840.1587\n",
      "Validate Epoch: 198 | Loss: 1822.8201\n",
      "Epoch 198: Train Loss = 1840.1587 (Best: 1738.3732)\n",
      "Train Epoch: 199 | Loss: 1808.7837\n",
      "Validate Epoch: 199 | Loss: 1887.5267\n",
      "Epoch 199: Train Loss = 1808.7837 (Best: 1738.3732)\n",
      "Train Epoch: 200 | Loss: 1850.3843\n",
      "Validate Epoch: 200 | Loss: 1913.8786\n",
      "Epoch 200: Train Loss = 1850.3843 (Best: 1738.3732)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T20:55:29.875904Z",
     "start_time": "2025-05-12T20:55:29.868372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_samples(samples, output_dir=result_dir):\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Zapis do pliku PNG\n",
    "        save_image(sample, os.path.join(output_dir, f\"sample_{i}.png\"))"
   ],
   "id": "517d82076973677c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T20:55:33.399032Z",
     "start_time": "2025-05-12T20:55:32.963941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Przykład użycia:\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "desired_variance = 0.5\n",
    "desired_mu = 0.3\n",
    "\n",
    "model = VAE().to(device)\n",
    "state_dict = torch.load(result_dir+'/cnnvae_best.pth', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "mu = torch.tensor(desired_mu) * torch.ones(LATENT_DIM).to(device)\n",
    "logvar = torch.log(torch.tensor(desired_variance)) * torch.ones(LATENT_DIM).to(device)\n",
    "samples = model.generate_from_accurate_params(mu, logvar, num_samples=5, device=device)\n",
    "\n",
    "save_samples(samples)"
   ],
   "id": "9f2aee8ea629f1a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No VAE checkpoint found.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.3.weight\", \"encoder.3.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_conv.0.weight\", \"encoder_conv.0.bias\", \"encoder_conv.2.weight\", \"encoder_conv.2.bias\", \"encoder_conv.4.weight\", \"encoder_conv.4.bias\", \"encoder_conv.6.weight\", \"encoder_conv.6.bias\", \"encoder_conv.8.weight\", \"encoder_conv.8.bias\", \"decoder_fc.0.weight\", \"decoder_fc.0.bias\", \"decoder_conv.0.weight\", \"decoder_conv.0.bias\", \"decoder_conv.2.weight\", \"decoder_conv.2.bias\", \"decoder_conv.4.weight\", \"decoder_conv.4.bias\", \"decoder_conv.6.weight\", \"decoder_conv.6.bias\", \"decoder_conv.8.weight\", \"decoder_conv.8.bias\". \n\tsize mismatch for fc_mu.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n\tsize mismatch for fc_mu.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc_var.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n\tsize mismatch for fc_var.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m model = VAE().to(device)\n\u001B[32m     11\u001B[39m state_dict = torch.load(result_dir+\u001B[33m'\u001B[39m\u001B[33m/cnnvae_best.pth\u001B[39m\u001B[33m'\u001B[39m, map_location=\u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m model.eval()\n\u001B[32m     15\u001B[39m mu = torch.tensor(desired_mu) * torch.ones(LATENT_DIM).to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2148\u001B[39m         error_msgs.insert(\n\u001B[32m   2149\u001B[39m             \u001B[32m0\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m'\u001B[39m.format(\n\u001B[32m   2150\u001B[39m                 \u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[32m   2152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2153\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m'\u001B[39m.format(\n\u001B[32m   2154\u001B[39m                        \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)))\n\u001B[32m   2155\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.3.weight\", \"encoder.3.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_conv.0.weight\", \"encoder_conv.0.bias\", \"encoder_conv.2.weight\", \"encoder_conv.2.bias\", \"encoder_conv.4.weight\", \"encoder_conv.4.bias\", \"encoder_conv.6.weight\", \"encoder_conv.6.bias\", \"encoder_conv.8.weight\", \"encoder_conv.8.bias\", \"decoder_fc.0.weight\", \"decoder_fc.0.bias\", \"decoder_conv.0.weight\", \"decoder_conv.0.bias\", \"decoder_conv.2.weight\", \"decoder_conv.2.bias\", \"decoder_conv.4.weight\", \"decoder_conv.4.bias\", \"decoder_conv.6.weight\", \"decoder_conv.6.bias\", \"decoder_conv.8.weight\", \"decoder_conv.8.bias\". \n\tsize mismatch for fc_mu.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n\tsize mismatch for fc_mu.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc_var.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n\tsize mismatch for fc_var.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Przykład użycia:\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "\n",
    "model= VAE().to(device)\n",
    "state_dict = torch.load(result_dir+'vae_best.pth', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "data = next(iter(test_loader)).to(device)\n",
    "tmp_image = data[0]  # Zakładając, że data[0] to obrazy z batcha\n",
    "\n",
    "# Dodaj wymiar batcha, jeśli obraz jest pojedynczy\n",
    "tmp_image = tmp_image.unsqueeze(0)  # Dodaje wymiar batcha na początku\n",
    "\n",
    "\n",
    "\n",
    "# Upewnij się, że obraz jest na odpowiednim urządzeniu\n",
    "tmp_image = tmp_image.to(device)\n",
    "\n",
    "# Przekazujemy obraz do modelu\n",
    "recon, mu, logvar = model(tmp_image)\n",
    "\n",
    "samples = model.generate_from_accurate_params(mu, logvar, num_samples=5, device=device)\n",
    "\n",
    "save_samples(samples)"
   ],
   "id": "17502b409a5a024a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GAN/DCGAN\n"
   ],
   "id": "a47f7bb4a4abea06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "4409c3b41585d707"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:00:26.680712Z",
     "start_time": "2025-05-17T18:00:26.381887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "gan=GAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/GAN\",load_pretrained=True)\n",
    "\n",
    "#gan=CNNGAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/CNNGAN\",load_pretrained=True)\n"
   ],
   "id": "b5afe5d6e075f27f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No generator checkpoint found.\n",
      "[INFO] No discriminator checkpoint found.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pętla główna treningu GAN/CNNGAN"
   ],
   "id": "948724baeb4e984b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:23:25.578736Z",
     "start_time": "2025-05-17T18:00:44.377503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_g_loss = float('inf')\n",
    "best_d_loss = float('inf')\n",
    "no_improve = 0  # Licznik epok bez poprawy\n",
    "\n",
    "logger = GANLogger(save_dir=\"results/GAN\", filename=\"gan_metrics\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    g_loss, d_loss = gan.train_gan(epoch, dataloader=train_loader)\n",
    "\n",
    "    # Walidacja\n",
    "    validate_loss = gan.validate(epoch, val_loader)\n",
    "\n",
    "    # Wizualizacja rekonstrukcji\n",
    "    gan.visualize_reconstruction(epoch)\n",
    "\n",
    "    logger.log(epoch, g_loss, d_loss, validate_loss)\n",
    "\n",
    "    if g_loss < best_g_loss:\n",
    "        best_g_loss = g_loss\n",
    "        gan.save_generator()\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if d_loss < best_d_loss:\n",
    "        best_d_loss = d_loss\n",
    "        gan.save_discriminator()\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f'\\nEarly stopping after {PATIENCE} epochs without improvement')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch {epoch}: Validation Loss = {validate_loss:.4f} Generator Loss = {g_loss:.4f}, Discriminator Loss = {d_loss:.4f} \\n (Best G: {best_g_loss:.4f} Best D: {best_d_loss:.4f}) ')\n",
    "\n",
    "logger.save_json()"
   ],
   "id": "9cab0271a1e48783",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Generator Loss = 9.7600, Discriminator Loss = 6.0208\n",
      "Epoch 1: Validation Loss = 2.9360\n",
      "Epoch 1: Validation Loss = 2.9360 Generator Loss = 9.7600, Discriminator Loss = 6.0208 \n",
      " (Best G: 9.7600 Best D: 6.0208) \n",
      "Epoch 2: Generator Loss = 10.8798, Discriminator Loss = 4.6944\n",
      "Epoch 2: Validation Loss = 3.9196\n",
      "Epoch 2: Validation Loss = 3.9196 Generator Loss = 10.8798, Discriminator Loss = 4.6944 \n",
      " (Best G: 9.7600 Best D: 4.6944) \n",
      "Epoch 3: Generator Loss = 6.0911, Discriminator Loss = 7.9562\n",
      "Epoch 3: Validation Loss = 4.2210\n",
      "Epoch 3: Validation Loss = 4.2210 Generator Loss = 6.0911, Discriminator Loss = 7.9562 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 4: Generator Loss = 8.4777, Discriminator Loss = 6.2495\n",
      "Epoch 4: Validation Loss = 3.9419\n",
      "Epoch 4: Validation Loss = 3.9419 Generator Loss = 8.4777, Discriminator Loss = 6.2495 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 5: Generator Loss = 6.8712, Discriminator Loss = 7.2053\n",
      "Epoch 5: Validation Loss = 6.1037\n",
      "Epoch 5: Validation Loss = 6.1037 Generator Loss = 6.8712, Discriminator Loss = 7.2053 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 6: Generator Loss = 7.1771, Discriminator Loss = 7.8558\n",
      "Epoch 6: Validation Loss = 5.6923\n",
      "Epoch 6: Validation Loss = 5.6923 Generator Loss = 7.1771, Discriminator Loss = 7.8558 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 7: Generator Loss = 6.7497, Discriminator Loss = 8.0135\n",
      "Epoch 7: Validation Loss = 7.5345\n",
      "Epoch 7: Validation Loss = 7.5345 Generator Loss = 6.7497, Discriminator Loss = 8.0135 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 8: Generator Loss = 6.5106, Discriminator Loss = 8.9580\n",
      "Epoch 8: Validation Loss = 5.7820\n",
      "Epoch 8: Validation Loss = 5.7820 Generator Loss = 6.5106, Discriminator Loss = 8.9580 \n",
      " (Best G: 6.0911 Best D: 4.6944) \n",
      "Epoch 9: Generator Loss = 6.0248, Discriminator Loss = 8.9274\n",
      "Epoch 9: Validation Loss = 7.4620\n",
      "Epoch 9: Validation Loss = 7.4620 Generator Loss = 6.0248, Discriminator Loss = 8.9274 \n",
      " (Best G: 6.0248 Best D: 4.6944) \n",
      "Epoch 10: Generator Loss = 5.7788, Discriminator Loss = 9.8934\n",
      "Epoch 10: Validation Loss = 6.7545\n",
      "Epoch 10: Validation Loss = 6.7545 Generator Loss = 5.7788, Discriminator Loss = 9.8934 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 11: Generator Loss = 6.4598, Discriminator Loss = 8.8831\n",
      "Epoch 11: Validation Loss = 5.3651\n",
      "Epoch 11: Validation Loss = 5.3651 Generator Loss = 6.4598, Discriminator Loss = 8.8831 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 12: Generator Loss = 7.8274, Discriminator Loss = 8.3136\n",
      "Epoch 12: Validation Loss = 4.8210\n",
      "Epoch 12: Validation Loss = 4.8210 Generator Loss = 7.8274, Discriminator Loss = 8.3136 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 13: Generator Loss = 7.2280, Discriminator Loss = 7.5629\n",
      "Epoch 13: Validation Loss = 5.3835\n",
      "Epoch 13: Validation Loss = 5.3835 Generator Loss = 7.2280, Discriminator Loss = 7.5629 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 14: Generator Loss = 7.2760, Discriminator Loss = 9.3887\n",
      "Epoch 14: Validation Loss = 6.2387\n",
      "Epoch 14: Validation Loss = 6.2387 Generator Loss = 7.2760, Discriminator Loss = 9.3887 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 15: Generator Loss = 6.0328, Discriminator Loss = 9.4507\n",
      "Epoch 15: Validation Loss = 6.8202\n",
      "Epoch 15: Validation Loss = 6.8202 Generator Loss = 6.0328, Discriminator Loss = 9.4507 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 16: Generator Loss = 8.4924, Discriminator Loss = 8.6682\n",
      "Epoch 16: Validation Loss = 5.6520\n",
      "Epoch 16: Validation Loss = 5.6520 Generator Loss = 8.4924, Discriminator Loss = 8.6682 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 17: Generator Loss = 6.8802, Discriminator Loss = 9.2018\n",
      "Epoch 17: Validation Loss = 5.8690\n",
      "Epoch 17: Validation Loss = 5.8690 Generator Loss = 6.8802, Discriminator Loss = 9.2018 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 18: Generator Loss = 8.2514, Discriminator Loss = 11.6568\n",
      "Epoch 18: Validation Loss = 9.7483\n",
      "Epoch 18: Validation Loss = 9.7483 Generator Loss = 8.2514, Discriminator Loss = 11.6568 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 19: Generator Loss = 16.3807, Discriminator Loss = 18.7847\n",
      "Epoch 19: Validation Loss = 11.4975\n",
      "Epoch 19: Validation Loss = 11.4975 Generator Loss = 16.3807, Discriminator Loss = 18.7847 \n",
      " (Best G: 5.7788 Best D: 4.6944) \n",
      "Epoch 20: Generator Loss = 5.0693, Discriminator Loss = 14.1214\n",
      "Epoch 20: Validation Loss = 5.0641\n",
      "Epoch 20: Validation Loss = 5.0641 Generator Loss = 5.0693, Discriminator Loss = 14.1214 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 21: Generator Loss = 7.4322, Discriminator Loss = 7.7999\n",
      "Epoch 21: Validation Loss = 6.2296\n",
      "Epoch 21: Validation Loss = 6.2296 Generator Loss = 7.4322, Discriminator Loss = 7.7999 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 22: Generator Loss = 7.7827, Discriminator Loss = 12.2524\n",
      "Epoch 22: Validation Loss = 8.1272\n",
      "Epoch 22: Validation Loss = 8.1272 Generator Loss = 7.7827, Discriminator Loss = 12.2524 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 23: Generator Loss = 5.9959, Discriminator Loss = 10.9871\n",
      "Epoch 23: Validation Loss = 5.7139\n",
      "Epoch 23: Validation Loss = 5.7139 Generator Loss = 5.9959, Discriminator Loss = 10.9871 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 24: Generator Loss = 7.7428, Discriminator Loss = 8.8565\n",
      "Epoch 24: Validation Loss = 5.7387\n",
      "Epoch 24: Validation Loss = 5.7387 Generator Loss = 7.7428, Discriminator Loss = 8.8565 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 25: Generator Loss = 7.0892, Discriminator Loss = 8.8652\n",
      "Epoch 25: Validation Loss = 6.3497\n",
      "Epoch 25: Validation Loss = 6.3497 Generator Loss = 7.0892, Discriminator Loss = 8.8652 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 26: Generator Loss = 8.5085, Discriminator Loss = 9.6297\n",
      "Epoch 26: Validation Loss = 6.1187\n",
      "Epoch 26: Validation Loss = 6.1187 Generator Loss = 8.5085, Discriminator Loss = 9.6297 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 27: Generator Loss = 7.4014, Discriminator Loss = 9.4336\n",
      "Epoch 27: Validation Loss = 5.9016\n",
      "Epoch 27: Validation Loss = 5.9016 Generator Loss = 7.4014, Discriminator Loss = 9.4336 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 28: Generator Loss = 8.7855, Discriminator Loss = 9.2025\n",
      "Epoch 28: Validation Loss = 6.0268\n",
      "Epoch 28: Validation Loss = 6.0268 Generator Loss = 8.7855, Discriminator Loss = 9.2025 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 29: Generator Loss = 8.0426, Discriminator Loss = 9.4309\n",
      "Epoch 29: Validation Loss = 6.1933\n",
      "Epoch 29: Validation Loss = 6.1933 Generator Loss = 8.0426, Discriminator Loss = 9.4309 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 30: Generator Loss = 7.5750, Discriminator Loss = 9.7880\n",
      "Epoch 30: Validation Loss = 7.0656\n",
      "Epoch 30: Validation Loss = 7.0656 Generator Loss = 7.5750, Discriminator Loss = 9.7880 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 31: Generator Loss = 10.0115, Discriminator Loss = 11.5336\n",
      "Epoch 31: Validation Loss = 6.5779\n",
      "Epoch 31: Validation Loss = 6.5779 Generator Loss = 10.0115, Discriminator Loss = 11.5336 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 32: Generator Loss = 11.3884, Discriminator Loss = 11.3615\n",
      "Epoch 32: Validation Loss = 5.4061\n",
      "Epoch 32: Validation Loss = 5.4061 Generator Loss = 11.3884, Discriminator Loss = 11.3615 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 33: Generator Loss = 11.1881, Discriminator Loss = 12.0632\n",
      "Epoch 33: Validation Loss = 6.4756\n",
      "Epoch 33: Validation Loss = 6.4756 Generator Loss = 11.1881, Discriminator Loss = 12.0632 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 34: Generator Loss = 8.0954, Discriminator Loss = 10.1929\n",
      "Epoch 34: Validation Loss = 5.6536\n",
      "Epoch 34: Validation Loss = 5.6536 Generator Loss = 8.0954, Discriminator Loss = 10.1929 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 35: Generator Loss = 9.0739, Discriminator Loss = 11.5730\n",
      "Epoch 35: Validation Loss = 6.3876\n",
      "Epoch 35: Validation Loss = 6.3876 Generator Loss = 9.0739, Discriminator Loss = 11.5730 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 36: Generator Loss = 8.2376, Discriminator Loss = 9.5577\n",
      "Epoch 36: Validation Loss = 5.8133\n",
      "Epoch 36: Validation Loss = 5.8133 Generator Loss = 8.2376, Discriminator Loss = 9.5577 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 37: Generator Loss = 8.7708, Discriminator Loss = 12.8774\n",
      "Epoch 37: Validation Loss = 6.6520\n",
      "Epoch 37: Validation Loss = 6.6520 Generator Loss = 8.7708, Discriminator Loss = 12.8774 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 38: Generator Loss = 7.8435, Discriminator Loss = 9.3010\n",
      "Epoch 38: Validation Loss = 5.6867\n",
      "Epoch 38: Validation Loss = 5.6867 Generator Loss = 7.8435, Discriminator Loss = 9.3010 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 39: Generator Loss = 8.3640, Discriminator Loss = 9.1453\n",
      "Epoch 39: Validation Loss = 6.8827\n",
      "Epoch 39: Validation Loss = 6.8827 Generator Loss = 8.3640, Discriminator Loss = 9.1453 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 40: Generator Loss = 10.6794, Discriminator Loss = 11.5714\n",
      "Epoch 40: Validation Loss = 6.2367\n",
      "Epoch 40: Validation Loss = 6.2367 Generator Loss = 10.6794, Discriminator Loss = 11.5714 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 41: Generator Loss = 8.8970, Discriminator Loss = 10.5110\n",
      "Epoch 41: Validation Loss = 5.8545\n",
      "Epoch 41: Validation Loss = 5.8545 Generator Loss = 8.8970, Discriminator Loss = 10.5110 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 42: Generator Loss = 7.8549, Discriminator Loss = 9.3911\n",
      "Epoch 42: Validation Loss = 6.5189\n",
      "Epoch 42: Validation Loss = 6.5189 Generator Loss = 7.8549, Discriminator Loss = 9.3911 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 43: Generator Loss = 9.3701, Discriminator Loss = 11.2540\n",
      "Epoch 43: Validation Loss = 7.2181\n",
      "Epoch 43: Validation Loss = 7.2181 Generator Loss = 9.3701, Discriminator Loss = 11.2540 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 44: Generator Loss = 10.4711, Discriminator Loss = 9.6586\n",
      "Epoch 44: Validation Loss = 5.6944\n",
      "Epoch 44: Validation Loss = 5.6944 Generator Loss = 10.4711, Discriminator Loss = 9.6586 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 45: Generator Loss = 8.8965, Discriminator Loss = 9.5122\n",
      "Epoch 45: Validation Loss = 6.9263\n",
      "Epoch 45: Validation Loss = 6.9263 Generator Loss = 8.8965, Discriminator Loss = 9.5122 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 46: Generator Loss = 8.5896, Discriminator Loss = 10.1490\n",
      "Epoch 46: Validation Loss = 5.6369\n",
      "Epoch 46: Validation Loss = 5.6369 Generator Loss = 8.5896, Discriminator Loss = 10.1490 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 47: Generator Loss = 12.7093, Discriminator Loss = 15.4660\n",
      "Epoch 47: Validation Loss = 8.1884\n",
      "Epoch 47: Validation Loss = 8.1884 Generator Loss = 12.7093, Discriminator Loss = 15.4660 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 48: Generator Loss = 8.9752, Discriminator Loss = 15.4798\n",
      "Epoch 48: Validation Loss = 7.3785\n",
      "Epoch 48: Validation Loss = 7.3785 Generator Loss = 8.9752, Discriminator Loss = 15.4798 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 49: Generator Loss = 7.1422, Discriminator Loss = 10.7152\n",
      "Epoch 49: Validation Loss = 6.4174\n",
      "Epoch 49: Validation Loss = 6.4174 Generator Loss = 7.1422, Discriminator Loss = 10.7152 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 50: Generator Loss = 10.7533, Discriminator Loss = 10.5298\n",
      "Epoch 50: Validation Loss = 6.4444\n",
      "Epoch 50: Validation Loss = 6.4444 Generator Loss = 10.7533, Discriminator Loss = 10.5298 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 51: Generator Loss = 7.3053, Discriminator Loss = 9.9422\n",
      "Epoch 51: Validation Loss = 6.5720\n",
      "Epoch 51: Validation Loss = 6.5720 Generator Loss = 7.3053, Discriminator Loss = 9.9422 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 52: Generator Loss = 10.0106, Discriminator Loss = 10.0316\n",
      "Epoch 52: Validation Loss = 6.2146\n",
      "Epoch 52: Validation Loss = 6.2146 Generator Loss = 10.0106, Discriminator Loss = 10.0316 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 53: Generator Loss = 9.3780, Discriminator Loss = 9.5859\n",
      "Epoch 53: Validation Loss = 6.3818\n",
      "Epoch 53: Validation Loss = 6.3818 Generator Loss = 9.3780, Discriminator Loss = 9.5859 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 54: Generator Loss = 7.6329, Discriminator Loss = 10.3953\n",
      "Epoch 54: Validation Loss = 6.5021\n",
      "Epoch 54: Validation Loss = 6.5021 Generator Loss = 7.6329, Discriminator Loss = 10.3953 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 55: Generator Loss = 9.4691, Discriminator Loss = 9.9421\n",
      "Epoch 55: Validation Loss = 6.2121\n",
      "Epoch 55: Validation Loss = 6.2121 Generator Loss = 9.4691, Discriminator Loss = 9.9421 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 56: Generator Loss = 9.8803, Discriminator Loss = 9.3385\n",
      "Epoch 56: Validation Loss = 5.8402\n",
      "Epoch 56: Validation Loss = 5.8402 Generator Loss = 9.8803, Discriminator Loss = 9.3385 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 57: Generator Loss = 10.1579, Discriminator Loss = 11.5265\n",
      "Epoch 57: Validation Loss = 7.2376\n",
      "Epoch 57: Validation Loss = 7.2376 Generator Loss = 10.1579, Discriminator Loss = 11.5265 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 58: Generator Loss = 7.9020, Discriminator Loss = 10.1574\n",
      "Epoch 58: Validation Loss = 6.2369\n",
      "Epoch 58: Validation Loss = 6.2369 Generator Loss = 7.9020, Discriminator Loss = 10.1574 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 59: Generator Loss = 9.1546, Discriminator Loss = 9.2143\n",
      "Epoch 59: Validation Loss = 5.7740\n",
      "Epoch 59: Validation Loss = 5.7740 Generator Loss = 9.1546, Discriminator Loss = 9.2143 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 60: Generator Loss = 9.5956, Discriminator Loss = 9.0083\n",
      "Epoch 60: Validation Loss = 6.2174\n",
      "Epoch 60: Validation Loss = 6.2174 Generator Loss = 9.5956, Discriminator Loss = 9.0083 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 61: Generator Loss = 9.2675, Discriminator Loss = 11.0052\n",
      "Epoch 61: Validation Loss = 6.9308\n",
      "Epoch 61: Validation Loss = 6.9308 Generator Loss = 9.2675, Discriminator Loss = 11.0052 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 62: Generator Loss = 10.3259, Discriminator Loss = 10.0751\n",
      "Epoch 62: Validation Loss = 5.7865\n",
      "Epoch 62: Validation Loss = 5.7865 Generator Loss = 10.3259, Discriminator Loss = 10.0751 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 63: Generator Loss = 11.1798, Discriminator Loss = 8.9983\n",
      "Epoch 63: Validation Loss = 6.3331\n",
      "Epoch 63: Validation Loss = 6.3331 Generator Loss = 11.1798, Discriminator Loss = 8.9983 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 64: Generator Loss = 8.9328, Discriminator Loss = 10.3830\n",
      "Epoch 64: Validation Loss = 6.5170\n",
      "Epoch 64: Validation Loss = 6.5170 Generator Loss = 8.9328, Discriminator Loss = 10.3830 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 65: Generator Loss = 9.3504, Discriminator Loss = 9.5454\n",
      "Epoch 65: Validation Loss = 5.9216\n",
      "Epoch 65: Validation Loss = 5.9216 Generator Loss = 9.3504, Discriminator Loss = 9.5454 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 66: Generator Loss = 14.7593, Discriminator Loss = 11.3602\n",
      "Epoch 66: Validation Loss = 9.3720\n",
      "Epoch 66: Validation Loss = 9.3720 Generator Loss = 14.7593, Discriminator Loss = 11.3602 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 67: Generator Loss = 7.7505, Discriminator Loss = 11.8777\n",
      "Epoch 67: Validation Loss = 7.1198\n",
      "Epoch 67: Validation Loss = 7.1198 Generator Loss = 7.7505, Discriminator Loss = 11.8777 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 68: Generator Loss = 10.2320, Discriminator Loss = 10.4063\n",
      "Epoch 68: Validation Loss = 6.1706\n",
      "Epoch 68: Validation Loss = 6.1706 Generator Loss = 10.2320, Discriminator Loss = 10.4063 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 69: Generator Loss = 10.9933, Discriminator Loss = 11.8952\n",
      "Epoch 69: Validation Loss = 5.9212\n",
      "Epoch 69: Validation Loss = 5.9212 Generator Loss = 10.9933, Discriminator Loss = 11.8952 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 70: Generator Loss = 9.7209, Discriminator Loss = 9.0801\n",
      "Epoch 70: Validation Loss = 6.5784\n",
      "Epoch 70: Validation Loss = 6.5784 Generator Loss = 9.7209, Discriminator Loss = 9.0801 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 71: Generator Loss = 8.4650, Discriminator Loss = 11.5433\n",
      "Epoch 71: Validation Loss = 6.0600\n",
      "Epoch 71: Validation Loss = 6.0600 Generator Loss = 8.4650, Discriminator Loss = 11.5433 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 72: Generator Loss = 9.3177, Discriminator Loss = 9.3518\n",
      "Epoch 72: Validation Loss = 6.1252\n",
      "Epoch 72: Validation Loss = 6.1252 Generator Loss = 9.3177, Discriminator Loss = 9.3518 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 73: Generator Loss = 8.8661, Discriminator Loss = 9.7675\n",
      "Epoch 73: Validation Loss = 6.3743\n",
      "Epoch 73: Validation Loss = 6.3743 Generator Loss = 8.8661, Discriminator Loss = 9.7675 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 74: Generator Loss = 9.3710, Discriminator Loss = 9.6935\n",
      "Epoch 74: Validation Loss = 6.1652\n",
      "Epoch 74: Validation Loss = 6.1652 Generator Loss = 9.3710, Discriminator Loss = 9.6935 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 75: Generator Loss = 8.8199, Discriminator Loss = 9.5980\n",
      "Epoch 75: Validation Loss = 7.0621\n",
      "Epoch 75: Validation Loss = 7.0621 Generator Loss = 8.8199, Discriminator Loss = 9.5980 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 76: Generator Loss = 11.2167, Discriminator Loss = 10.4470\n",
      "Epoch 76: Validation Loss = 6.2545\n",
      "Epoch 76: Validation Loss = 6.2545 Generator Loss = 11.2167, Discriminator Loss = 10.4470 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 77: Generator Loss = 9.0543, Discriminator Loss = 9.4446\n",
      "Epoch 77: Validation Loss = 6.2311\n",
      "Epoch 77: Validation Loss = 6.2311 Generator Loss = 9.0543, Discriminator Loss = 9.4446 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 78: Generator Loss = 9.4611, Discriminator Loss = 9.5200\n",
      "Epoch 78: Validation Loss = 6.3124\n",
      "Epoch 78: Validation Loss = 6.3124 Generator Loss = 9.4611, Discriminator Loss = 9.5200 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 79: Generator Loss = 8.9467, Discriminator Loss = 9.7114\n",
      "Epoch 79: Validation Loss = 6.2820\n",
      "Epoch 79: Validation Loss = 6.2820 Generator Loss = 8.9467, Discriminator Loss = 9.7114 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 80: Generator Loss = 10.8387, Discriminator Loss = 9.5582\n",
      "Epoch 80: Validation Loss = 5.9377\n",
      "Epoch 80: Validation Loss = 5.9377 Generator Loss = 10.8387, Discriminator Loss = 9.5582 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 81: Generator Loss = 9.6686, Discriminator Loss = 9.3759\n",
      "Epoch 81: Validation Loss = 6.3553\n",
      "Epoch 81: Validation Loss = 6.3553 Generator Loss = 9.6686, Discriminator Loss = 9.3759 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 82: Generator Loss = 9.9403, Discriminator Loss = 10.1423\n",
      "Epoch 82: Validation Loss = 6.3655\n",
      "Epoch 82: Validation Loss = 6.3655 Generator Loss = 9.9403, Discriminator Loss = 10.1423 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 83: Generator Loss = 16.0487, Discriminator Loss = 13.0372\n",
      "Epoch 83: Validation Loss = 10.3665\n",
      "Epoch 83: Validation Loss = 10.3665 Generator Loss = 16.0487, Discriminator Loss = 13.0372 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 84: Generator Loss = 14.0520, Discriminator Loss = 14.4460\n",
      "Epoch 84: Validation Loss = 5.7804\n",
      "Epoch 84: Validation Loss = 5.7804 Generator Loss = 14.0520, Discriminator Loss = 14.4460 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 85: Generator Loss = 10.2647, Discriminator Loss = 11.6830\n",
      "Epoch 85: Validation Loss = 6.0652\n",
      "Epoch 85: Validation Loss = 6.0652 Generator Loss = 10.2647, Discriminator Loss = 11.6830 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 86: Generator Loss = 9.1737, Discriminator Loss = 9.8950\n",
      "Epoch 86: Validation Loss = 6.7488\n",
      "Epoch 86: Validation Loss = 6.7488 Generator Loss = 9.1737, Discriminator Loss = 9.8950 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 87: Generator Loss = 10.4040, Discriminator Loss = 10.1856\n",
      "Epoch 87: Validation Loss = 6.2716\n",
      "Epoch 87: Validation Loss = 6.2716 Generator Loss = 10.4040, Discriminator Loss = 10.1856 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 88: Generator Loss = 8.2801, Discriminator Loss = 9.7203\n",
      "Epoch 88: Validation Loss = 6.0276\n",
      "Epoch 88: Validation Loss = 6.0276 Generator Loss = 8.2801, Discriminator Loss = 9.7203 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 89: Generator Loss = 9.5269, Discriminator Loss = 9.3891\n",
      "Epoch 89: Validation Loss = 6.2236\n",
      "Epoch 89: Validation Loss = 6.2236 Generator Loss = 9.5269, Discriminator Loss = 9.3891 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 90: Generator Loss = 9.3459, Discriminator Loss = 10.7435\n",
      "Epoch 90: Validation Loss = 6.2344\n",
      "Epoch 90: Validation Loss = 6.2344 Generator Loss = 9.3459, Discriminator Loss = 10.7435 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 91: Generator Loss = 8.6239, Discriminator Loss = 9.4446\n",
      "Epoch 91: Validation Loss = 6.0278\n",
      "Epoch 91: Validation Loss = 6.0278 Generator Loss = 8.6239, Discriminator Loss = 9.4446 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 92: Generator Loss = 9.4070, Discriminator Loss = 9.3086\n",
      "Epoch 92: Validation Loss = 6.0108\n",
      "Epoch 92: Validation Loss = 6.0108 Generator Loss = 9.4070, Discriminator Loss = 9.3086 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 93: Generator Loss = 9.0641, Discriminator Loss = 9.3480\n",
      "Epoch 93: Validation Loss = 6.2258\n",
      "Epoch 93: Validation Loss = 6.2258 Generator Loss = 9.0641, Discriminator Loss = 9.3480 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 94: Generator Loss = 8.6298, Discriminator Loss = 9.6332\n",
      "Epoch 94: Validation Loss = 6.1580\n",
      "Epoch 94: Validation Loss = 6.1580 Generator Loss = 8.6298, Discriminator Loss = 9.6332 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 95: Generator Loss = 8.2834, Discriminator Loss = 9.8089\n",
      "Epoch 95: Validation Loss = 6.4274\n",
      "Epoch 95: Validation Loss = 6.4274 Generator Loss = 8.2834, Discriminator Loss = 9.8089 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 96: Generator Loss = 13.2943, Discriminator Loss = 10.9014\n",
      "Epoch 96: Validation Loss = 6.5657\n",
      "Epoch 96: Validation Loss = 6.5657 Generator Loss = 13.2943, Discriminator Loss = 10.9014 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 97: Generator Loss = 8.2452, Discriminator Loss = 10.3358\n",
      "Epoch 97: Validation Loss = 6.2864\n",
      "Epoch 97: Validation Loss = 6.2864 Generator Loss = 8.2452, Discriminator Loss = 10.3358 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 98: Generator Loss = 10.2484, Discriminator Loss = 10.1881\n",
      "Epoch 98: Validation Loss = 6.3180\n",
      "Epoch 98: Validation Loss = 6.3180 Generator Loss = 10.2484, Discriminator Loss = 10.1881 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 99: Generator Loss = 10.2433, Discriminator Loss = 9.6786\n",
      "Epoch 99: Validation Loss = 6.1502\n",
      "Epoch 99: Validation Loss = 6.1502 Generator Loss = 10.2433, Discriminator Loss = 9.6786 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 100: Generator Loss = 9.7001, Discriminator Loss = 9.7680\n",
      "Epoch 100: Validation Loss = 6.6652\n",
      "Epoch 100: Validation Loss = 6.6652 Generator Loss = 9.7001, Discriminator Loss = 9.7680 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 101: Generator Loss = 8.7411, Discriminator Loss = 9.9749\n",
      "Epoch 101: Validation Loss = 6.5329\n",
      "Epoch 101: Validation Loss = 6.5329 Generator Loss = 8.7411, Discriminator Loss = 9.9749 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 102: Generator Loss = 10.0488, Discriminator Loss = 9.5822\n",
      "Epoch 102: Validation Loss = 6.1691\n",
      "Epoch 102: Validation Loss = 6.1691 Generator Loss = 10.0488, Discriminator Loss = 9.5822 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 103: Generator Loss = 9.3257, Discriminator Loss = 9.3915\n",
      "Epoch 103: Validation Loss = 6.2021\n",
      "Epoch 103: Validation Loss = 6.2021 Generator Loss = 9.3257, Discriminator Loss = 9.3915 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 104: Generator Loss = 9.2542, Discriminator Loss = 9.6408\n",
      "Epoch 104: Validation Loss = 6.1798\n",
      "Epoch 104: Validation Loss = 6.1798 Generator Loss = 9.2542, Discriminator Loss = 9.6408 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 105: Generator Loss = 9.8067, Discriminator Loss = 9.4759\n",
      "Epoch 105: Validation Loss = 6.1515\n",
      "Epoch 105: Validation Loss = 6.1515 Generator Loss = 9.8067, Discriminator Loss = 9.4759 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 106: Generator Loss = 10.0596, Discriminator Loss = 9.6529\n",
      "Epoch 106: Validation Loss = 6.8081\n",
      "Epoch 106: Validation Loss = 6.8081 Generator Loss = 10.0596, Discriminator Loss = 9.6529 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 107: Generator Loss = 11.5908, Discriminator Loss = 10.3321\n",
      "Epoch 107: Validation Loss = 6.2512\n",
      "Epoch 107: Validation Loss = 6.2512 Generator Loss = 11.5908, Discriminator Loss = 10.3321 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 108: Generator Loss = 9.8209, Discriminator Loss = 9.4614\n",
      "Epoch 108: Validation Loss = 5.7649\n",
      "Epoch 108: Validation Loss = 5.7649 Generator Loss = 9.8209, Discriminator Loss = 9.4614 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 109: Generator Loss = 9.7632, Discriminator Loss = 9.2189\n",
      "Epoch 109: Validation Loss = 6.8193\n",
      "Epoch 109: Validation Loss = 6.8193 Generator Loss = 9.7632, Discriminator Loss = 9.2189 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 110: Generator Loss = 12.3400, Discriminator Loss = 11.2618\n",
      "Epoch 110: Validation Loss = 5.7349\n",
      "Epoch 110: Validation Loss = 5.7349 Generator Loss = 12.3400, Discriminator Loss = 11.2618 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 111: Generator Loss = 15.6670, Discriminator Loss = 14.1237\n",
      "Epoch 111: Validation Loss = 9.9103\n",
      "Epoch 111: Validation Loss = 9.9103 Generator Loss = 15.6670, Discriminator Loss = 14.1237 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 112: Generator Loss = 11.1372, Discriminator Loss = 13.6074\n",
      "Epoch 112: Validation Loss = 8.2132\n",
      "Epoch 112: Validation Loss = 8.2132 Generator Loss = 11.1372, Discriminator Loss = 13.6074 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 113: Generator Loss = 12.2133, Discriminator Loss = 11.7937\n",
      "Epoch 113: Validation Loss = 6.3614\n",
      "Epoch 113: Validation Loss = 6.3614 Generator Loss = 12.2133, Discriminator Loss = 11.7937 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 114: Generator Loss = 7.9501, Discriminator Loss = 9.1726\n",
      "Epoch 114: Validation Loss = 5.8597\n",
      "Epoch 114: Validation Loss = 5.8597 Generator Loss = 7.9501, Discriminator Loss = 9.1726 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 115: Generator Loss = 9.9093, Discriminator Loss = 9.3233\n",
      "Epoch 115: Validation Loss = 6.5729\n",
      "Epoch 115: Validation Loss = 6.5729 Generator Loss = 9.9093, Discriminator Loss = 9.3233 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 116: Generator Loss = 7.4362, Discriminator Loss = 9.7766\n",
      "Epoch 116: Validation Loss = 6.7968\n",
      "Epoch 116: Validation Loss = 6.7968 Generator Loss = 7.4362, Discriminator Loss = 9.7766 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 117: Generator Loss = 9.2078, Discriminator Loss = 10.0442\n",
      "Epoch 117: Validation Loss = 6.4666\n",
      "Epoch 117: Validation Loss = 6.4666 Generator Loss = 9.2078, Discriminator Loss = 10.0442 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 118: Generator Loss = 8.4336, Discriminator Loss = 9.6268\n",
      "Epoch 118: Validation Loss = 6.0250\n",
      "Epoch 118: Validation Loss = 6.0250 Generator Loss = 8.4336, Discriminator Loss = 9.6268 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 119: Generator Loss = 9.7813, Discriminator Loss = 9.8193\n",
      "Epoch 119: Validation Loss = 6.2971\n",
      "Epoch 119: Validation Loss = 6.2971 Generator Loss = 9.7813, Discriminator Loss = 9.8193 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 120: Generator Loss = 10.6430, Discriminator Loss = 9.7817\n",
      "Epoch 120: Validation Loss = 6.7054\n",
      "Epoch 120: Validation Loss = 6.7054 Generator Loss = 10.6430, Discriminator Loss = 9.7817 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 121: Generator Loss = 8.2107, Discriminator Loss = 10.1998\n",
      "Epoch 121: Validation Loss = 6.3430\n",
      "Epoch 121: Validation Loss = 6.3430 Generator Loss = 8.2107, Discriminator Loss = 10.1998 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 122: Generator Loss = 9.9017, Discriminator Loss = 9.5484\n",
      "Epoch 122: Validation Loss = 6.1098\n",
      "Epoch 122: Validation Loss = 6.1098 Generator Loss = 9.9017, Discriminator Loss = 9.5484 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 123: Generator Loss = 10.3975, Discriminator Loss = 9.2710\n",
      "Epoch 123: Validation Loss = 6.0967\n",
      "Epoch 123: Validation Loss = 6.0967 Generator Loss = 10.3975, Discriminator Loss = 9.2710 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 124: Generator Loss = 11.8087, Discriminator Loss = 9.9913\n",
      "Epoch 124: Validation Loss = 7.0337\n",
      "Epoch 124: Validation Loss = 7.0337 Generator Loss = 11.8087, Discriminator Loss = 9.9913 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 125: Generator Loss = 8.5186, Discriminator Loss = 10.4737\n",
      "Epoch 125: Validation Loss = 6.1299\n",
      "Epoch 125: Validation Loss = 6.1299 Generator Loss = 8.5186, Discriminator Loss = 10.4737 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 126: Generator Loss = 11.2619, Discriminator Loss = 10.3915\n",
      "Epoch 126: Validation Loss = 6.1468\n",
      "Epoch 126: Validation Loss = 6.1468 Generator Loss = 11.2619, Discriminator Loss = 10.3915 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 127: Generator Loss = 8.9310, Discriminator Loss = 9.4278\n",
      "Epoch 127: Validation Loss = 6.2639\n",
      "Epoch 127: Validation Loss = 6.2639 Generator Loss = 8.9310, Discriminator Loss = 9.4278 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 128: Generator Loss = 10.4690, Discriminator Loss = 9.5479\n",
      "Epoch 128: Validation Loss = 6.4648\n",
      "Epoch 128: Validation Loss = 6.4648 Generator Loss = 10.4690, Discriminator Loss = 9.5479 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 129: Generator Loss = 8.6170, Discriminator Loss = 9.9946\n",
      "Epoch 129: Validation Loss = 6.5845\n",
      "Epoch 129: Validation Loss = 6.5845 Generator Loss = 8.6170, Discriminator Loss = 9.9946 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 130: Generator Loss = 9.9907, Discriminator Loss = 9.7007\n",
      "Epoch 130: Validation Loss = 5.9405\n",
      "Epoch 130: Validation Loss = 5.9405 Generator Loss = 9.9907, Discriminator Loss = 9.7007 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 131: Generator Loss = 10.8045, Discriminator Loss = 8.9507\n",
      "Epoch 131: Validation Loss = 5.8456\n",
      "Epoch 131: Validation Loss = 5.8456 Generator Loss = 10.8045, Discriminator Loss = 8.9507 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 132: Generator Loss = 10.0250, Discriminator Loss = 10.2510\n",
      "Epoch 132: Validation Loss = 10.1834\n",
      "Epoch 132: Validation Loss = 10.1834 Generator Loss = 10.0250, Discriminator Loss = 10.2510 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 133: Generator Loss = 10.7655, Discriminator Loss = 11.3205\n",
      "Epoch 133: Validation Loss = 5.7610\n",
      "Epoch 133: Validation Loss = 5.7610 Generator Loss = 10.7655, Discriminator Loss = 11.3205 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 134: Generator Loss = 10.9277, Discriminator Loss = 8.5951\n",
      "Epoch 134: Validation Loss = 5.5327\n",
      "Epoch 134: Validation Loss = 5.5327 Generator Loss = 10.9277, Discriminator Loss = 8.5951 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 135: Generator Loss = 10.5526, Discriminator Loss = 9.6707\n",
      "Epoch 135: Validation Loss = 7.3366\n",
      "Epoch 135: Validation Loss = 7.3366 Generator Loss = 10.5526, Discriminator Loss = 9.6707 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 136: Generator Loss = 8.4051, Discriminator Loss = 10.1013\n",
      "Epoch 136: Validation Loss = 6.1219\n",
      "Epoch 136: Validation Loss = 6.1219 Generator Loss = 8.4051, Discriminator Loss = 10.1013 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 137: Generator Loss = 9.5574, Discriminator Loss = 9.2914\n",
      "Epoch 137: Validation Loss = 6.3133\n",
      "Epoch 137: Validation Loss = 6.3133 Generator Loss = 9.5574, Discriminator Loss = 9.2914 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 138: Generator Loss = 11.4577, Discriminator Loss = 10.1307\n",
      "Epoch 138: Validation Loss = 6.5769\n",
      "Epoch 138: Validation Loss = 6.5769 Generator Loss = 11.4577, Discriminator Loss = 10.1307 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 139: Generator Loss = 10.8101, Discriminator Loss = 10.2575\n",
      "Epoch 139: Validation Loss = 6.0217\n",
      "Epoch 139: Validation Loss = 6.0217 Generator Loss = 10.8101, Discriminator Loss = 10.2575 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 140: Generator Loss = 10.8147, Discriminator Loss = 10.2944\n",
      "Epoch 140: Validation Loss = 7.4095\n",
      "Epoch 140: Validation Loss = 7.4095 Generator Loss = 10.8147, Discriminator Loss = 10.2944 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 141: Generator Loss = 8.0927, Discriminator Loss = 9.9357\n",
      "Epoch 141: Validation Loss = 6.1744\n",
      "Epoch 141: Validation Loss = 6.1744 Generator Loss = 8.0927, Discriminator Loss = 9.9357 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 142: Generator Loss = 10.2972, Discriminator Loss = 9.4293\n",
      "Epoch 142: Validation Loss = 6.3415\n",
      "Epoch 142: Validation Loss = 6.3415 Generator Loss = 10.2972, Discriminator Loss = 9.4293 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 143: Generator Loss = 9.3270, Discriminator Loss = 10.4330\n",
      "Epoch 143: Validation Loss = 6.6687\n",
      "Epoch 143: Validation Loss = 6.6687 Generator Loss = 9.3270, Discriminator Loss = 10.4330 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 144: Generator Loss = 9.1795, Discriminator Loss = 9.6094\n",
      "Epoch 144: Validation Loss = 5.9423\n",
      "Epoch 144: Validation Loss = 5.9423 Generator Loss = 9.1795, Discriminator Loss = 9.6094 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 145: Generator Loss = 10.6670, Discriminator Loss = 9.1767\n",
      "Epoch 145: Validation Loss = 6.4839\n",
      "Epoch 145: Validation Loss = 6.4839 Generator Loss = 10.6670, Discriminator Loss = 9.1767 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 146: Generator Loss = 11.0873, Discriminator Loss = 10.6987\n",
      "Epoch 146: Validation Loss = 6.5213\n",
      "Epoch 146: Validation Loss = 6.5213 Generator Loss = 11.0873, Discriminator Loss = 10.6987 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 147: Generator Loss = 11.2075, Discriminator Loss = 9.2852\n",
      "Epoch 147: Validation Loss = 5.8315\n",
      "Epoch 147: Validation Loss = 5.8315 Generator Loss = 11.2075, Discriminator Loss = 9.2852 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 148: Generator Loss = 11.1394, Discriminator Loss = 13.1504\n",
      "Epoch 148: Validation Loss = 6.2863\n",
      "Epoch 148: Validation Loss = 6.2863 Generator Loss = 11.1394, Discriminator Loss = 13.1504 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 149: Generator Loss = 10.8798, Discriminator Loss = 9.4590\n",
      "Epoch 149: Validation Loss = 5.9317\n",
      "Epoch 149: Validation Loss = 5.9317 Generator Loss = 10.8798, Discriminator Loss = 9.4590 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 150: Generator Loss = 9.1404, Discriminator Loss = 10.4045\n",
      "Epoch 150: Validation Loss = 6.1147\n",
      "Epoch 150: Validation Loss = 6.1147 Generator Loss = 9.1404, Discriminator Loss = 10.4045 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 151: Generator Loss = 9.0436, Discriminator Loss = 9.3061\n",
      "Epoch 151: Validation Loss = 6.0542\n",
      "Epoch 151: Validation Loss = 6.0542 Generator Loss = 9.0436, Discriminator Loss = 9.3061 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 152: Generator Loss = 9.3012, Discriminator Loss = 9.4501\n",
      "Epoch 152: Validation Loss = 6.1750\n",
      "Epoch 152: Validation Loss = 6.1750 Generator Loss = 9.3012, Discriminator Loss = 9.4501 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 153: Generator Loss = 8.9969, Discriminator Loss = 9.5943\n",
      "Epoch 153: Validation Loss = 6.3832\n",
      "Epoch 153: Validation Loss = 6.3832 Generator Loss = 8.9969, Discriminator Loss = 9.5943 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 154: Generator Loss = 10.1205, Discriminator Loss = 9.8518\n",
      "Epoch 154: Validation Loss = 6.3450\n",
      "Epoch 154: Validation Loss = 6.3450 Generator Loss = 10.1205, Discriminator Loss = 9.8518 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 155: Generator Loss = 9.6942, Discriminator Loss = 9.5932\n",
      "Epoch 155: Validation Loss = 6.1027\n",
      "Epoch 155: Validation Loss = 6.1027 Generator Loss = 9.6942, Discriminator Loss = 9.5932 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 156: Generator Loss = 10.3377, Discriminator Loss = 9.6126\n",
      "Epoch 156: Validation Loss = 6.2477\n",
      "Epoch 156: Validation Loss = 6.2477 Generator Loss = 10.3377, Discriminator Loss = 9.6126 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 157: Generator Loss = 9.7786, Discriminator Loss = 9.8153\n",
      "Epoch 157: Validation Loss = 6.3621\n",
      "Epoch 157: Validation Loss = 6.3621 Generator Loss = 9.7786, Discriminator Loss = 9.8153 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 158: Generator Loss = 9.1373, Discriminator Loss = 9.6456\n",
      "Epoch 158: Validation Loss = 6.2196\n",
      "Epoch 158: Validation Loss = 6.2196 Generator Loss = 9.1373, Discriminator Loss = 9.6456 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 159: Generator Loss = 10.8128, Discriminator Loss = 9.9128\n",
      "Epoch 159: Validation Loss = 6.4683\n",
      "Epoch 159: Validation Loss = 6.4683 Generator Loss = 10.8128, Discriminator Loss = 9.9128 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 160: Generator Loss = 10.1491, Discriminator Loss = 9.8192\n",
      "Epoch 160: Validation Loss = 6.4310\n",
      "Epoch 160: Validation Loss = 6.4310 Generator Loss = 10.1491, Discriminator Loss = 9.8192 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 161: Generator Loss = 8.9461, Discriminator Loss = 9.6579\n",
      "Epoch 161: Validation Loss = 6.3836\n",
      "Epoch 161: Validation Loss = 6.3836 Generator Loss = 8.9461, Discriminator Loss = 9.6579 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 162: Generator Loss = 10.7682, Discriminator Loss = 9.4581\n",
      "Epoch 162: Validation Loss = 6.3200\n",
      "Epoch 162: Validation Loss = 6.3200 Generator Loss = 10.7682, Discriminator Loss = 9.4581 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 163: Generator Loss = 12.0959, Discriminator Loss = 10.7516\n",
      "Epoch 163: Validation Loss = 6.5124\n",
      "Epoch 163: Validation Loss = 6.5124 Generator Loss = 12.0959, Discriminator Loss = 10.7516 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 164: Generator Loss = 9.7345, Discriminator Loss = 9.5959\n",
      "Epoch 164: Validation Loss = 6.1834\n",
      "Epoch 164: Validation Loss = 6.1834 Generator Loss = 9.7345, Discriminator Loss = 9.5959 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 165: Generator Loss = 10.0380, Discriminator Loss = 9.4414\n",
      "Epoch 165: Validation Loss = 6.0888\n",
      "Epoch 165: Validation Loss = 6.0888 Generator Loss = 10.0380, Discriminator Loss = 9.4414 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 166: Generator Loss = 10.7815, Discriminator Loss = 10.1436\n",
      "Epoch 166: Validation Loss = 6.5165\n",
      "Epoch 166: Validation Loss = 6.5165 Generator Loss = 10.7815, Discriminator Loss = 10.1436 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 167: Generator Loss = 12.5655, Discriminator Loss = 9.8232\n",
      "Epoch 167: Validation Loss = 7.1153\n",
      "Epoch 167: Validation Loss = 7.1153 Generator Loss = 12.5655, Discriminator Loss = 9.8232 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 168: Generator Loss = 8.6366, Discriminator Loss = 10.1269\n",
      "Epoch 168: Validation Loss = 6.2398\n",
      "Epoch 168: Validation Loss = 6.2398 Generator Loss = 8.6366, Discriminator Loss = 10.1269 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 169: Generator Loss = 9.4335, Discriminator Loss = 9.5549\n",
      "Epoch 169: Validation Loss = 6.1671\n",
      "Epoch 169: Validation Loss = 6.1671 Generator Loss = 9.4335, Discriminator Loss = 9.5549 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 170: Generator Loss = 10.0514, Discriminator Loss = 9.5950\n",
      "Epoch 170: Validation Loss = 6.4345\n",
      "Epoch 170: Validation Loss = 6.4345 Generator Loss = 10.0514, Discriminator Loss = 9.5950 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 171: Generator Loss = 9.6900, Discriminator Loss = 9.7149\n",
      "Epoch 171: Validation Loss = 6.0834\n",
      "Epoch 171: Validation Loss = 6.0834 Generator Loss = 9.6900, Discriminator Loss = 9.7149 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 172: Generator Loss = 9.4442, Discriminator Loss = 9.5667\n",
      "Epoch 172: Validation Loss = 6.0875\n",
      "Epoch 172: Validation Loss = 6.0875 Generator Loss = 9.4442, Discriminator Loss = 9.5667 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 173: Generator Loss = 14.7770, Discriminator Loss = 12.7080\n",
      "Epoch 173: Validation Loss = 7.1295\n",
      "Epoch 173: Validation Loss = 7.1295 Generator Loss = 14.7770, Discriminator Loss = 12.7080 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 174: Generator Loss = 10.0413, Discriminator Loss = 9.8186\n",
      "Epoch 174: Validation Loss = 6.2204\n",
      "Epoch 174: Validation Loss = 6.2204 Generator Loss = 10.0413, Discriminator Loss = 9.8186 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 175: Generator Loss = 8.7834, Discriminator Loss = 9.4664\n",
      "Epoch 175: Validation Loss = 6.3724\n",
      "Epoch 175: Validation Loss = 6.3724 Generator Loss = 8.7834, Discriminator Loss = 9.4664 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 176: Generator Loss = 10.1831, Discriminator Loss = 9.7447\n",
      "Epoch 176: Validation Loss = 6.1520\n",
      "Epoch 176: Validation Loss = 6.1520 Generator Loss = 10.1831, Discriminator Loss = 9.7447 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 177: Generator Loss = 9.0680, Discriminator Loss = 9.5049\n",
      "Epoch 177: Validation Loss = 6.3486\n",
      "Epoch 177: Validation Loss = 6.3486 Generator Loss = 9.0680, Discriminator Loss = 9.5049 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 178: Generator Loss = 10.1333, Discriminator Loss = 9.8160\n",
      "Epoch 178: Validation Loss = 6.4267\n",
      "Epoch 178: Validation Loss = 6.4267 Generator Loss = 10.1333, Discriminator Loss = 9.8160 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 179: Generator Loss = 9.3744, Discriminator Loss = 9.7814\n",
      "Epoch 179: Validation Loss = 6.1072\n",
      "Epoch 179: Validation Loss = 6.1072 Generator Loss = 9.3744, Discriminator Loss = 9.7814 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 180: Generator Loss = 11.3370, Discriminator Loss = 9.4682\n",
      "Epoch 180: Validation Loss = 6.5468\n",
      "Epoch 180: Validation Loss = 6.5468 Generator Loss = 11.3370, Discriminator Loss = 9.4682 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 181: Generator Loss = 9.1478, Discriminator Loss = 10.3694\n",
      "Epoch 181: Validation Loss = 6.3396\n",
      "Epoch 181: Validation Loss = 6.3396 Generator Loss = 9.1478, Discriminator Loss = 10.3694 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 182: Generator Loss = 9.6399, Discriminator Loss = 9.6228\n",
      "Epoch 182: Validation Loss = 6.2631\n",
      "Epoch 182: Validation Loss = 6.2631 Generator Loss = 9.6399, Discriminator Loss = 9.6228 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 183: Generator Loss = 10.1513, Discriminator Loss = 9.5383\n",
      "Epoch 183: Validation Loss = 6.1944\n",
      "Epoch 183: Validation Loss = 6.1944 Generator Loss = 10.1513, Discriminator Loss = 9.5383 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 184: Generator Loss = 10.2322, Discriminator Loss = 9.7128\n",
      "Epoch 184: Validation Loss = 6.7000\n",
      "Epoch 184: Validation Loss = 6.7000 Generator Loss = 10.2322, Discriminator Loss = 9.7128 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 185: Generator Loss = 9.0918, Discriminator Loss = 10.0502\n",
      "Epoch 185: Validation Loss = 6.3128\n",
      "Epoch 185: Validation Loss = 6.3128 Generator Loss = 9.0918, Discriminator Loss = 10.0502 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 186: Generator Loss = 10.6563, Discriminator Loss = 9.3306\n",
      "Epoch 186: Validation Loss = 5.9799\n",
      "Epoch 186: Validation Loss = 5.9799 Generator Loss = 10.6563, Discriminator Loss = 9.3306 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 187: Generator Loss = 10.0056, Discriminator Loss = 9.4045\n",
      "Epoch 187: Validation Loss = 7.0075\n",
      "Epoch 187: Validation Loss = 7.0075 Generator Loss = 10.0056, Discriminator Loss = 9.4045 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 188: Generator Loss = 9.5662, Discriminator Loss = 10.6266\n",
      "Epoch 188: Validation Loss = 6.5032\n",
      "Epoch 188: Validation Loss = 6.5032 Generator Loss = 9.5662, Discriminator Loss = 10.6266 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 189: Generator Loss = 10.5281, Discriminator Loss = 9.2164\n",
      "Epoch 189: Validation Loss = 5.2050\n",
      "Epoch 189: Validation Loss = 5.2050 Generator Loss = 10.5281, Discriminator Loss = 9.2164 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 190: Generator Loss = 15.5943, Discriminator Loss = 11.5230\n",
      "Epoch 190: Validation Loss = 12.3135\n",
      "Epoch 190: Validation Loss = 12.3135 Generator Loss = 15.5943, Discriminator Loss = 11.5230 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 191: Generator Loss = 10.9681, Discriminator Loss = 11.7913\n",
      "Epoch 191: Validation Loss = 6.1051\n",
      "Epoch 191: Validation Loss = 6.1051 Generator Loss = 10.9681, Discriminator Loss = 11.7913 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 192: Generator Loss = 8.9707, Discriminator Loss = 9.7550\n",
      "Epoch 192: Validation Loss = 5.9182\n",
      "Epoch 192: Validation Loss = 5.9182 Generator Loss = 8.9707, Discriminator Loss = 9.7550 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 193: Generator Loss = 9.2701, Discriminator Loss = 9.2678\n",
      "Epoch 193: Validation Loss = 6.3249\n",
      "Epoch 193: Validation Loss = 6.3249 Generator Loss = 9.2701, Discriminator Loss = 9.2678 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 194: Generator Loss = 9.9150, Discriminator Loss = 10.6299\n",
      "Epoch 194: Validation Loss = 6.6065\n",
      "Epoch 194: Validation Loss = 6.6065 Generator Loss = 9.9150, Discriminator Loss = 10.6299 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 195: Generator Loss = 10.8998, Discriminator Loss = 9.8004\n",
      "Epoch 195: Validation Loss = 6.0992\n",
      "Epoch 195: Validation Loss = 6.0992 Generator Loss = 10.8998, Discriminator Loss = 9.8004 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 196: Generator Loss = 9.4129, Discriminator Loss = 9.4462\n",
      "Epoch 196: Validation Loss = 6.2702\n",
      "Epoch 196: Validation Loss = 6.2702 Generator Loss = 9.4129, Discriminator Loss = 9.4462 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 197: Generator Loss = 9.0469, Discriminator Loss = 9.8941\n",
      "Epoch 197: Validation Loss = 6.4358\n",
      "Epoch 197: Validation Loss = 6.4358 Generator Loss = 9.0469, Discriminator Loss = 9.8941 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 198: Generator Loss = 10.8108, Discriminator Loss = 10.2735\n",
      "Epoch 198: Validation Loss = 6.6335\n",
      "Epoch 198: Validation Loss = 6.6335 Generator Loss = 10.8108, Discriminator Loss = 10.2735 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 199: Generator Loss = 8.9834, Discriminator Loss = 9.8245\n",
      "Epoch 199: Validation Loss = 6.4022\n",
      "Epoch 199: Validation Loss = 6.4022 Generator Loss = 8.9834, Discriminator Loss = 9.8245 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n",
      "Epoch 200: Generator Loss = 9.5529, Discriminator Loss = 10.0012\n",
      "Epoch 200: Validation Loss = 6.5515\n",
      "Epoch 200: Validation Loss = 6.5515 Generator Loss = 9.5529, Discriminator Loss = 10.0012 \n",
      " (Best G: 5.0693 Best D: 4.6944) \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generating New Data for each method"
   ],
   "id": "be098fdb6fd7c1b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:50:04.171241Z",
     "start_time": "2025-05-18T11:49:51.031361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gan = GAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/GAN\",load_pretrained=True)\n",
    "cnngan = CNNGAN(IMG_SIZE,CHANNELS,LATENT_DIM,device,result_dir=\"results/CNNGAN\",load_pretrained=True)\n",
    "vae = VAE(device=device,result_dir='results/VAE',load_pretrained=True).to(device)\n",
    "cnnvae = CNNVAE(device=device,result_dir='results/CNNVAE',load_pretrained=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# gan.generate_new_data(num_samples=300)\n",
    "# cnngan.generate_new_data(num_samples=300)\n",
    "\n",
    "vae.generate_similar_data(dataset, num_samples=100)\n",
    "cnnvae.generate_similar_data(dataset, num_samples=100)\n"
   ],
   "id": "4a483222518fbdb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded best generator weights.\n",
      "[INFO] Loaded best discriminator weights.\n",
      "[INFO] Loaded best generator weights.\n",
      "[INFO] Loaded best discriminator weights.\n",
      "[INFO] Loaded best VAE weights.\n",
      "[INFO] Loaded best VAE weights.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f69f811c5a97506"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dirs_to_create = [\n",
    "    'res_net_model/',\n",
    "    'res_net_model/original_dataset/',\n",
    "    'res_net_model/oversampled_dataset/',\n",
    "    'res_net_model/oversampled_dataset/CNNGAN/',\n",
    "    'res_net_model/oversampled_dataset/GAN/',\n",
    "    'res_net_model/oversampled_dataset/CNNVAE/',\n",
    "    'res_net_model/oversampled_dataset/VAE/',\n",
    "    'res_net_model/oversampled_dataset/',\n",
    "    'res_net_model/synthetic_dataset/CNNGAN/',\n",
    "    'res_net_model/synthetic_dataset/GAN/',\n",
    "    'res_net_model/synthetic_dataset/CNNVAE/',\n",
    "    'res_net_model/synthetic_dataset/VAE/',\n",
    "    'res_net_model/over_oversampled_dataset/',\n",
    "    'res_net_model/over_oversampled_dataset/CNNGAN/',\n",
    "    'res_net_model/over_oversampled_dataset/GAN/',\n",
    "    'res_net_model/over_oversampled_dataset/CNNVAE/',\n",
    "    'res_net_model/over_oversampled_dataset/VAE/',\n",
    "]\n",
    "\n",
    "for dir_path in dirs_to_create:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=2,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:52:00.687269Z",
     "start_time": "2025-05-18T11:52:00.680921Z"
    }
   },
   "id": "7af14d7d449386a1",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6f2363fa3cede6e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "class CapsuleDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 pos_dir: str,\n",
    "                 neg_dirs,\n",
    "                 transform=None,\n",
    "                 max_per_dir: int = None):\n",
    "        \"\"\"\n",
    "        neg_dirs: either a single path string or a list of paths\n",
    "        max_per_dir: if set, take at most this many files from EACH directory\n",
    "        \"\"\"\n",
    "        if isinstance(neg_dirs, str):\n",
    "            neg_dirs = [neg_dirs]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        def _gather_files(directory):\n",
    "            files = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "            if max_per_dir is not None:\n",
    "                return files[:max_per_dir]\n",
    "            return files\n",
    "\n",
    "        # positives (label=0)\n",
    "        pos_files = sorted(glob.glob(os.path.join(pos_dir, \"*\")))\n",
    "        self.samples += [(p, 0) for p in pos_files]\n",
    "\n",
    "        # negatives (label=1)\n",
    "        for nd in neg_dirs:\n",
    "            neg_files = _gather_files(nd)\n",
    "            self.samples += [(p, 1) for p in neg_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# hyper‑parameters\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "EPOCHS = 25\n",
    "# normalization\n",
    "if CHANNELS == 3:\n",
    "    normalize = transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "else:\n",
    "    normalize = transforms.Normalize([0.5], [0.5])\n",
    "\n",
    "# your augmentations + normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05,0.05), scale=(1.1,1.15), fill=255),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "# download or mount your Kaggle data however you like; suppose:\n",
    "# path = \".../capsule-defects\"\n",
    "pos_folder = os.path.join(path, \"capsule/positive\")\n",
    "neg_folder = os.path.join(path, \"capsule/negative\")\n",
    "print(len(glob.glob(os.path.join(pos_folder, \"*\"))))\n",
    "print(len(glob.glob(os.path.join(neg_folder, \"*\"))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:52:05.204413Z",
     "start_time": "2025-05-18T11:52:04.755943Z"
    }
   },
   "id": "f8a2b3c17f730679",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Original Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef545229f0077afd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    }
   ],
   "source": [
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:52:09.047952Z",
     "start_time": "2025-05-18T11:52:09.041773Z"
    }
   },
   "id": "d1168f38bfa0c58",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Oversampled Dataset (Equality in classes' instances)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21ea064fff490045"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dcc7463c9979bb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T00:53:52.310466Z",
     "start_time": "2025-05-18T00:53:52.303944Z"
    }
   },
   "id": "286d588a171d34b0",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec4fe8e16342fc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f5a077254be244"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/GAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T01:40:43.495720Z",
     "start_time": "2025-05-18T01:40:43.477235Z"
    }
   },
   "id": "6ed01316b027481b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.6045 Acc: 0.7123 | Val Loss: 0.5538 Acc: 0.6941\n",
      "Epoch 2/25 | Train Loss: 0.4889 Acc: 0.7626 | Val Loss: 0.5527 Acc: 0.6667\n",
      "Epoch 3/25 | Train Loss: 0.4821 Acc: 0.7763 | Val Loss: 0.5211 Acc: 0.7123\n",
      "Epoch 4/25 | Train Loss: 0.4036 Acc: 0.7854 | Val Loss: 0.5602 Acc: 0.7169\n",
      "Epoch 5/25 | Train Loss: 0.4133 Acc: 0.7900 | Val Loss: 0.5249 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.4047 Acc: 0.8265 | Val Loss: 0.4695 Acc: 0.7260\n",
      "Epoch 7/25 | Train Loss: 0.4467 Acc: 0.7991 | Val Loss: 0.6075 Acc: 0.6164\n",
      "Epoch 8/25 | Train Loss: 0.3359 Acc: 0.8493 | Val Loss: 0.4613 Acc: 0.7763\n",
      "Epoch 9/25 | Train Loss: 0.3102 Acc: 0.8767 | Val Loss: 0.5540 Acc: 0.7443\n",
      "Epoch 10/25 | Train Loss: 0.2942 Acc: 0.8539 | Val Loss: 0.4927 Acc: 0.7489\n",
      "Epoch 11/25 | Train Loss: 0.2722 Acc: 0.8630 | Val Loss: 0.4516 Acc: 0.7763\n",
      "Epoch 12/25 | Train Loss: 0.3189 Acc: 0.8630 | Val Loss: 0.3970 Acc: 0.7945\n",
      "Epoch 13/25 | Train Loss: 0.2433 Acc: 0.9132 | Val Loss: 0.4203 Acc: 0.7945\n",
      "Epoch 14/25 | Train Loss: 0.2488 Acc: 0.8858 | Val Loss: 0.4341 Acc: 0.7854\n",
      "Epoch 15/25 | Train Loss: 0.2880 Acc: 0.8584 | Val Loss: 0.4037 Acc: 0.8265\n",
      "Epoch 16/25 | Train Loss: 0.2550 Acc: 0.8950 | Val Loss: 0.4363 Acc: 0.7991\n",
      "Epoch 17/25 | Train Loss: 0.2634 Acc: 0.8995 | Val Loss: 0.4276 Acc: 0.8037\n",
      "Epoch 18/25 | Train Loss: 0.2571 Acc: 0.8858 | Val Loss: 0.3787 Acc: 0.8265\n",
      "Epoch 19/25 | Train Loss: 0.2409 Acc: 0.8767 | Val Loss: 0.3862 Acc: 0.7991\n",
      "Epoch 20/25 | Train Loss: 0.2327 Acc: 0.9041 | Val Loss: 0.4502 Acc: 0.7854\n",
      "Epoch 21/25 | Train Loss: 0.2679 Acc: 0.8858 | Val Loss: 0.3797 Acc: 0.8128\n",
      "Epoch 22/25 | Train Loss: 0.2439 Acc: 0.8904 | Val Loss: 0.3782 Acc: 0.8174\n",
      "Epoch 23/25 | Train Loss: 0.2611 Acc: 0.8721 | Val Loss: 0.4060 Acc: 0.7854\n",
      "Epoch 24/25 | Train Loss: 0.2824 Acc: 0.8858 | Val Loss: 0.3964 Acc: 0.8037\n",
      "Epoch 25/25 | Train Loss: 0.2886 Acc: 0.8813 | Val Loss: 0.4344 Acc: 0.8037\n",
      "Fold 1 Test Accuracy: 0.7854\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6649 Acc: 0.6301 | Val Loss: 0.5056 Acc: 0.8037\n",
      "Epoch 2/25 | Train Loss: 0.5722 Acc: 0.6575 | Val Loss: 0.4414 Acc: 0.7489\n",
      "Epoch 3/25 | Train Loss: 0.6075 Acc: 0.6895 | Val Loss: 0.4684 Acc: 0.8082\n",
      "Epoch 4/25 | Train Loss: 0.5926 Acc: 0.6849 | Val Loss: 0.5562 Acc: 0.6484\n",
      "Epoch 5/25 | Train Loss: 0.6774 Acc: 0.6119 | Val Loss: 0.3958 Acc: 0.7945\n",
      "Epoch 6/25 | Train Loss: 0.5459 Acc: 0.7397 | Val Loss: 0.5273 Acc: 0.7534\n",
      "Epoch 7/25 | Train Loss: 0.5341 Acc: 0.7489 | Val Loss: 0.5203 Acc: 0.7032\n",
      "Epoch 8/25 | Train Loss: 0.5345 Acc: 0.7489 | Val Loss: 0.4118 Acc: 0.7763\n",
      "Epoch 9/25 | Train Loss: 0.4734 Acc: 0.7580 | Val Loss: 0.3942 Acc: 0.8082\n",
      "Epoch 10/25 | Train Loss: 0.4891 Acc: 0.7626 | Val Loss: 0.3840 Acc: 0.8265\n",
      "Epoch 11/25 | Train Loss: 0.4586 Acc: 0.7763 | Val Loss: 0.3903 Acc: 0.8037\n",
      "Epoch 12/25 | Train Loss: 0.4350 Acc: 0.7671 | Val Loss: 0.3273 Acc: 0.8539\n",
      "Epoch 13/25 | Train Loss: 0.4700 Acc: 0.7443 | Val Loss: 0.3736 Acc: 0.8219\n",
      "Epoch 14/25 | Train Loss: 0.4240 Acc: 0.7854 | Val Loss: 0.3618 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.3971 Acc: 0.8128 | Val Loss: 0.3488 Acc: 0.8493\n",
      "Epoch 16/25 | Train Loss: 0.3740 Acc: 0.8311 | Val Loss: 0.3459 Acc: 0.8311\n",
      "Epoch 17/25 | Train Loss: 0.3952 Acc: 0.8128 | Val Loss: 0.3244 Acc: 0.8630\n",
      "Epoch 18/25 | Train Loss: 0.4229 Acc: 0.7900 | Val Loss: 0.3572 Acc: 0.8493\n",
      "Epoch 19/25 | Train Loss: 0.3950 Acc: 0.8174 | Val Loss: 0.3107 Acc: 0.8493\n",
      "Epoch 20/25 | Train Loss: 0.4004 Acc: 0.8082 | Val Loss: 0.3596 Acc: 0.8219\n",
      "Epoch 21/25 | Train Loss: 0.3773 Acc: 0.8265 | Val Loss: 0.3789 Acc: 0.8174\n",
      "Epoch 22/25 | Train Loss: 0.3583 Acc: 0.8265 | Val Loss: 0.3143 Acc: 0.8813\n",
      "Epoch 23/25 | Train Loss: 0.4002 Acc: 0.8128 | Val Loss: 0.3231 Acc: 0.8539\n",
      "Epoch 24/25 | Train Loss: 0.4067 Acc: 0.7900 | Val Loss: 0.3296 Acc: 0.8721\n",
      "Epoch 25/25 | Train Loss: 0.4001 Acc: 0.8128 | Val Loss: 0.3212 Acc: 0.8584\n",
      "Fold 2 Test Accuracy: 0.8447\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5814 Acc: 0.6712 | Val Loss: 0.5198 Acc: 0.7580\n",
      "Epoch 2/25 | Train Loss: 0.5450 Acc: 0.7215 | Val Loss: 1.2964 Acc: 0.4977\n",
      "Epoch 3/25 | Train Loss: 0.5908 Acc: 0.6804 | Val Loss: 0.4860 Acc: 0.6941\n",
      "Epoch 4/25 | Train Loss: 0.5145 Acc: 0.7580 | Val Loss: 0.7328 Acc: 0.6073\n",
      "Epoch 5/25 | Train Loss: 0.5206 Acc: 0.7306 | Val Loss: 0.5036 Acc: 0.7306\n",
      "Epoch 6/25 | Train Loss: 0.4644 Acc: 0.7717 | Val Loss: 0.4141 Acc: 0.7900\n",
      "Epoch 7/25 | Train Loss: 0.3909 Acc: 0.8128 | Val Loss: 0.5601 Acc: 0.7215\n",
      "Epoch 8/25 | Train Loss: 0.3934 Acc: 0.8311 | Val Loss: 0.4243 Acc: 0.7945\n",
      "Epoch 9/25 | Train Loss: 0.4080 Acc: 0.8311 | Val Loss: 0.4243 Acc: 0.7991\n",
      "Epoch 10/25 | Train Loss: 0.3440 Acc: 0.8219 | Val Loss: 0.3562 Acc: 0.8356\n",
      "Epoch 11/25 | Train Loss: 0.3568 Acc: 0.8402 | Val Loss: 0.3940 Acc: 0.8128\n",
      "Epoch 12/25 | Train Loss: 0.2898 Acc: 0.8904 | Val Loss: 0.3626 Acc: 0.8265\n",
      "Epoch 13/25 | Train Loss: 0.3843 Acc: 0.8128 | Val Loss: 0.3251 Acc: 0.8356\n",
      "Epoch 14/25 | Train Loss: 0.3793 Acc: 0.8265 | Val Loss: 0.3506 Acc: 0.8493\n",
      "Epoch 15/25 | Train Loss: 0.2824 Acc: 0.8721 | Val Loss: 0.3500 Acc: 0.8447\n",
      "Epoch 16/25 | Train Loss: 0.3241 Acc: 0.8721 | Val Loss: 0.2970 Acc: 0.8721\n",
      "Epoch 17/25 | Train Loss: 0.2882 Acc: 0.8630 | Val Loss: 0.3204 Acc: 0.8676\n",
      "Epoch 18/25 | Train Loss: 0.3048 Acc: 0.8721 | Val Loss: 0.3038 Acc: 0.8676\n",
      "Epoch 19/25 | Train Loss: 0.2775 Acc: 0.9087 | Val Loss: 0.3236 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.2896 Acc: 0.8676 | Val Loss: 0.3258 Acc: 0.8493\n",
      "Epoch 21/25 | Train Loss: 0.3083 Acc: 0.8584 | Val Loss: 0.3568 Acc: 0.8174\n",
      "Epoch 22/25 | Train Loss: 0.2837 Acc: 0.8858 | Val Loss: 0.3526 Acc: 0.8402\n",
      "Epoch 23/25 | Train Loss: 0.3151 Acc: 0.8584 | Val Loss: 0.3233 Acc: 0.8539\n",
      "Epoch 24/25 | Train Loss: 0.3220 Acc: 0.8676 | Val Loss: 0.3400 Acc: 0.8402\n",
      "Epoch 25/25 | Train Loss: 0.2951 Acc: 0.8676 | Val Loss: 0.3324 Acc: 0.8311\n",
      "Fold 3 Test Accuracy: 0.8493\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5756 Acc: 0.6986 | Val Loss: 1.0425 Acc: 0.7397\n",
      "Epoch 2/25 | Train Loss: 0.6349 Acc: 0.7169 | Val Loss: 0.5508 Acc: 0.7260\n",
      "Epoch 3/25 | Train Loss: 0.4993 Acc: 0.7260 | Val Loss: 0.5206 Acc: 0.6804\n",
      "Epoch 4/25 | Train Loss: 0.4665 Acc: 0.7580 | Val Loss: 0.4739 Acc: 0.7443\n",
      "Epoch 5/25 | Train Loss: 0.4295 Acc: 0.7626 | Val Loss: 0.5599 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.5016 Acc: 0.7489 | Val Loss: 0.5651 Acc: 0.6256\n",
      "Epoch 7/25 | Train Loss: 0.4338 Acc: 0.7945 | Val Loss: 0.5741 Acc: 0.7671\n",
      "Epoch 8/25 | Train Loss: 0.4860 Acc: 0.7717 | Val Loss: 0.4827 Acc: 0.7534\n",
      "Epoch 9/25 | Train Loss: 0.4424 Acc: 0.7580 | Val Loss: 0.4366 Acc: 0.7626\n",
      "Epoch 10/25 | Train Loss: 0.4125 Acc: 0.7991 | Val Loss: 0.4541 Acc: 0.7534\n",
      "Epoch 11/25 | Train Loss: 0.3995 Acc: 0.8265 | Val Loss: 0.4162 Acc: 0.7945\n",
      "Epoch 12/25 | Train Loss: 0.3772 Acc: 0.8539 | Val Loss: 0.4303 Acc: 0.7991\n",
      "Epoch 13/25 | Train Loss: 0.3809 Acc: 0.8265 | Val Loss: 0.4461 Acc: 0.7626\n",
      "Epoch 14/25 | Train Loss: 0.3402 Acc: 0.8584 | Val Loss: 0.4099 Acc: 0.7991\n",
      "Epoch 15/25 | Train Loss: 0.3825 Acc: 0.8082 | Val Loss: 0.3954 Acc: 0.7808\n",
      "Epoch 16/25 | Train Loss: 0.3709 Acc: 0.8539 | Val Loss: 0.4156 Acc: 0.7717\n",
      "Epoch 17/25 | Train Loss: 0.3368 Acc: 0.8539 | Val Loss: 0.4158 Acc: 0.7900\n",
      "Epoch 18/25 | Train Loss: 0.3858 Acc: 0.8219 | Val Loss: 0.4094 Acc: 0.8082\n",
      "Epoch 19/25 | Train Loss: 0.3535 Acc: 0.8447 | Val Loss: 0.4322 Acc: 0.7671\n",
      "Epoch 20/25 | Train Loss: 0.3735 Acc: 0.8174 | Val Loss: 0.4099 Acc: 0.7991\n",
      "Epoch 21/25 | Train Loss: 0.3539 Acc: 0.8311 | Val Loss: 0.3955 Acc: 0.8128\n",
      "Epoch 22/25 | Train Loss: 0.3626 Acc: 0.8402 | Val Loss: 0.4006 Acc: 0.8174\n",
      "Epoch 23/25 | Train Loss: 0.3488 Acc: 0.8447 | Val Loss: 0.3922 Acc: 0.8128\n",
      "Epoch 24/25 | Train Loss: 0.3522 Acc: 0.8311 | Val Loss: 0.3843 Acc: 0.8082\n",
      "Epoch 25/25 | Train Loss: 0.3677 Acc: 0.8219 | Val Loss: 0.3913 Acc: 0.8082\n",
      "Fold 4 Test Accuracy: 0.8265\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.7381 Acc: 0.6256 | Val Loss: 0.5096 Acc: 0.7580\n",
      "Epoch 2/25 | Train Loss: 0.4718 Acc: 0.7489 | Val Loss: 0.6652 Acc: 0.7260\n",
      "Epoch 3/25 | Train Loss: 0.5057 Acc: 0.7443 | Val Loss: 0.5160 Acc: 0.7123\n",
      "Epoch 4/25 | Train Loss: 0.4774 Acc: 0.7306 | Val Loss: 0.6157 Acc: 0.6164\n",
      "Epoch 5/25 | Train Loss: 0.4899 Acc: 0.7580 | Val Loss: 0.5138 Acc: 0.7169\n",
      "Epoch 6/25 | Train Loss: 0.4388 Acc: 0.7717 | Val Loss: 0.5398 Acc: 0.6849\n",
      "Epoch 7/25 | Train Loss: 0.4778 Acc: 0.7717 | Val Loss: 0.6729 Acc: 0.5936\n",
      "Epoch 8/25 | Train Loss: 0.4310 Acc: 0.7489 | Val Loss: 0.4767 Acc: 0.7443\n",
      "Epoch 9/25 | Train Loss: 0.3912 Acc: 0.8311 | Val Loss: 0.4605 Acc: 0.7489\n",
      "Epoch 10/25 | Train Loss: 0.3959 Acc: 0.8174 | Val Loss: 0.4499 Acc: 0.7580\n",
      "Epoch 11/25 | Train Loss: 0.3721 Acc: 0.8447 | Val Loss: 0.4483 Acc: 0.7717\n",
      "Epoch 12/25 | Train Loss: 0.3701 Acc: 0.8447 | Val Loss: 0.4574 Acc: 0.7717\n",
      "Epoch 13/25 | Train Loss: 0.4162 Acc: 0.7763 | Val Loss: 0.4662 Acc: 0.7626\n",
      "Epoch 14/25 | Train Loss: 0.3692 Acc: 0.7945 | Val Loss: 0.4183 Acc: 0.7854\n",
      "Epoch 15/25 | Train Loss: 0.3547 Acc: 0.8311 | Val Loss: 0.4386 Acc: 0.7854\n",
      "Epoch 16/25 | Train Loss: 0.3342 Acc: 0.8174 | Val Loss: 0.4301 Acc: 0.7763\n",
      "Epoch 17/25 | Train Loss: 0.3751 Acc: 0.8219 | Val Loss: 0.4315 Acc: 0.7991\n",
      "Epoch 18/25 | Train Loss: 0.3785 Acc: 0.8174 | Val Loss: 0.3980 Acc: 0.8311\n",
      "Epoch 19/25 | Train Loss: 0.3421 Acc: 0.8447 | Val Loss: 0.4229 Acc: 0.7671\n",
      "Epoch 20/25 | Train Loss: 0.3873 Acc: 0.8402 | Val Loss: 0.4305 Acc: 0.7671\n",
      "Epoch 21/25 | Train Loss: 0.3453 Acc: 0.8584 | Val Loss: 0.4363 Acc: 0.7671\n",
      "Epoch 22/25 | Train Loss: 0.3299 Acc: 0.8584 | Val Loss: 0.4348 Acc: 0.7808\n",
      "Epoch 23/25 | Train Loss: 0.3132 Acc: 0.8676 | Val Loss: 0.4334 Acc: 0.7717\n",
      "Epoch 24/25 | Train Loss: 0.3420 Acc: 0.8402 | Val Loss: 0.4387 Acc: 0.7808\n",
      "Epoch 25/25 | Train Loss: 0.3322 Acc: 0.8493 | Val Loss: 0.4331 Acc: 0.7854\n",
      "Fold 5 Test Accuracy: 0.7763\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.6497 Acc: 0.6393 | Val Loss: 0.4737 Acc: 0.7489\n",
      "Epoch 2/25 | Train Loss: 0.4976 Acc: 0.7671 | Val Loss: 0.8202 Acc: 0.5251\n",
      "Epoch 3/25 | Train Loss: 0.5915 Acc: 0.7032 | Val Loss: 0.5684 Acc: 0.6438\n",
      "Epoch 4/25 | Train Loss: 0.5080 Acc: 0.7443 | Val Loss: 0.4842 Acc: 0.7808\n",
      "Epoch 5/25 | Train Loss: 0.4599 Acc: 0.7580 | Val Loss: 0.4628 Acc: 0.7580\n",
      "Epoch 6/25 | Train Loss: 0.5607 Acc: 0.6986 | Val Loss: 0.4610 Acc: 0.7580\n",
      "Epoch 7/25 | Train Loss: 0.4494 Acc: 0.7763 | Val Loss: 0.4666 Acc: 0.7854\n",
      "Epoch 8/25 | Train Loss: 0.4233 Acc: 0.7854 | Val Loss: 0.3836 Acc: 0.8174\n",
      "Epoch 9/25 | Train Loss: 0.3810 Acc: 0.8356 | Val Loss: 0.3986 Acc: 0.7945\n",
      "Epoch 10/25 | Train Loss: 0.3965 Acc: 0.8128 | Val Loss: 0.4038 Acc: 0.8128\n",
      "Epoch 11/25 | Train Loss: 0.4001 Acc: 0.8082 | Val Loss: 0.4172 Acc: 0.8037\n",
      "Epoch 12/25 | Train Loss: 0.3807 Acc: 0.8082 | Val Loss: 0.3944 Acc: 0.8219\n",
      "Epoch 13/25 | Train Loss: 0.3609 Acc: 0.8493 | Val Loss: 0.3599 Acc: 0.8174\n",
      "Epoch 14/25 | Train Loss: 0.4628 Acc: 0.7626 | Val Loss: 0.4044 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3780 Acc: 0.8128 | Val Loss: 0.3749 Acc: 0.8082\n",
      "Epoch 16/25 | Train Loss: 0.3764 Acc: 0.8219 | Val Loss: 0.3955 Acc: 0.8037\n",
      "Epoch 17/25 | Train Loss: 0.3801 Acc: 0.8265 | Val Loss: 0.4284 Acc: 0.8128\n",
      "Epoch 18/25 | Train Loss: 0.3759 Acc: 0.8082 | Val Loss: 0.3881 Acc: 0.8265\n",
      "Epoch 19/25 | Train Loss: 0.3615 Acc: 0.8539 | Val Loss: 0.4169 Acc: 0.7945\n",
      "Epoch 20/25 | Train Loss: 0.3559 Acc: 0.8265 | Val Loss: 0.4073 Acc: 0.8219\n",
      "Epoch 21/25 | Train Loss: 0.3688 Acc: 0.8311 | Val Loss: 0.3861 Acc: 0.8174\n",
      "Epoch 22/25 | Train Loss: 0.3250 Acc: 0.8584 | Val Loss: 0.3915 Acc: 0.8082\n",
      "Epoch 23/25 | Train Loss: 0.3496 Acc: 0.8493 | Val Loss: 0.4023 Acc: 0.8174\n",
      "Epoch 24/25 | Train Loss: 0.3481 Acc: 0.8356 | Val Loss: 0.3822 Acc: 0.8128\n",
      "Epoch 25/25 | Train Loss: 0.3569 Acc: 0.8219 | Val Loss: 0.3857 Acc: 0.8082\n",
      "Fold 6 Test Accuracy: 0.8265\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.7303 Acc: 0.5982 | Val Loss: 0.5010 Acc: 0.8037\n",
      "Epoch 2/25 | Train Loss: 0.5626 Acc: 0.6393 | Val Loss: 0.5693 Acc: 0.5890\n",
      "Epoch 3/25 | Train Loss: 0.6631 Acc: 0.6530 | Val Loss: 0.7245 Acc: 0.5753\n",
      "Epoch 4/25 | Train Loss: 0.5226 Acc: 0.7489 | Val Loss: 0.4621 Acc: 0.7534\n",
      "Epoch 5/25 | Train Loss: 0.6319 Acc: 0.7032 | Val Loss: 0.4686 Acc: 0.7763\n",
      "Epoch 6/25 | Train Loss: 0.4804 Acc: 0.7671 | Val Loss: 0.6819 Acc: 0.6758\n",
      "Epoch 7/25 | Train Loss: 0.4964 Acc: 0.7123 | Val Loss: 0.4543 Acc: 0.7808\n",
      "Epoch 8/25 | Train Loss: 0.4860 Acc: 0.7580 | Val Loss: 0.4337 Acc: 0.7808\n",
      "Epoch 9/25 | Train Loss: 0.4677 Acc: 0.7580 | Val Loss: 0.4636 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.4353 Acc: 0.7991 | Val Loss: 0.4484 Acc: 0.7443\n",
      "Epoch 11/25 | Train Loss: 0.4293 Acc: 0.8037 | Val Loss: 0.4124 Acc: 0.7900\n",
      "Epoch 12/25 | Train Loss: 0.4237 Acc: 0.7900 | Val Loss: 0.3907 Acc: 0.8128\n",
      "Epoch 13/25 | Train Loss: 0.4097 Acc: 0.7808 | Val Loss: 0.4334 Acc: 0.7854\n",
      "Epoch 14/25 | Train Loss: 0.4439 Acc: 0.7854 | Val Loss: 0.4273 Acc: 0.7626\n",
      "Epoch 15/25 | Train Loss: 0.4321 Acc: 0.8037 | Val Loss: 0.4137 Acc: 0.7945\n",
      "Epoch 16/25 | Train Loss: 0.4220 Acc: 0.7854 | Val Loss: 0.4364 Acc: 0.8128\n",
      "Epoch 17/25 | Train Loss: 0.3975 Acc: 0.7991 | Val Loss: 0.3840 Acc: 0.8128\n",
      "Epoch 18/25 | Train Loss: 0.4146 Acc: 0.7854 | Val Loss: 0.3834 Acc: 0.8082\n",
      "Epoch 19/25 | Train Loss: 0.4795 Acc: 0.7580 | Val Loss: 0.3946 Acc: 0.7991\n",
      "Epoch 20/25 | Train Loss: 0.4173 Acc: 0.7945 | Val Loss: 0.4054 Acc: 0.7854\n",
      "Epoch 21/25 | Train Loss: 0.3797 Acc: 0.8265 | Val Loss: 0.3867 Acc: 0.7900\n",
      "Epoch 22/25 | Train Loss: 0.4269 Acc: 0.7580 | Val Loss: 0.3485 Acc: 0.8311\n",
      "Epoch 23/25 | Train Loss: 0.3964 Acc: 0.8265 | Val Loss: 0.3885 Acc: 0.8037\n",
      "Epoch 24/25 | Train Loss: 0.3657 Acc: 0.8402 | Val Loss: 0.3994 Acc: 0.7945\n",
      "Epoch 25/25 | Train Loss: 0.4097 Acc: 0.7991 | Val Loss: 0.3593 Acc: 0.8174\n",
      "Fold 7 Test Accuracy: 0.7900\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.5706 Acc: 0.7123 | Val Loss: 0.6418 Acc: 0.7169\n",
      "Epoch 2/25 | Train Loss: 0.4842 Acc: 0.7489 | Val Loss: 0.6293 Acc: 0.6301\n",
      "Epoch 3/25 | Train Loss: 0.4536 Acc: 0.7717 | Val Loss: 0.7064 Acc: 0.5982\n",
      "Epoch 4/25 | Train Loss: 0.6547 Acc: 0.7169 | Val Loss: 0.5411 Acc: 0.6712\n",
      "Epoch 5/25 | Train Loss: 0.4619 Acc: 0.7626 | Val Loss: 0.5513 Acc: 0.6986\n",
      "Epoch 6/25 | Train Loss: 0.4239 Acc: 0.7854 | Val Loss: 0.6192 Acc: 0.7215\n",
      "Epoch 7/25 | Train Loss: 0.4383 Acc: 0.7854 | Val Loss: 0.5598 Acc: 0.7443\n",
      "Epoch 8/25 | Train Loss: 0.3708 Acc: 0.8128 | Val Loss: 0.5175 Acc: 0.7489\n",
      "Epoch 9/25 | Train Loss: 0.3552 Acc: 0.8402 | Val Loss: 0.5102 Acc: 0.6895\n",
      "Epoch 10/25 | Train Loss: 0.3343 Acc: 0.8584 | Val Loss: 0.5272 Acc: 0.7397\n",
      "Epoch 11/25 | Train Loss: 0.3464 Acc: 0.8311 | Val Loss: 0.5493 Acc: 0.7260\n",
      "Epoch 12/25 | Train Loss: 0.3020 Acc: 0.8539 | Val Loss: 0.5042 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3426 Acc: 0.8356 | Val Loss: 0.4649 Acc: 0.7808\n",
      "Epoch 14/25 | Train Loss: 0.3258 Acc: 0.8630 | Val Loss: 0.4516 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3398 Acc: 0.8447 | Val Loss: 0.4566 Acc: 0.7945\n",
      "Epoch 16/25 | Train Loss: 0.2909 Acc: 0.8950 | Val Loss: 0.4409 Acc: 0.7763\n",
      "Epoch 17/25 | Train Loss: 0.3191 Acc: 0.8493 | Val Loss: 0.4436 Acc: 0.7671\n",
      "Epoch 18/25 | Train Loss: 0.3186 Acc: 0.8630 | Val Loss: 0.4807 Acc: 0.7580\n",
      "Epoch 19/25 | Train Loss: 0.3461 Acc: 0.8447 | Val Loss: 0.4779 Acc: 0.7671\n",
      "Epoch 20/25 | Train Loss: 0.3188 Acc: 0.8493 | Val Loss: 0.4717 Acc: 0.7854\n",
      "Epoch 21/25 | Train Loss: 0.2994 Acc: 0.8676 | Val Loss: 0.4837 Acc: 0.7671\n",
      "Epoch 22/25 | Train Loss: 0.3053 Acc: 0.8630 | Val Loss: 0.4553 Acc: 0.8128\n",
      "Epoch 23/25 | Train Loss: 0.3300 Acc: 0.8493 | Val Loss: 0.4713 Acc: 0.7763\n",
      "Epoch 24/25 | Train Loss: 0.3193 Acc: 0.8721 | Val Loss: 0.4654 Acc: 0.7626\n",
      "Epoch 25/25 | Train Loss: 0.3209 Acc: 0.8356 | Val Loss: 0.4858 Acc: 0.7489\n",
      "Fold 8 Test Accuracy: 0.7900\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.7398 Acc: 0.5479 | Val Loss: 0.6867 Acc: 0.7763\n",
      "Epoch 2/25 | Train Loss: 0.5683 Acc: 0.6986 | Val Loss: 0.6195 Acc: 0.7763\n",
      "Epoch 3/25 | Train Loss: 0.5924 Acc: 0.7123 | Val Loss: 0.4828 Acc: 0.7626\n",
      "Epoch 4/25 | Train Loss: 0.5956 Acc: 0.6986 | Val Loss: 0.5057 Acc: 0.6986\n",
      "Epoch 5/25 | Train Loss: 0.5153 Acc: 0.7123 | Val Loss: 0.5095 Acc: 0.7626\n",
      "Epoch 6/25 | Train Loss: 0.5424 Acc: 0.7215 | Val Loss: 0.4235 Acc: 0.8037\n",
      "Epoch 7/25 | Train Loss: 0.4767 Acc: 0.7671 | Val Loss: 0.4948 Acc: 0.6895\n",
      "Epoch 8/25 | Train Loss: 0.4998 Acc: 0.7489 | Val Loss: 0.4333 Acc: 0.7580\n",
      "Epoch 9/25 | Train Loss: 0.5004 Acc: 0.7123 | Val Loss: 0.4003 Acc: 0.7854\n",
      "Epoch 10/25 | Train Loss: 0.4411 Acc: 0.7717 | Val Loss: 0.4136 Acc: 0.7991\n",
      "Epoch 11/25 | Train Loss: 0.4854 Acc: 0.7534 | Val Loss: 0.3889 Acc: 0.8082\n",
      "Epoch 12/25 | Train Loss: 0.3919 Acc: 0.8037 | Val Loss: 0.4116 Acc: 0.8128\n",
      "Epoch 13/25 | Train Loss: 0.3959 Acc: 0.8082 | Val Loss: 0.3668 Acc: 0.8311\n",
      "Epoch 14/25 | Train Loss: 0.4301 Acc: 0.7763 | Val Loss: 0.3779 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.3762 Acc: 0.8219 | Val Loss: 0.3627 Acc: 0.8174\n",
      "Epoch 16/25 | Train Loss: 0.3999 Acc: 0.8082 | Val Loss: 0.3219 Acc: 0.8630\n",
      "Epoch 17/25 | Train Loss: 0.3577 Acc: 0.8265 | Val Loss: 0.3578 Acc: 0.8128\n",
      "Epoch 18/25 | Train Loss: 0.3755 Acc: 0.8174 | Val Loss: 0.3345 Acc: 0.8630\n",
      "Epoch 19/25 | Train Loss: 0.3740 Acc: 0.8311 | Val Loss: 0.3177 Acc: 0.8447\n",
      "Epoch 20/25 | Train Loss: 0.4146 Acc: 0.8128 | Val Loss: 0.3844 Acc: 0.8174\n",
      "Epoch 21/25 | Train Loss: 0.4094 Acc: 0.7991 | Val Loss: 0.3423 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.3992 Acc: 0.7945 | Val Loss: 0.3428 Acc: 0.8356\n",
      "Epoch 23/25 | Train Loss: 0.3857 Acc: 0.7808 | Val Loss: 0.3330 Acc: 0.8447\n",
      "Epoch 24/25 | Train Loss: 0.3620 Acc: 0.8174 | Val Loss: 0.3523 Acc: 0.8539\n",
      "Epoch 25/25 | Train Loss: 0.3602 Acc: 0.8493 | Val Loss: 0.3346 Acc: 0.8584\n",
      "Fold 9 Test Accuracy: 0.8493\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5903 Acc: 0.6895 | Val Loss: 0.6122 Acc: 0.7169\n",
      "Epoch 2/25 | Train Loss: 0.5349 Acc: 0.7671 | Val Loss: 0.5571 Acc: 0.6941\n",
      "Epoch 3/25 | Train Loss: 0.5602 Acc: 0.7078 | Val Loss: 0.5674 Acc: 0.6895\n",
      "Epoch 4/25 | Train Loss: 0.5015 Acc: 0.7443 | Val Loss: 0.5052 Acc: 0.7397\n",
      "Epoch 5/25 | Train Loss: 0.4858 Acc: 0.7671 | Val Loss: 0.5038 Acc: 0.7489\n",
      "Epoch 6/25 | Train Loss: 0.4713 Acc: 0.7397 | Val Loss: 0.7393 Acc: 0.7352\n",
      "Epoch 7/25 | Train Loss: 0.5138 Acc: 0.7763 | Val Loss: 0.5508 Acc: 0.7260\n",
      "Epoch 8/25 | Train Loss: 0.4353 Acc: 0.7808 | Val Loss: 0.5292 Acc: 0.7397\n",
      "Epoch 9/25 | Train Loss: 0.3629 Acc: 0.8356 | Val Loss: 0.4469 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.3686 Acc: 0.8402 | Val Loss: 0.4665 Acc: 0.7808\n",
      "Epoch 11/25 | Train Loss: 0.3671 Acc: 0.8265 | Val Loss: 0.4555 Acc: 0.7580\n",
      "Epoch 12/25 | Train Loss: 0.3753 Acc: 0.8493 | Val Loss: 0.4595 Acc: 0.7854\n",
      "Epoch 13/25 | Train Loss: 0.3830 Acc: 0.7991 | Val Loss: 0.4255 Acc: 0.7808\n",
      "Epoch 14/25 | Train Loss: 0.3800 Acc: 0.8174 | Val Loss: 0.4693 Acc: 0.7854\n",
      "Epoch 15/25 | Train Loss: 0.3786 Acc: 0.8447 | Val Loss: 0.4308 Acc: 0.7991\n",
      "Epoch 16/25 | Train Loss: 0.3336 Acc: 0.8311 | Val Loss: 0.4332 Acc: 0.8082\n",
      "Epoch 17/25 | Train Loss: 0.3270 Acc: 0.8584 | Val Loss: 0.3908 Acc: 0.8037\n",
      "Epoch 18/25 | Train Loss: 0.3603 Acc: 0.8493 | Val Loss: 0.3935 Acc: 0.8037\n",
      "Epoch 19/25 | Train Loss: 0.3597 Acc: 0.8219 | Val Loss: 0.4108 Acc: 0.8037\n",
      "Epoch 20/25 | Train Loss: 0.3415 Acc: 0.8356 | Val Loss: 0.4391 Acc: 0.7717\n",
      "Epoch 21/25 | Train Loss: 0.3694 Acc: 0.8311 | Val Loss: 0.4121 Acc: 0.8037\n",
      "Epoch 22/25 | Train Loss: 0.3276 Acc: 0.8630 | Val Loss: 0.4441 Acc: 0.7854\n",
      "Epoch 23/25 | Train Loss: 0.3365 Acc: 0.8493 | Val Loss: 0.4297 Acc: 0.7991\n",
      "Epoch 24/25 | Train Loss: 0.3673 Acc: 0.8493 | Val Loss: 0.3978 Acc: 0.7900\n",
      "Epoch 25/25 | Train Loss: 0.3474 Acc: 0.8630 | Val Loss: 0.4267 Acc: 0.7991\n",
      "Fold 10 Test Accuracy: 0.7717\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8110, Std Dev = 0.0314\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/GAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/GAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T02:58:36.570855Z",
     "start_time": "2025-05-18T01:40:46.075243Z"
    }
   },
   "id": "a7af889be76c0fa8",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4be0eab9b9ef1744"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:54:19.802839Z",
     "start_time": "2025-05-18T11:54:19.795308Z"
    }
   },
   "id": "e14420ca7060463",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.5752 Acc: 0.7032 | Val Loss: 0.6516 Acc: 0.6758\n",
      "Epoch 2/25 | Train Loss: 0.3923 Acc: 0.8128 | Val Loss: 0.5846 Acc: 0.6484\n",
      "Epoch 3/25 | Train Loss: 0.5497 Acc: 0.7717 | Val Loss: 0.9610 Acc: 0.4977\n",
      "Epoch 4/25 | Train Loss: 0.5461 Acc: 0.7443 | Val Loss: 0.5847 Acc: 0.6301\n",
      "Epoch 5/25 | Train Loss: 0.4664 Acc: 0.7808 | Val Loss: 0.6255 Acc: 0.5890\n",
      "Epoch 6/25 | Train Loss: 0.3942 Acc: 0.8174 | Val Loss: 0.5404 Acc: 0.7215\n",
      "Epoch 7/25 | Train Loss: 0.3537 Acc: 0.8265 | Val Loss: 0.6454 Acc: 0.6347\n",
      "Epoch 8/25 | Train Loss: 0.3135 Acc: 0.8356 | Val Loss: 0.5049 Acc: 0.6849\n",
      "Epoch 9/25 | Train Loss: 0.3048 Acc: 0.8584 | Val Loss: 0.5567 Acc: 0.7306\n",
      "Epoch 10/25 | Train Loss: 0.3468 Acc: 0.8311 | Val Loss: 0.4570 Acc: 0.7489\n",
      "Epoch 11/25 | Train Loss: 0.3034 Acc: 0.8493 | Val Loss: 0.4533 Acc: 0.7534\n",
      "Epoch 12/25 | Train Loss: 0.2887 Acc: 0.8950 | Val Loss: 0.4952 Acc: 0.7260\n",
      "Epoch 13/25 | Train Loss: 0.2772 Acc: 0.8676 | Val Loss: 0.5176 Acc: 0.7306\n",
      "Epoch 14/25 | Train Loss: 0.3283 Acc: 0.8493 | Val Loss: 0.4072 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3062 Acc: 0.8402 | Val Loss: 0.3979 Acc: 0.7854\n",
      "Epoch 16/25 | Train Loss: 0.3232 Acc: 0.8676 | Val Loss: 0.4044 Acc: 0.7854\n",
      "Epoch 17/25 | Train Loss: 0.3117 Acc: 0.8721 | Val Loss: 0.4501 Acc: 0.7534\n",
      "Epoch 18/25 | Train Loss: 0.2664 Acc: 0.8858 | Val Loss: 0.4478 Acc: 0.7626\n",
      "Epoch 19/25 | Train Loss: 0.2549 Acc: 0.8904 | Val Loss: 0.4545 Acc: 0.7489\n",
      "Epoch 20/25 | Train Loss: 0.2978 Acc: 0.8858 | Val Loss: 0.4603 Acc: 0.7808\n",
      "Epoch 21/25 | Train Loss: 0.2997 Acc: 0.8813 | Val Loss: 0.4104 Acc: 0.7900\n",
      "Epoch 22/25 | Train Loss: 0.2856 Acc: 0.8584 | Val Loss: 0.4081 Acc: 0.7900\n",
      "Epoch 23/25 | Train Loss: 0.2973 Acc: 0.8447 | Val Loss: 0.4247 Acc: 0.7991\n",
      "Epoch 24/25 | Train Loss: 0.3354 Acc: 0.8402 | Val Loss: 0.4022 Acc: 0.7580\n",
      "Epoch 25/25 | Train Loss: 0.3189 Acc: 0.8721 | Val Loss: 0.4289 Acc: 0.7626\n",
      "Fold 1 Test Accuracy: 0.8128\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6463 Acc: 0.5982 | Val Loss: 0.5443 Acc: 0.7808\n",
      "Epoch 2/25 | Train Loss: 0.5723 Acc: 0.6895 | Val Loss: 0.5591 Acc: 0.6119\n",
      "Epoch 3/25 | Train Loss: 0.5640 Acc: 0.7260 | Val Loss: 0.5955 Acc: 0.6393\n",
      "Epoch 4/25 | Train Loss: 0.6146 Acc: 0.6986 | Val Loss: 0.5767 Acc: 0.5799\n",
      "Epoch 5/25 | Train Loss: 0.5458 Acc: 0.6895 | Val Loss: 0.3867 Acc: 0.8128\n",
      "Epoch 6/25 | Train Loss: 0.5381 Acc: 0.7306 | Val Loss: 0.7016 Acc: 0.5799\n",
      "Epoch 7/25 | Train Loss: 0.4852 Acc: 0.7671 | Val Loss: 0.5099 Acc: 0.7215\n",
      "Epoch 8/25 | Train Loss: 0.4583 Acc: 0.7945 | Val Loss: 0.2962 Acc: 0.8813\n",
      "Epoch 9/25 | Train Loss: 0.4108 Acc: 0.7945 | Val Loss: 0.2913 Acc: 0.8721\n",
      "Epoch 10/25 | Train Loss: 0.3663 Acc: 0.8265 | Val Loss: 0.3434 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.4072 Acc: 0.8265 | Val Loss: 0.3390 Acc: 0.8174\n",
      "Epoch 12/25 | Train Loss: 0.3496 Acc: 0.8447 | Val Loss: 0.2849 Acc: 0.8950\n",
      "Epoch 13/25 | Train Loss: 0.4199 Acc: 0.8037 | Val Loss: 0.2543 Acc: 0.8813\n",
      "Epoch 14/25 | Train Loss: 0.3831 Acc: 0.8265 | Val Loss: 0.3606 Acc: 0.8356\n",
      "Epoch 15/25 | Train Loss: 0.3656 Acc: 0.8219 | Val Loss: 0.3181 Acc: 0.8584\n",
      "Epoch 16/25 | Train Loss: 0.3160 Acc: 0.8676 | Val Loss: 0.2892 Acc: 0.8676\n",
      "Epoch 17/25 | Train Loss: 0.3446 Acc: 0.8493 | Val Loss: 0.2893 Acc: 0.8904\n",
      "Epoch 18/25 | Train Loss: 0.3484 Acc: 0.8584 | Val Loss: 0.2680 Acc: 0.8995\n",
      "Epoch 19/25 | Train Loss: 0.3306 Acc: 0.8539 | Val Loss: 0.2724 Acc: 0.8995\n",
      "Epoch 20/25 | Train Loss: 0.3199 Acc: 0.8676 | Val Loss: 0.2774 Acc: 0.8539\n",
      "Epoch 21/25 | Train Loss: 0.2979 Acc: 0.8767 | Val Loss: 0.2631 Acc: 0.9087\n",
      "Epoch 22/25 | Train Loss: 0.3390 Acc: 0.8630 | Val Loss: 0.2756 Acc: 0.8676\n",
      "Epoch 23/25 | Train Loss: 0.3008 Acc: 0.8630 | Val Loss: 0.2990 Acc: 0.8813\n",
      "Epoch 24/25 | Train Loss: 0.3429 Acc: 0.8539 | Val Loss: 0.2675 Acc: 0.8950\n",
      "Epoch 25/25 | Train Loss: 0.3567 Acc: 0.8082 | Val Loss: 0.2785 Acc: 0.8813\n",
      "Fold 2 Test Accuracy: 0.8950\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.6242 Acc: 0.6484 | Val Loss: 0.8196 Acc: 0.5342\n",
      "Epoch 2/25 | Train Loss: 0.5786 Acc: 0.6712 | Val Loss: 0.5036 Acc: 0.7306\n",
      "Epoch 3/25 | Train Loss: 0.5309 Acc: 0.7489 | Val Loss: 0.5612 Acc: 0.7489\n",
      "Epoch 4/25 | Train Loss: 0.5004 Acc: 0.7717 | Val Loss: 0.7451 Acc: 0.7580\n",
      "Epoch 5/25 | Train Loss: 0.5166 Acc: 0.7900 | Val Loss: 0.5313 Acc: 0.7626\n",
      "Epoch 6/25 | Train Loss: 0.4487 Acc: 0.7763 | Val Loss: 0.7877 Acc: 0.7671\n",
      "Epoch 7/25 | Train Loss: 0.3977 Acc: 0.7991 | Val Loss: 1.2537 Acc: 0.5068\n",
      "Epoch 8/25 | Train Loss: 0.3618 Acc: 0.8265 | Val Loss: 0.4071 Acc: 0.8082\n",
      "Epoch 9/25 | Train Loss: 0.3525 Acc: 0.8447 | Val Loss: 0.3956 Acc: 0.8219\n",
      "Epoch 10/25 | Train Loss: 0.2477 Acc: 0.8995 | Val Loss: 0.4586 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.3482 Acc: 0.8447 | Val Loss: 0.3782 Acc: 0.7991\n",
      "Epoch 12/25 | Train Loss: 0.2623 Acc: 0.8858 | Val Loss: 0.3781 Acc: 0.8174\n",
      "Epoch 13/25 | Train Loss: 0.2497 Acc: 0.9132 | Val Loss: 0.3402 Acc: 0.8402\n",
      "Epoch 14/25 | Train Loss: 0.3047 Acc: 0.8858 | Val Loss: 0.4605 Acc: 0.7854\n",
      "Epoch 15/25 | Train Loss: 0.3278 Acc: 0.8493 | Val Loss: 0.3700 Acc: 0.8402\n",
      "Epoch 16/25 | Train Loss: 0.2995 Acc: 0.8721 | Val Loss: 0.3662 Acc: 0.8265\n",
      "Epoch 17/25 | Train Loss: 0.2819 Acc: 0.8630 | Val Loss: 0.3545 Acc: 0.8311\n",
      "Epoch 18/25 | Train Loss: 0.3213 Acc: 0.8356 | Val Loss: 0.3863 Acc: 0.8311\n",
      "Epoch 19/25 | Train Loss: 0.2634 Acc: 0.8767 | Val Loss: 0.3363 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.3067 Acc: 0.8858 | Val Loss: 0.3778 Acc: 0.8493\n",
      "Epoch 21/25 | Train Loss: 0.2950 Acc: 0.8493 | Val Loss: 0.3834 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.2341 Acc: 0.8995 | Val Loss: 0.3397 Acc: 0.8356\n",
      "Epoch 23/25 | Train Loss: 0.2845 Acc: 0.8813 | Val Loss: 0.3698 Acc: 0.8356\n",
      "Epoch 24/25 | Train Loss: 0.2800 Acc: 0.8539 | Val Loss: 0.3571 Acc: 0.8265\n",
      "Epoch 25/25 | Train Loss: 0.3048 Acc: 0.8493 | Val Loss: 0.3122 Acc: 0.8402\n",
      "Fold 3 Test Accuracy: 0.8356\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5935 Acc: 0.6895 | Val Loss: 0.5782 Acc: 0.6256\n",
      "Epoch 2/25 | Train Loss: 0.5491 Acc: 0.7306 | Val Loss: 0.5758 Acc: 0.7078\n",
      "Epoch 3/25 | Train Loss: 0.6559 Acc: 0.6804 | Val Loss: 0.6591 Acc: 0.7489\n",
      "Epoch 4/25 | Train Loss: 0.5253 Acc: 0.7352 | Val Loss: 0.4722 Acc: 0.7397\n",
      "Epoch 5/25 | Train Loss: 0.5172 Acc: 0.7671 | Val Loss: 0.5695 Acc: 0.6758\n",
      "Epoch 6/25 | Train Loss: 0.4335 Acc: 0.7900 | Val Loss: 0.4436 Acc: 0.7945\n",
      "Epoch 7/25 | Train Loss: 0.5307 Acc: 0.7397 | Val Loss: 0.5459 Acc: 0.7032\n",
      "Epoch 8/25 | Train Loss: 0.4750 Acc: 0.7626 | Val Loss: 0.4999 Acc: 0.7260\n",
      "Epoch 9/25 | Train Loss: 0.3884 Acc: 0.7945 | Val Loss: 0.4328 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.4075 Acc: 0.7808 | Val Loss: 0.4304 Acc: 0.8082\n",
      "Epoch 11/25 | Train Loss: 0.3994 Acc: 0.8082 | Val Loss: 0.4549 Acc: 0.7945\n",
      "Epoch 12/25 | Train Loss: 0.3864 Acc: 0.8174 | Val Loss: 0.4417 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3986 Acc: 0.8356 | Val Loss: 0.4526 Acc: 0.7854\n",
      "Epoch 14/25 | Train Loss: 0.3645 Acc: 0.8219 | Val Loss: 0.3997 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.4080 Acc: 0.7945 | Val Loss: 0.4262 Acc: 0.7671\n",
      "Epoch 16/25 | Train Loss: 0.3973 Acc: 0.7991 | Val Loss: 0.4088 Acc: 0.7945\n",
      "Epoch 17/25 | Train Loss: 0.3663 Acc: 0.8311 | Val Loss: 0.4438 Acc: 0.7945\n",
      "Epoch 18/25 | Train Loss: 0.3754 Acc: 0.8311 | Val Loss: 0.4210 Acc: 0.7945\n",
      "Epoch 19/25 | Train Loss: 0.3660 Acc: 0.8265 | Val Loss: 0.3797 Acc: 0.8311\n",
      "Epoch 20/25 | Train Loss: 0.3464 Acc: 0.8402 | Val Loss: 0.3923 Acc: 0.7945\n",
      "Epoch 21/25 | Train Loss: 0.4079 Acc: 0.7763 | Val Loss: 0.3891 Acc: 0.7991\n",
      "Epoch 22/25 | Train Loss: 0.3704 Acc: 0.8082 | Val Loss: 0.4301 Acc: 0.7808\n",
      "Epoch 23/25 | Train Loss: 0.3698 Acc: 0.8447 | Val Loss: 0.4004 Acc: 0.7991\n",
      "Epoch 24/25 | Train Loss: 0.3505 Acc: 0.8402 | Val Loss: 0.4126 Acc: 0.7854\n",
      "Epoch 25/25 | Train Loss: 0.3494 Acc: 0.8493 | Val Loss: 0.3888 Acc: 0.8265\n",
      "Fold 4 Test Accuracy: 0.7945\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.7110 Acc: 0.5936 | Val Loss: 0.5292 Acc: 0.7123\n",
      "Epoch 2/25 | Train Loss: 0.5512 Acc: 0.7215 | Val Loss: 0.6018 Acc: 0.6393\n",
      "Epoch 3/25 | Train Loss: 0.5128 Acc: 0.7306 | Val Loss: 0.5324 Acc: 0.7169\n",
      "Epoch 4/25 | Train Loss: 0.5640 Acc: 0.7671 | Val Loss: 0.6799 Acc: 0.5799\n",
      "Epoch 5/25 | Train Loss: 0.4783 Acc: 0.7808 | Val Loss: 0.4982 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.4452 Acc: 0.7900 | Val Loss: 0.4607 Acc: 0.7397\n",
      "Epoch 7/25 | Train Loss: 0.4406 Acc: 0.7900 | Val Loss: 0.6323 Acc: 0.7306\n",
      "Epoch 8/25 | Train Loss: 0.4806 Acc: 0.7900 | Val Loss: 0.5131 Acc: 0.7260\n",
      "Epoch 9/25 | Train Loss: 0.3780 Acc: 0.8174 | Val Loss: 0.4341 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.4170 Acc: 0.7534 | Val Loss: 0.4543 Acc: 0.7626\n",
      "Epoch 11/25 | Train Loss: 0.4215 Acc: 0.7945 | Val Loss: 0.4611 Acc: 0.7534\n",
      "Epoch 12/25 | Train Loss: 0.3717 Acc: 0.8356 | Val Loss: 0.4368 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3782 Acc: 0.7991 | Val Loss: 0.4285 Acc: 0.7945\n",
      "Epoch 14/25 | Train Loss: 0.3907 Acc: 0.7945 | Val Loss: 0.4422 Acc: 0.7580\n",
      "Epoch 15/25 | Train Loss: 0.3960 Acc: 0.8082 | Val Loss: 0.4130 Acc: 0.7717\n",
      "Epoch 16/25 | Train Loss: 0.3785 Acc: 0.8174 | Val Loss: 0.4313 Acc: 0.7854\n",
      "Epoch 17/25 | Train Loss: 0.3866 Acc: 0.7763 | Val Loss: 0.4212 Acc: 0.7717\n",
      "Epoch 18/25 | Train Loss: 0.3660 Acc: 0.8174 | Val Loss: 0.4332 Acc: 0.7534\n",
      "Epoch 19/25 | Train Loss: 0.3912 Acc: 0.8082 | Val Loss: 0.4281 Acc: 0.7671\n",
      "Epoch 20/25 | Train Loss: 0.3767 Acc: 0.8082 | Val Loss: 0.4035 Acc: 0.7945\n",
      "Epoch 21/25 | Train Loss: 0.3943 Acc: 0.7945 | Val Loss: 0.4232 Acc: 0.7671\n",
      "Epoch 22/25 | Train Loss: 0.4114 Acc: 0.7991 | Val Loss: 0.4356 Acc: 0.7854\n",
      "Epoch 23/25 | Train Loss: 0.3724 Acc: 0.8402 | Val Loss: 0.3982 Acc: 0.7900\n",
      "Epoch 24/25 | Train Loss: 0.4313 Acc: 0.7900 | Val Loss: 0.4283 Acc: 0.7854\n",
      "Epoch 25/25 | Train Loss: 0.3960 Acc: 0.8037 | Val Loss: 0.4119 Acc: 0.7763\n",
      "Fold 5 Test Accuracy: 0.7717\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.6295 Acc: 0.6438 | Val Loss: 0.5869 Acc: 0.7717\n",
      "Epoch 2/25 | Train Loss: 0.5707 Acc: 0.6895 | Val Loss: 0.4373 Acc: 0.7945\n",
      "Epoch 3/25 | Train Loss: 0.5946 Acc: 0.7260 | Val Loss: 0.8192 Acc: 0.5708\n",
      "Epoch 4/25 | Train Loss: 0.5153 Acc: 0.7489 | Val Loss: 0.4525 Acc: 0.7763\n",
      "Epoch 5/25 | Train Loss: 0.4796 Acc: 0.7489 | Val Loss: 0.6136 Acc: 0.7808\n",
      "Epoch 6/25 | Train Loss: 0.4653 Acc: 0.7854 | Val Loss: 0.4260 Acc: 0.7808\n",
      "Epoch 7/25 | Train Loss: 0.4216 Acc: 0.8219 | Val Loss: 0.5213 Acc: 0.7991\n",
      "Epoch 8/25 | Train Loss: 0.4151 Acc: 0.8174 | Val Loss: 0.3903 Acc: 0.8174\n",
      "Epoch 9/25 | Train Loss: 0.4120 Acc: 0.7854 | Val Loss: 0.3593 Acc: 0.8311\n",
      "Epoch 10/25 | Train Loss: 0.3613 Acc: 0.8447 | Val Loss: 0.3773 Acc: 0.8174\n",
      "Epoch 11/25 | Train Loss: 0.3626 Acc: 0.8265 | Val Loss: 0.3633 Acc: 0.8174\n",
      "Epoch 12/25 | Train Loss: 0.3494 Acc: 0.8356 | Val Loss: 0.3655 Acc: 0.8265\n",
      "Epoch 13/25 | Train Loss: 0.3383 Acc: 0.8447 | Val Loss: 0.3366 Acc: 0.8584\n",
      "Epoch 14/25 | Train Loss: 0.2958 Acc: 0.8676 | Val Loss: 0.3437 Acc: 0.8311\n",
      "Epoch 15/25 | Train Loss: 0.2901 Acc: 0.8950 | Val Loss: 0.3544 Acc: 0.8493\n",
      "Epoch 16/25 | Train Loss: 0.2628 Acc: 0.9041 | Val Loss: 0.3561 Acc: 0.8539\n",
      "Epoch 17/25 | Train Loss: 0.3604 Acc: 0.8402 | Val Loss: 0.3078 Acc: 0.8767\n",
      "Epoch 18/25 | Train Loss: 0.3051 Acc: 0.8630 | Val Loss: 0.3319 Acc: 0.8539\n",
      "Epoch 19/25 | Train Loss: 0.3077 Acc: 0.8584 | Val Loss: 0.3426 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.2715 Acc: 0.8858 | Val Loss: 0.3389 Acc: 0.8219\n",
      "Epoch 21/25 | Train Loss: 0.2896 Acc: 0.8767 | Val Loss: 0.3547 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.2656 Acc: 0.8950 | Val Loss: 0.3497 Acc: 0.8676\n",
      "Epoch 23/25 | Train Loss: 0.3203 Acc: 0.8493 | Val Loss: 0.3172 Acc: 0.8630\n",
      "Epoch 24/25 | Train Loss: 0.3410 Acc: 0.8402 | Val Loss: 0.3340 Acc: 0.8356\n",
      "Epoch 25/25 | Train Loss: 0.3235 Acc: 0.8493 | Val Loss: 0.3851 Acc: 0.8219\n",
      "Fold 6 Test Accuracy: 0.8265\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.6419 Acc: 0.6393 | Val Loss: 0.4971 Acc: 0.7534\n",
      "Epoch 2/25 | Train Loss: 0.5408 Acc: 0.7215 | Val Loss: 0.4693 Acc: 0.7991\n",
      "Epoch 3/25 | Train Loss: 0.5476 Acc: 0.7078 | Val Loss: 0.5908 Acc: 0.6164\n",
      "Epoch 4/25 | Train Loss: 0.5801 Acc: 0.7123 | Val Loss: 0.6161 Acc: 0.7717\n",
      "Epoch 5/25 | Train Loss: 0.4395 Acc: 0.8128 | Val Loss: 0.4954 Acc: 0.7808\n",
      "Epoch 6/25 | Train Loss: 0.4696 Acc: 0.7443 | Val Loss: 0.4389 Acc: 0.7945\n",
      "Epoch 7/25 | Train Loss: 0.4821 Acc: 0.7671 | Val Loss: 0.4000 Acc: 0.7945\n",
      "Epoch 8/25 | Train Loss: 0.4241 Acc: 0.7991 | Val Loss: 0.4393 Acc: 0.7580\n",
      "Epoch 9/25 | Train Loss: 0.4298 Acc: 0.7763 | Val Loss: 0.4422 Acc: 0.7489\n",
      "Epoch 10/25 | Train Loss: 0.3933 Acc: 0.8128 | Val Loss: 0.4190 Acc: 0.7534\n",
      "Epoch 11/25 | Train Loss: 0.4043 Acc: 0.8037 | Val Loss: 0.4102 Acc: 0.8128\n",
      "Epoch 12/25 | Train Loss: 0.4226 Acc: 0.7854 | Val Loss: 0.4035 Acc: 0.8265\n",
      "Epoch 13/25 | Train Loss: 0.3448 Acc: 0.8402 | Val Loss: 0.3936 Acc: 0.7808\n",
      "Epoch 14/25 | Train Loss: 0.3648 Acc: 0.8584 | Val Loss: 0.3832 Acc: 0.7945\n",
      "Epoch 15/25 | Train Loss: 0.3657 Acc: 0.8356 | Val Loss: 0.3697 Acc: 0.7900\n",
      "Epoch 16/25 | Train Loss: 0.3517 Acc: 0.8630 | Val Loss: 0.3710 Acc: 0.8356\n",
      "Epoch 17/25 | Train Loss: 0.4368 Acc: 0.7763 | Val Loss: 0.3898 Acc: 0.8037\n",
      "Epoch 18/25 | Train Loss: 0.3454 Acc: 0.8493 | Val Loss: 0.3601 Acc: 0.8356\n",
      "Epoch 19/25 | Train Loss: 0.3740 Acc: 0.8356 | Val Loss: 0.3978 Acc: 0.7945\n",
      "Epoch 20/25 | Train Loss: 0.3623 Acc: 0.8402 | Val Loss: 0.3701 Acc: 0.8037\n",
      "Epoch 21/25 | Train Loss: 0.3505 Acc: 0.8356 | Val Loss: 0.3458 Acc: 0.8311\n",
      "Epoch 22/25 | Train Loss: 0.3715 Acc: 0.8447 | Val Loss: 0.3774 Acc: 0.7900\n",
      "Epoch 23/25 | Train Loss: 0.3529 Acc: 0.8402 | Val Loss: 0.3666 Acc: 0.8311\n",
      "Epoch 24/25 | Train Loss: 0.3532 Acc: 0.8493 | Val Loss: 0.3529 Acc: 0.8356\n",
      "Epoch 25/25 | Train Loss: 0.3289 Acc: 0.8813 | Val Loss: 0.3524 Acc: 0.8219\n",
      "Fold 7 Test Accuracy: 0.8447\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.6299 Acc: 0.6575 | Val Loss: 0.5992 Acc: 0.5890\n",
      "Epoch 2/25 | Train Loss: 0.6312 Acc: 0.7169 | Val Loss: 0.9002 Acc: 0.5160\n",
      "Epoch 3/25 | Train Loss: 0.5342 Acc: 0.7443 | Val Loss: 0.5249 Acc: 0.7352\n",
      "Epoch 4/25 | Train Loss: 0.4500 Acc: 0.7580 | Val Loss: 0.5540 Acc: 0.6804\n",
      "Epoch 5/25 | Train Loss: 0.4804 Acc: 0.7717 | Val Loss: 0.5478 Acc: 0.6895\n",
      "Epoch 6/25 | Train Loss: 0.5154 Acc: 0.7717 | Val Loss: 0.5307 Acc: 0.7352\n",
      "Epoch 7/25 | Train Loss: 0.4626 Acc: 0.7626 | Val Loss: 0.5344 Acc: 0.6758\n",
      "Epoch 8/25 | Train Loss: 0.4260 Acc: 0.7854 | Val Loss: 0.5015 Acc: 0.7489\n",
      "Epoch 9/25 | Train Loss: 0.3895 Acc: 0.8037 | Val Loss: 0.4851 Acc: 0.7352\n",
      "Epoch 10/25 | Train Loss: 0.3640 Acc: 0.8311 | Val Loss: 0.4912 Acc: 0.7352\n",
      "Epoch 11/25 | Train Loss: 0.3577 Acc: 0.8265 | Val Loss: 0.4767 Acc: 0.7260\n",
      "Epoch 12/25 | Train Loss: 0.3426 Acc: 0.8539 | Val Loss: 0.4538 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3568 Acc: 0.8356 | Val Loss: 0.4774 Acc: 0.7626\n",
      "Epoch 14/25 | Train Loss: 0.3649 Acc: 0.8174 | Val Loss: 0.5178 Acc: 0.7534\n",
      "Epoch 15/25 | Train Loss: 0.3296 Acc: 0.8356 | Val Loss: 0.4867 Acc: 0.7626\n",
      "Epoch 16/25 | Train Loss: 0.3322 Acc: 0.8584 | Val Loss: 0.4618 Acc: 0.7763\n",
      "Epoch 17/25 | Train Loss: 0.3770 Acc: 0.8219 | Val Loss: 0.4627 Acc: 0.7626\n",
      "Epoch 18/25 | Train Loss: 0.3529 Acc: 0.8265 | Val Loss: 0.4266 Acc: 0.7717\n",
      "Epoch 19/25 | Train Loss: 0.4076 Acc: 0.8128 | Val Loss: 0.5121 Acc: 0.7123\n",
      "Epoch 20/25 | Train Loss: 0.3427 Acc: 0.8311 | Val Loss: 0.4489 Acc: 0.7854\n",
      "Epoch 21/25 | Train Loss: 0.3587 Acc: 0.8174 | Val Loss: 0.4747 Acc: 0.7808\n",
      "Epoch 22/25 | Train Loss: 0.3807 Acc: 0.8402 | Val Loss: 0.4628 Acc: 0.7489\n",
      "Epoch 23/25 | Train Loss: 0.3357 Acc: 0.8311 | Val Loss: 0.4534 Acc: 0.7717\n",
      "Epoch 24/25 | Train Loss: 0.3440 Acc: 0.8356 | Val Loss: 0.4611 Acc: 0.7763\n",
      "Epoch 25/25 | Train Loss: 0.3458 Acc: 0.8356 | Val Loss: 0.5009 Acc: 0.7626\n",
      "Fold 8 Test Accuracy: 0.7123\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5849 Acc: 0.6667 | Val Loss: 0.5402 Acc: 0.6849\n",
      "Epoch 2/25 | Train Loss: 0.5510 Acc: 0.6758 | Val Loss: 0.4582 Acc: 0.7580\n",
      "Epoch 3/25 | Train Loss: 0.5637 Acc: 0.6804 | Val Loss: 0.5111 Acc: 0.7306\n",
      "Epoch 4/25 | Train Loss: 0.5622 Acc: 0.7078 | Val Loss: 0.4265 Acc: 0.7900\n",
      "Epoch 5/25 | Train Loss: 0.5264 Acc: 0.7032 | Val Loss: 0.5216 Acc: 0.6986\n",
      "Epoch 6/25 | Train Loss: 0.5126 Acc: 0.7397 | Val Loss: 0.5417 Acc: 0.6347\n",
      "Epoch 7/25 | Train Loss: 0.5252 Acc: 0.7260 | Val Loss: 0.9126 Acc: 0.5114\n",
      "Epoch 8/25 | Train Loss: 0.4754 Acc: 0.7352 | Val Loss: 0.4499 Acc: 0.7808\n",
      "Epoch 9/25 | Train Loss: 0.4267 Acc: 0.7945 | Val Loss: 0.4045 Acc: 0.7854\n",
      "Epoch 10/25 | Train Loss: 0.4137 Acc: 0.8082 | Val Loss: 0.3768 Acc: 0.8037\n",
      "Epoch 11/25 | Train Loss: 0.4097 Acc: 0.8219 | Val Loss: 0.4124 Acc: 0.8082\n",
      "Epoch 12/25 | Train Loss: 0.3915 Acc: 0.8356 | Val Loss: 0.4504 Acc: 0.7717\n",
      "Epoch 13/25 | Train Loss: 0.3962 Acc: 0.8037 | Val Loss: 0.3580 Acc: 0.8402\n",
      "Epoch 14/25 | Train Loss: 0.4045 Acc: 0.7900 | Val Loss: 0.3647 Acc: 0.8128\n",
      "Epoch 15/25 | Train Loss: 0.4083 Acc: 0.8174 | Val Loss: 0.3444 Acc: 0.8584\n",
      "Epoch 16/25 | Train Loss: 0.3877 Acc: 0.8128 | Val Loss: 0.3695 Acc: 0.8219\n",
      "Epoch 17/25 | Train Loss: 0.4038 Acc: 0.8219 | Val Loss: 0.3748 Acc: 0.8265\n",
      "Epoch 18/25 | Train Loss: 0.3915 Acc: 0.8311 | Val Loss: 0.3666 Acc: 0.7945\n",
      "Epoch 19/25 | Train Loss: 0.3617 Acc: 0.8447 | Val Loss: 0.3567 Acc: 0.8402\n",
      "Epoch 20/25 | Train Loss: 0.3717 Acc: 0.8082 | Val Loss: 0.3383 Acc: 0.8493\n",
      "Epoch 21/25 | Train Loss: 0.3994 Acc: 0.7808 | Val Loss: 0.3719 Acc: 0.8311\n",
      "Epoch 22/25 | Train Loss: 0.4104 Acc: 0.8174 | Val Loss: 0.3649 Acc: 0.8128\n",
      "Epoch 23/25 | Train Loss: 0.4003 Acc: 0.7991 | Val Loss: 0.4008 Acc: 0.8037\n",
      "Epoch 24/25 | Train Loss: 0.3622 Acc: 0.8174 | Val Loss: 0.3258 Acc: 0.8539\n",
      "Epoch 25/25 | Train Loss: 0.4176 Acc: 0.8128 | Val Loss: 0.3596 Acc: 0.8447\n",
      "Fold 9 Test Accuracy: 0.8219\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.6299 Acc: 0.6575 | Val Loss: 0.5763 Acc: 0.6941\n",
      "Epoch 2/25 | Train Loss: 0.5459 Acc: 0.7260 | Val Loss: 0.5108 Acc: 0.7078\n",
      "Epoch 3/25 | Train Loss: 0.5298 Acc: 0.7215 | Val Loss: 0.5680 Acc: 0.6712\n",
      "Epoch 4/25 | Train Loss: 0.4612 Acc: 0.7991 | Val Loss: 0.4860 Acc: 0.7489\n",
      "Epoch 5/25 | Train Loss: 0.4572 Acc: 0.7945 | Val Loss: 0.5424 Acc: 0.7306\n",
      "Epoch 6/25 | Train Loss: 0.3962 Acc: 0.8128 | Val Loss: 0.4690 Acc: 0.7489\n",
      "Epoch 7/25 | Train Loss: 0.3770 Acc: 0.8219 | Val Loss: 0.5861 Acc: 0.7626\n",
      "Epoch 8/25 | Train Loss: 0.3529 Acc: 0.8447 | Val Loss: 0.4440 Acc: 0.7945\n",
      "Epoch 9/25 | Train Loss: 0.3483 Acc: 0.8128 | Val Loss: 0.4977 Acc: 0.7626\n",
      "Epoch 10/25 | Train Loss: 0.2905 Acc: 0.8950 | Val Loss: 0.4349 Acc: 0.8037\n",
      "Epoch 11/25 | Train Loss: 0.3546 Acc: 0.8584 | Val Loss: 0.4582 Acc: 0.7900\n",
      "Epoch 12/25 | Train Loss: 0.3132 Acc: 0.8493 | Val Loss: 0.4388 Acc: 0.7900\n",
      "Epoch 13/25 | Train Loss: 0.2960 Acc: 0.8767 | Val Loss: 0.3873 Acc: 0.7945\n",
      "Epoch 14/25 | Train Loss: 0.3080 Acc: 0.8676 | Val Loss: 0.3959 Acc: 0.8174\n",
      "Epoch 15/25 | Train Loss: 0.2682 Acc: 0.8813 | Val Loss: 0.3733 Acc: 0.8174\n",
      "Epoch 16/25 | Train Loss: 0.3105 Acc: 0.8493 | Val Loss: 0.4022 Acc: 0.7991\n",
      "Epoch 17/25 | Train Loss: 0.2972 Acc: 0.8858 | Val Loss: 0.3904 Acc: 0.8174\n",
      "Epoch 18/25 | Train Loss: 0.2449 Acc: 0.9087 | Val Loss: 0.4194 Acc: 0.8037\n",
      "Epoch 19/25 | Train Loss: 0.2698 Acc: 0.8950 | Val Loss: 0.3960 Acc: 0.8174\n",
      "Epoch 20/25 | Train Loss: 0.2627 Acc: 0.8995 | Val Loss: 0.3871 Acc: 0.8082\n",
      "Epoch 21/25 | Train Loss: 0.2930 Acc: 0.8858 | Val Loss: 0.4255 Acc: 0.7991\n",
      "Epoch 22/25 | Train Loss: 0.2808 Acc: 0.8904 | Val Loss: 0.4067 Acc: 0.8174\n",
      "Epoch 23/25 | Train Loss: 0.2691 Acc: 0.8995 | Val Loss: 0.4094 Acc: 0.8037\n",
      "Epoch 24/25 | Train Loss: 0.2513 Acc: 0.8995 | Val Loss: 0.4379 Acc: 0.8128\n",
      "Epoch 25/25 | Train Loss: 0.2934 Acc: 0.8813 | Val Loss: 0.3856 Acc: 0.8265\n",
      "Fold 10 Test Accuracy: 0.7991\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8114, Std Dev = 0.0481\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T13:18:38.335751Z",
     "start_time": "2025-05-18T11:54:41.552675Z"
    }
   },
   "id": "af21f1d23069c568",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96359fbd4486ad57"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/VAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=110, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T13:18:38.356759Z",
     "start_time": "2025-05-18T13:18:38.340762Z"
    }
   },
   "id": "c18bc0ae366834f9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.6117 Acc: 0.6530 | Val Loss: 0.5974 Acc: 0.6164\n",
      "Epoch 2/25 | Train Loss: 0.5454 Acc: 0.7489 | Val Loss: 0.6997 Acc: 0.5342\n",
      "Epoch 3/25 | Train Loss: 0.4794 Acc: 0.7215 | Val Loss: 0.6078 Acc: 0.6347\n",
      "Epoch 4/25 | Train Loss: 0.4333 Acc: 0.7671 | Val Loss: 0.5612 Acc: 0.6575\n",
      "Epoch 5/25 | Train Loss: 0.4406 Acc: 0.8174 | Val Loss: 0.6017 Acc: 0.6027\n",
      "Epoch 6/25 | Train Loss: 0.4265 Acc: 0.7717 | Val Loss: 0.6204 Acc: 0.6164\n",
      "Epoch 7/25 | Train Loss: 0.4423 Acc: 0.7717 | Val Loss: 0.5847 Acc: 0.7260\n",
      "Epoch 8/25 | Train Loss: 0.3820 Acc: 0.8037 | Val Loss: 0.5428 Acc: 0.7671\n",
      "Epoch 9/25 | Train Loss: 0.3939 Acc: 0.7991 | Val Loss: 0.5040 Acc: 0.7215\n",
      "Epoch 10/25 | Train Loss: 0.3416 Acc: 0.8311 | Val Loss: 0.4876 Acc: 0.7352\n",
      "Epoch 11/25 | Train Loss: 0.3412 Acc: 0.8356 | Val Loss: 0.4636 Acc: 0.7534\n",
      "Epoch 12/25 | Train Loss: 0.3279 Acc: 0.8630 | Val Loss: 0.4640 Acc: 0.7443\n",
      "Epoch 13/25 | Train Loss: 0.3372 Acc: 0.8447 | Val Loss: 0.5210 Acc: 0.7443\n",
      "Epoch 14/25 | Train Loss: 0.3100 Acc: 0.8493 | Val Loss: 0.4934 Acc: 0.7489\n",
      "Epoch 15/25 | Train Loss: 0.2986 Acc: 0.8858 | Val Loss: 0.4584 Acc: 0.7854\n",
      "Epoch 16/25 | Train Loss: 0.4009 Acc: 0.7991 | Val Loss: 0.4700 Acc: 0.7489\n",
      "Epoch 17/25 | Train Loss: 0.3397 Acc: 0.8447 | Val Loss: 0.4445 Acc: 0.7763\n",
      "Epoch 18/25 | Train Loss: 0.3931 Acc: 0.8311 | Val Loss: 0.4496 Acc: 0.7763\n",
      "Epoch 19/25 | Train Loss: 0.3218 Acc: 0.8402 | Val Loss: 0.4843 Acc: 0.7580\n",
      "Epoch 20/25 | Train Loss: 0.2898 Acc: 0.8904 | Val Loss: 0.4445 Acc: 0.7671\n",
      "Epoch 21/25 | Train Loss: 0.3135 Acc: 0.8721 | Val Loss: 0.4615 Acc: 0.7489\n",
      "Epoch 22/25 | Train Loss: 0.3188 Acc: 0.8447 | Val Loss: 0.4698 Acc: 0.7626\n",
      "Epoch 23/25 | Train Loss: 0.3288 Acc: 0.8584 | Val Loss: 0.4488 Acc: 0.7808\n",
      "Epoch 24/25 | Train Loss: 0.3068 Acc: 0.8584 | Val Loss: 0.4904 Acc: 0.7306\n",
      "Epoch 25/25 | Train Loss: 0.3400 Acc: 0.8402 | Val Loss: 0.4656 Acc: 0.7534\n",
      "Fold 1 Test Accuracy: 0.7397\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6578 Acc: 0.6073 | Val Loss: 0.4952 Acc: 0.8037\n",
      "Epoch 2/25 | Train Loss: 0.5931 Acc: 0.6621 | Val Loss: 0.5299 Acc: 0.6484\n",
      "Epoch 3/25 | Train Loss: 0.5788 Acc: 0.6986 | Val Loss: 0.6741 Acc: 0.5205\n",
      "Epoch 4/25 | Train Loss: 0.5697 Acc: 0.7489 | Val Loss: 0.4657 Acc: 0.7352\n",
      "Epoch 5/25 | Train Loss: 0.5113 Acc: 0.7352 | Val Loss: 0.3745 Acc: 0.8584\n",
      "Epoch 6/25 | Train Loss: 0.5879 Acc: 0.6895 | Val Loss: 0.3907 Acc: 0.7900\n",
      "Epoch 7/25 | Train Loss: 0.4785 Acc: 0.7854 | Val Loss: 0.4265 Acc: 0.7671\n",
      "Epoch 8/25 | Train Loss: 0.4243 Acc: 0.7763 | Val Loss: 0.3155 Acc: 0.8493\n",
      "Epoch 9/25 | Train Loss: 0.3985 Acc: 0.8219 | Val Loss: 0.3095 Acc: 0.8037\n",
      "Epoch 10/25 | Train Loss: 0.3611 Acc: 0.8539 | Val Loss: 0.3224 Acc: 0.8447\n",
      "Epoch 11/25 | Train Loss: 0.3919 Acc: 0.8219 | Val Loss: 0.3324 Acc: 0.8447\n",
      "Epoch 12/25 | Train Loss: 0.3439 Acc: 0.8447 | Val Loss: 0.2953 Acc: 0.8539\n",
      "Epoch 13/25 | Train Loss: 0.3719 Acc: 0.7991 | Val Loss: 0.3067 Acc: 0.8539\n",
      "Epoch 14/25 | Train Loss: 0.3700 Acc: 0.8402 | Val Loss: 0.3057 Acc: 0.8858\n",
      "Epoch 15/25 | Train Loss: 0.3331 Acc: 0.8539 | Val Loss: 0.3061 Acc: 0.8493\n",
      "Epoch 16/25 | Train Loss: 0.3469 Acc: 0.8447 | Val Loss: 0.3072 Acc: 0.8584\n",
      "Epoch 17/25 | Train Loss: 0.3854 Acc: 0.8311 | Val Loss: 0.3157 Acc: 0.8447\n",
      "Epoch 18/25 | Train Loss: 0.3397 Acc: 0.8447 | Val Loss: 0.2704 Acc: 0.8950\n",
      "Epoch 19/25 | Train Loss: 0.3410 Acc: 0.8447 | Val Loss: 0.2815 Acc: 0.8813\n",
      "Epoch 20/25 | Train Loss: 0.3600 Acc: 0.8356 | Val Loss: 0.2748 Acc: 0.8767\n",
      "Epoch 21/25 | Train Loss: 0.2973 Acc: 0.8858 | Val Loss: 0.2811 Acc: 0.8630\n",
      "Epoch 22/25 | Train Loss: 0.3136 Acc: 0.8539 | Val Loss: 0.3012 Acc: 0.8767\n",
      "Epoch 23/25 | Train Loss: 0.3108 Acc: 0.8402 | Val Loss: 0.2683 Acc: 0.8767\n",
      "Epoch 24/25 | Train Loss: 0.3359 Acc: 0.8402 | Val Loss: 0.2692 Acc: 0.8539\n",
      "Epoch 25/25 | Train Loss: 0.3107 Acc: 0.8676 | Val Loss: 0.2686 Acc: 0.8858\n",
      "Fold 2 Test Accuracy: 0.8813\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.6356 Acc: 0.6119 | Val Loss: 0.7110 Acc: 0.7580\n",
      "Epoch 2/25 | Train Loss: 0.6108 Acc: 0.7306 | Val Loss: 0.5887 Acc: 0.6804\n",
      "Epoch 3/25 | Train Loss: 0.4710 Acc: 0.7763 | Val Loss: 0.8798 Acc: 0.5251\n",
      "Epoch 4/25 | Train Loss: 0.4573 Acc: 0.7900 | Val Loss: 0.5869 Acc: 0.6667\n",
      "Epoch 5/25 | Train Loss: 0.4970 Acc: 0.7717 | Val Loss: 0.8214 Acc: 0.5936\n",
      "Epoch 6/25 | Train Loss: 0.5331 Acc: 0.7443 | Val Loss: 0.6783 Acc: 0.6393\n",
      "Epoch 7/25 | Train Loss: 0.4728 Acc: 0.7489 | Val Loss: 0.4643 Acc: 0.7534\n",
      "Epoch 8/25 | Train Loss: 0.4279 Acc: 0.7854 | Val Loss: 0.4491 Acc: 0.7854\n",
      "Epoch 9/25 | Train Loss: 0.4011 Acc: 0.8037 | Val Loss: 0.3726 Acc: 0.8174\n",
      "Epoch 10/25 | Train Loss: 0.3451 Acc: 0.8311 | Val Loss: 0.3438 Acc: 0.8539\n",
      "Epoch 11/25 | Train Loss: 0.3763 Acc: 0.7900 | Val Loss: 0.3820 Acc: 0.8082\n",
      "Epoch 12/25 | Train Loss: 0.3196 Acc: 0.8904 | Val Loss: 0.4034 Acc: 0.7945\n",
      "Epoch 13/25 | Train Loss: 0.3515 Acc: 0.8356 | Val Loss: 0.3902 Acc: 0.8174\n",
      "Epoch 14/25 | Train Loss: 0.3221 Acc: 0.8584 | Val Loss: 0.3610 Acc: 0.8493\n",
      "Epoch 15/25 | Train Loss: 0.3227 Acc: 0.8676 | Val Loss: 0.3451 Acc: 0.8356\n",
      "Epoch 16/25 | Train Loss: 0.2823 Acc: 0.8904 | Val Loss: 0.3648 Acc: 0.8265\n",
      "Epoch 17/25 | Train Loss: 0.3101 Acc: 0.8539 | Val Loss: 0.3553 Acc: 0.8584\n",
      "Epoch 18/25 | Train Loss: 0.3386 Acc: 0.8447 | Val Loss: 0.3427 Acc: 0.8311\n",
      "Epoch 19/25 | Train Loss: 0.3209 Acc: 0.8356 | Val Loss: 0.3588 Acc: 0.8447\n",
      "Epoch 20/25 | Train Loss: 0.3062 Acc: 0.8767 | Val Loss: 0.3362 Acc: 0.8539\n",
      "Epoch 21/25 | Train Loss: 0.2987 Acc: 0.8721 | Val Loss: 0.4083 Acc: 0.8082\n",
      "Epoch 22/25 | Train Loss: 0.3152 Acc: 0.8311 | Val Loss: 0.3337 Acc: 0.8402\n",
      "Epoch 23/25 | Train Loss: 0.3436 Acc: 0.8493 | Val Loss: 0.3461 Acc: 0.8584\n",
      "Epoch 24/25 | Train Loss: 0.3419 Acc: 0.8584 | Val Loss: 0.3550 Acc: 0.8447\n",
      "Epoch 25/25 | Train Loss: 0.2774 Acc: 0.8767 | Val Loss: 0.3310 Acc: 0.8493\n",
      "Fold 3 Test Accuracy: 0.8265\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.6061 Acc: 0.6575 | Val Loss: 0.5785 Acc: 0.6484\n",
      "Epoch 2/25 | Train Loss: 0.6269 Acc: 0.7078 | Val Loss: 0.6164 Acc: 0.6393\n",
      "Epoch 3/25 | Train Loss: 0.5431 Acc: 0.7352 | Val Loss: 0.5312 Acc: 0.6667\n",
      "Epoch 4/25 | Train Loss: 0.4986 Acc: 0.7260 | Val Loss: 0.4665 Acc: 0.7352\n",
      "Epoch 5/25 | Train Loss: 0.4849 Acc: 0.7580 | Val Loss: 0.4557 Acc: 0.7443\n",
      "Epoch 6/25 | Train Loss: 0.4349 Acc: 0.7763 | Val Loss: 0.4774 Acc: 0.7626\n",
      "Epoch 7/25 | Train Loss: 0.4910 Acc: 0.7534 | Val Loss: 0.5384 Acc: 0.7260\n",
      "Epoch 8/25 | Train Loss: 0.4600 Acc: 0.7808 | Val Loss: 0.4769 Acc: 0.7489\n",
      "Epoch 9/25 | Train Loss: 0.4335 Acc: 0.7626 | Val Loss: 0.4336 Acc: 0.7580\n",
      "Epoch 10/25 | Train Loss: 0.4381 Acc: 0.7900 | Val Loss: 0.4660 Acc: 0.7443\n",
      "Epoch 11/25 | Train Loss: 0.4378 Acc: 0.7854 | Val Loss: 0.4479 Acc: 0.7306\n",
      "Epoch 12/25 | Train Loss: 0.4232 Acc: 0.7900 | Val Loss: 0.4366 Acc: 0.7717\n",
      "Epoch 13/25 | Train Loss: 0.3912 Acc: 0.7900 | Val Loss: 0.4390 Acc: 0.7808\n",
      "Epoch 14/25 | Train Loss: 0.4309 Acc: 0.7763 | Val Loss: 0.4499 Acc: 0.7489\n",
      "Epoch 15/25 | Train Loss: 0.4075 Acc: 0.7945 | Val Loss: 0.4378 Acc: 0.7443\n",
      "Epoch 16/25 | Train Loss: 0.4066 Acc: 0.8174 | Val Loss: 0.4270 Acc: 0.7717\n",
      "Epoch 17/25 | Train Loss: 0.3949 Acc: 0.7900 | Val Loss: 0.4214 Acc: 0.7671\n",
      "Epoch 18/25 | Train Loss: 0.3943 Acc: 0.8174 | Val Loss: 0.4043 Acc: 0.7900\n",
      "Epoch 19/25 | Train Loss: 0.4194 Acc: 0.7991 | Val Loss: 0.4552 Acc: 0.7443\n",
      "Epoch 20/25 | Train Loss: 0.4210 Acc: 0.7900 | Val Loss: 0.4296 Acc: 0.7763\n",
      "Epoch 21/25 | Train Loss: 0.3895 Acc: 0.7945 | Val Loss: 0.4181 Acc: 0.7489\n",
      "Epoch 22/25 | Train Loss: 0.4127 Acc: 0.7900 | Val Loss: 0.4178 Acc: 0.8128\n",
      "Epoch 23/25 | Train Loss: 0.4433 Acc: 0.7626 | Val Loss: 0.4003 Acc: 0.8082\n",
      "Epoch 24/25 | Train Loss: 0.4110 Acc: 0.8037 | Val Loss: 0.4254 Acc: 0.7717\n",
      "Epoch 25/25 | Train Loss: 0.3830 Acc: 0.8037 | Val Loss: 0.4232 Acc: 0.7808\n",
      "Fold 4 Test Accuracy: 0.7580\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.5927 Acc: 0.6758 | Val Loss: 0.6996 Acc: 0.7215\n",
      "Epoch 2/25 | Train Loss: 0.4801 Acc: 0.7900 | Val Loss: 0.5733 Acc: 0.6986\n",
      "Epoch 3/25 | Train Loss: 0.4985 Acc: 0.7078 | Val Loss: 0.7249 Acc: 0.7123\n",
      "Epoch 4/25 | Train Loss: 0.5646 Acc: 0.7260 | Val Loss: 0.5621 Acc: 0.7397\n",
      "Epoch 5/25 | Train Loss: 0.5159 Acc: 0.7260 | Val Loss: 0.5411 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.4982 Acc: 0.7215 | Val Loss: 0.5211 Acc: 0.7078\n",
      "Epoch 7/25 | Train Loss: 0.4463 Acc: 0.7489 | Val Loss: 0.4908 Acc: 0.7397\n",
      "Epoch 8/25 | Train Loss: 0.4800 Acc: 0.7763 | Val Loss: 0.4995 Acc: 0.7260\n",
      "Epoch 9/25 | Train Loss: 0.4507 Acc: 0.7945 | Val Loss: 0.4659 Acc: 0.7626\n",
      "Epoch 10/25 | Train Loss: 0.3995 Acc: 0.8082 | Val Loss: 0.4503 Acc: 0.7534\n",
      "Epoch 11/25 | Train Loss: 0.4131 Acc: 0.7991 | Val Loss: 0.4455 Acc: 0.7808\n",
      "Epoch 12/25 | Train Loss: 0.3643 Acc: 0.8265 | Val Loss: 0.4428 Acc: 0.7580\n",
      "Epoch 13/25 | Train Loss: 0.3705 Acc: 0.8174 | Val Loss: 0.4594 Acc: 0.7215\n",
      "Epoch 14/25 | Train Loss: 0.3963 Acc: 0.8037 | Val Loss: 0.4534 Acc: 0.7534\n",
      "Epoch 15/25 | Train Loss: 0.3497 Acc: 0.8356 | Val Loss: 0.4385 Acc: 0.7671\n",
      "Epoch 16/25 | Train Loss: 0.3812 Acc: 0.8037 | Val Loss: 0.4317 Acc: 0.7900\n",
      "Epoch 17/25 | Train Loss: 0.3931 Acc: 0.7854 | Val Loss: 0.4391 Acc: 0.7854\n",
      "Epoch 18/25 | Train Loss: 0.3655 Acc: 0.8174 | Val Loss: 0.4308 Acc: 0.7489\n",
      "Epoch 19/25 | Train Loss: 0.3729 Acc: 0.8082 | Val Loss: 0.4149 Acc: 0.7717\n",
      "Epoch 20/25 | Train Loss: 0.3885 Acc: 0.8037 | Val Loss: 0.4220 Acc: 0.7443\n",
      "Epoch 21/25 | Train Loss: 0.3906 Acc: 0.8037 | Val Loss: 0.4135 Acc: 0.7717\n",
      "Epoch 22/25 | Train Loss: 0.3598 Acc: 0.8311 | Val Loss: 0.4534 Acc: 0.7808\n",
      "Epoch 23/25 | Train Loss: 0.3579 Acc: 0.8493 | Val Loss: 0.4282 Acc: 0.7900\n",
      "Epoch 24/25 | Train Loss: 0.3759 Acc: 0.8128 | Val Loss: 0.4082 Acc: 0.7991\n",
      "Epoch 25/25 | Train Loss: 0.3548 Acc: 0.8265 | Val Loss: 0.4254 Acc: 0.7352\n",
      "Fold 5 Test Accuracy: 0.7991\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.6648 Acc: 0.5845 | Val Loss: 0.4426 Acc: 0.7854\n",
      "Epoch 2/25 | Train Loss: 0.5256 Acc: 0.7169 | Val Loss: 0.4987 Acc: 0.7717\n",
      "Epoch 3/25 | Train Loss: 0.6008 Acc: 0.6849 | Val Loss: 0.4616 Acc: 0.7580\n",
      "Epoch 4/25 | Train Loss: 0.6087 Acc: 0.7489 | Val Loss: 0.6326 Acc: 0.5708\n",
      "Epoch 5/25 | Train Loss: 0.5405 Acc: 0.7260 | Val Loss: 0.4670 Acc: 0.7215\n",
      "Epoch 6/25 | Train Loss: 0.5661 Acc: 0.6758 | Val Loss: 0.4562 Acc: 0.7763\n",
      "Epoch 7/25 | Train Loss: 0.4049 Acc: 0.7900 | Val Loss: 0.5207 Acc: 0.7854\n",
      "Epoch 8/25 | Train Loss: 0.4527 Acc: 0.7260 | Val Loss: 0.4663 Acc: 0.7900\n",
      "Epoch 9/25 | Train Loss: 0.4395 Acc: 0.7945 | Val Loss: 0.3784 Acc: 0.8174\n",
      "Epoch 10/25 | Train Loss: 0.4138 Acc: 0.8082 | Val Loss: 0.3681 Acc: 0.8082\n",
      "Epoch 11/25 | Train Loss: 0.3589 Acc: 0.8539 | Val Loss: 0.4187 Acc: 0.7808\n",
      "Epoch 12/25 | Train Loss: 0.3398 Acc: 0.8493 | Val Loss: 0.4021 Acc: 0.7991\n",
      "Epoch 13/25 | Train Loss: 0.3596 Acc: 0.8402 | Val Loss: 0.3599 Acc: 0.8356\n",
      "Epoch 14/25 | Train Loss: 0.3510 Acc: 0.8311 | Val Loss: 0.4139 Acc: 0.7900\n",
      "Epoch 15/25 | Train Loss: 0.3723 Acc: 0.8402 | Val Loss: 0.4031 Acc: 0.8128\n",
      "Epoch 16/25 | Train Loss: 0.3410 Acc: 0.8265 | Val Loss: 0.3599 Acc: 0.8402\n",
      "Epoch 17/25 | Train Loss: 0.3902 Acc: 0.8082 | Val Loss: 0.3467 Acc: 0.8311\n",
      "Epoch 18/25 | Train Loss: 0.3645 Acc: 0.8402 | Val Loss: 0.3907 Acc: 0.8493\n",
      "Epoch 19/25 | Train Loss: 0.3717 Acc: 0.8356 | Val Loss: 0.4180 Acc: 0.8128\n",
      "Epoch 20/25 | Train Loss: 0.3332 Acc: 0.8356 | Val Loss: 0.3454 Acc: 0.8447\n",
      "Epoch 21/25 | Train Loss: 0.3867 Acc: 0.8219 | Val Loss: 0.3553 Acc: 0.8037\n",
      "Epoch 22/25 | Train Loss: 0.3214 Acc: 0.8676 | Val Loss: 0.3415 Acc: 0.8219\n",
      "Epoch 23/25 | Train Loss: 0.3726 Acc: 0.8128 | Val Loss: 0.3805 Acc: 0.8265\n",
      "Epoch 24/25 | Train Loss: 0.3314 Acc: 0.8630 | Val Loss: 0.3498 Acc: 0.8402\n",
      "Epoch 25/25 | Train Loss: 0.3168 Acc: 0.8721 | Val Loss: 0.3827 Acc: 0.8174\n",
      "Fold 6 Test Accuracy: 0.8219\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.6290 Acc: 0.6256 | Val Loss: 0.4692 Acc: 0.7489\n",
      "Epoch 2/25 | Train Loss: 0.5759 Acc: 0.7078 | Val Loss: 0.9513 Acc: 0.4977\n",
      "Epoch 3/25 | Train Loss: 0.5399 Acc: 0.7123 | Val Loss: 0.4886 Acc: 0.7443\n",
      "Epoch 4/25 | Train Loss: 0.6117 Acc: 0.7123 | Val Loss: 0.5372 Acc: 0.7808\n",
      "Epoch 5/25 | Train Loss: 0.5233 Acc: 0.7078 | Val Loss: 0.4866 Acc: 0.6986\n",
      "Epoch 6/25 | Train Loss: 0.4752 Acc: 0.7671 | Val Loss: 1.1864 Acc: 0.5068\n",
      "Epoch 7/25 | Train Loss: 0.4942 Acc: 0.7260 | Val Loss: 0.5138 Acc: 0.7854\n",
      "Epoch 8/25 | Train Loss: 0.4684 Acc: 0.7671 | Val Loss: 0.4140 Acc: 0.7854\n",
      "Epoch 9/25 | Train Loss: 0.4181 Acc: 0.7717 | Val Loss: 0.4125 Acc: 0.7808\n",
      "Epoch 10/25 | Train Loss: 0.3819 Acc: 0.8128 | Val Loss: 0.3598 Acc: 0.8265\n",
      "Epoch 11/25 | Train Loss: 0.3932 Acc: 0.8174 | Val Loss: 0.3680 Acc: 0.8174\n",
      "Epoch 12/25 | Train Loss: 0.3551 Acc: 0.8447 | Val Loss: 0.3814 Acc: 0.7991\n",
      "Epoch 13/25 | Train Loss: 0.3540 Acc: 0.8539 | Val Loss: 0.3220 Acc: 0.8174\n",
      "Epoch 14/25 | Train Loss: 0.3942 Acc: 0.8037 | Val Loss: 0.3411 Acc: 0.8219\n",
      "Epoch 15/25 | Train Loss: 0.3458 Acc: 0.8767 | Val Loss: 0.3557 Acc: 0.8311\n",
      "Epoch 16/25 | Train Loss: 0.3600 Acc: 0.8311 | Val Loss: 0.3314 Acc: 0.8265\n",
      "Epoch 17/25 | Train Loss: 0.3351 Acc: 0.8721 | Val Loss: 0.3204 Acc: 0.8265\n",
      "Epoch 18/25 | Train Loss: 0.3770 Acc: 0.8402 | Val Loss: 0.3118 Acc: 0.8630\n",
      "Epoch 19/25 | Train Loss: 0.3449 Acc: 0.8402 | Val Loss: 0.3339 Acc: 0.8584\n",
      "Epoch 20/25 | Train Loss: 0.3233 Acc: 0.8539 | Val Loss: 0.3045 Acc: 0.8676\n",
      "Epoch 21/25 | Train Loss: 0.3508 Acc: 0.8356 | Val Loss: 0.3287 Acc: 0.8356\n",
      "Epoch 22/25 | Train Loss: 0.3390 Acc: 0.8539 | Val Loss: 0.3436 Acc: 0.8128\n",
      "Epoch 23/25 | Train Loss: 0.3173 Acc: 0.8539 | Val Loss: 0.3641 Acc: 0.8356\n",
      "Epoch 24/25 | Train Loss: 0.3048 Acc: 0.8904 | Val Loss: 0.3436 Acc: 0.8493\n",
      "Epoch 25/25 | Train Loss: 0.3157 Acc: 0.8721 | Val Loss: 0.3209 Acc: 0.8447\n",
      "Fold 7 Test Accuracy: 0.8265\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.6602 Acc: 0.6804 | Val Loss: 0.6138 Acc: 0.5936\n",
      "Epoch 2/25 | Train Loss: 0.5126 Acc: 0.7306 | Val Loss: 0.7519 Acc: 0.5616\n",
      "Epoch 3/25 | Train Loss: 0.4903 Acc: 0.7626 | Val Loss: 0.5241 Acc: 0.7169\n",
      "Epoch 4/25 | Train Loss: 0.5132 Acc: 0.7534 | Val Loss: 0.5448 Acc: 0.7032\n",
      "Epoch 5/25 | Train Loss: 0.5171 Acc: 0.7306 | Val Loss: 0.5481 Acc: 0.7306\n",
      "Epoch 6/25 | Train Loss: 0.4165 Acc: 0.8037 | Val Loss: 0.5496 Acc: 0.7260\n",
      "Epoch 7/25 | Train Loss: 0.4370 Acc: 0.7580 | Val Loss: 0.6614 Acc: 0.7123\n",
      "Epoch 8/25 | Train Loss: 0.4106 Acc: 0.7945 | Val Loss: 0.5398 Acc: 0.7078\n",
      "Epoch 9/25 | Train Loss: 0.4159 Acc: 0.7717 | Val Loss: 0.4890 Acc: 0.7534\n",
      "Epoch 10/25 | Train Loss: 0.3793 Acc: 0.8174 | Val Loss: 0.4561 Acc: 0.7671\n",
      "Epoch 11/25 | Train Loss: 0.3871 Acc: 0.7854 | Val Loss: 0.4821 Acc: 0.7671\n",
      "Epoch 12/25 | Train Loss: 0.4543 Acc: 0.7489 | Val Loss: 0.4717 Acc: 0.7443\n",
      "Epoch 13/25 | Train Loss: 0.3624 Acc: 0.8356 | Val Loss: 0.4726 Acc: 0.7397\n",
      "Epoch 14/25 | Train Loss: 0.3578 Acc: 0.8539 | Val Loss: 0.4924 Acc: 0.7443\n",
      "Epoch 15/25 | Train Loss: 0.3503 Acc: 0.8128 | Val Loss: 0.4314 Acc: 0.7489\n",
      "Epoch 16/25 | Train Loss: 0.3263 Acc: 0.8356 | Val Loss: 0.4652 Acc: 0.7763\n",
      "Epoch 17/25 | Train Loss: 0.3387 Acc: 0.8584 | Val Loss: 0.4760 Acc: 0.7626\n",
      "Epoch 18/25 | Train Loss: 0.3384 Acc: 0.8402 | Val Loss: 0.4690 Acc: 0.7397\n",
      "Epoch 19/25 | Train Loss: 0.3224 Acc: 0.8447 | Val Loss: 0.4762 Acc: 0.7534\n",
      "Epoch 20/25 | Train Loss: 0.3236 Acc: 0.8630 | Val Loss: 0.4555 Acc: 0.7580\n",
      "Epoch 21/25 | Train Loss: 0.3335 Acc: 0.8630 | Val Loss: 0.4657 Acc: 0.7352\n",
      "Epoch 22/25 | Train Loss: 0.3557 Acc: 0.8539 | Val Loss: 0.4474 Acc: 0.7626\n",
      "Epoch 23/25 | Train Loss: 0.3472 Acc: 0.8539 | Val Loss: 0.4775 Acc: 0.7397\n",
      "Epoch 24/25 | Train Loss: 0.3142 Acc: 0.8539 | Val Loss: 0.4381 Acc: 0.7763\n",
      "Epoch 25/25 | Train Loss: 0.3404 Acc: 0.8402 | Val Loss: 0.4609 Acc: 0.7717\n",
      "Fold 8 Test Accuracy: 0.7717\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.6134 Acc: 0.6621 | Val Loss: 0.5025 Acc: 0.7900\n",
      "Epoch 2/25 | Train Loss: 0.7012 Acc: 0.6621 | Val Loss: 0.5760 Acc: 0.7717\n",
      "Epoch 3/25 | Train Loss: 0.5879 Acc: 0.6758 | Val Loss: 0.4702 Acc: 0.7626\n",
      "Epoch 4/25 | Train Loss: 0.5584 Acc: 0.7215 | Val Loss: 0.4884 Acc: 0.7123\n",
      "Epoch 5/25 | Train Loss: 0.4738 Acc: 0.7534 | Val Loss: 0.4455 Acc: 0.7808\n",
      "Epoch 6/25 | Train Loss: 0.5220 Acc: 0.7306 | Val Loss: 0.4485 Acc: 0.7580\n",
      "Epoch 7/25 | Train Loss: 0.4877 Acc: 0.7534 | Val Loss: 0.4891 Acc: 0.7945\n",
      "Epoch 8/25 | Train Loss: 0.5625 Acc: 0.7626 | Val Loss: 0.4738 Acc: 0.7671\n",
      "Epoch 9/25 | Train Loss: 0.4560 Acc: 0.7443 | Val Loss: 0.3799 Acc: 0.8082\n",
      "Epoch 10/25 | Train Loss: 0.4373 Acc: 0.7854 | Val Loss: 0.3816 Acc: 0.8128\n",
      "Epoch 11/25 | Train Loss: 0.3864 Acc: 0.8037 | Val Loss: 0.3713 Acc: 0.8128\n",
      "Epoch 12/25 | Train Loss: 0.3894 Acc: 0.8128 | Val Loss: 0.4289 Acc: 0.7763\n",
      "Epoch 13/25 | Train Loss: 0.3880 Acc: 0.8356 | Val Loss: 0.3735 Acc: 0.8082\n",
      "Epoch 14/25 | Train Loss: 0.3517 Acc: 0.8493 | Val Loss: 0.3826 Acc: 0.8356\n",
      "Epoch 15/25 | Train Loss: 0.2994 Acc: 0.8721 | Val Loss: 0.3332 Acc: 0.8311\n",
      "Epoch 16/25 | Train Loss: 0.3606 Acc: 0.8174 | Val Loss: 0.3915 Acc: 0.8174\n",
      "Epoch 17/25 | Train Loss: 0.3468 Acc: 0.8676 | Val Loss: 0.3322 Acc: 0.8402\n",
      "Epoch 18/25 | Train Loss: 0.3674 Acc: 0.8447 | Val Loss: 0.3840 Acc: 0.7808\n",
      "Epoch 19/25 | Train Loss: 0.3363 Acc: 0.8676 | Val Loss: 0.3412 Acc: 0.8584\n",
      "Epoch 20/25 | Train Loss: 0.3808 Acc: 0.8174 | Val Loss: 0.3341 Acc: 0.8356\n",
      "Epoch 21/25 | Train Loss: 0.3467 Acc: 0.8356 | Val Loss: 0.3365 Acc: 0.8311\n",
      "Epoch 22/25 | Train Loss: 0.3468 Acc: 0.8493 | Val Loss: 0.3401 Acc: 0.8402\n",
      "Epoch 23/25 | Train Loss: 0.3692 Acc: 0.8174 | Val Loss: 0.3654 Acc: 0.8539\n",
      "Epoch 24/25 | Train Loss: 0.3537 Acc: 0.8128 | Val Loss: 0.3206 Acc: 0.8630\n",
      "Epoch 25/25 | Train Loss: 0.3079 Acc: 0.8721 | Val Loss: 0.3405 Acc: 0.8447\n",
      "Fold 9 Test Accuracy: 0.8402\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.7083 Acc: 0.5525 | Val Loss: 0.5093 Acc: 0.7489\n",
      "Epoch 2/25 | Train Loss: 0.5000 Acc: 0.7489 | Val Loss: 0.6424 Acc: 0.5479\n",
      "Epoch 3/25 | Train Loss: 0.5798 Acc: 0.7489 | Val Loss: 0.6342 Acc: 0.5616\n",
      "Epoch 4/25 | Train Loss: 0.6142 Acc: 0.6849 | Val Loss: 0.5587 Acc: 0.7260\n",
      "Epoch 5/25 | Train Loss: 0.4545 Acc: 0.8037 | Val Loss: 0.5139 Acc: 0.7260\n",
      "Epoch 6/25 | Train Loss: 0.4309 Acc: 0.7808 | Val Loss: 0.5561 Acc: 0.7169\n",
      "Epoch 7/25 | Train Loss: 0.4355 Acc: 0.7717 | Val Loss: 0.5102 Acc: 0.7078\n",
      "Epoch 8/25 | Train Loss: 0.4161 Acc: 0.7900 | Val Loss: 0.4705 Acc: 0.7808\n",
      "Epoch 9/25 | Train Loss: 0.4175 Acc: 0.7808 | Val Loss: 0.4916 Acc: 0.7397\n",
      "Epoch 10/25 | Train Loss: 0.4016 Acc: 0.8128 | Val Loss: 0.4724 Acc: 0.7443\n",
      "Epoch 11/25 | Train Loss: 0.3968 Acc: 0.8037 | Val Loss: 0.4589 Acc: 0.7397\n",
      "Epoch 12/25 | Train Loss: 0.4064 Acc: 0.7991 | Val Loss: 0.4456 Acc: 0.7626\n",
      "Epoch 13/25 | Train Loss: 0.3913 Acc: 0.8219 | Val Loss: 0.4665 Acc: 0.7580\n",
      "Epoch 14/25 | Train Loss: 0.3715 Acc: 0.8174 | Val Loss: 0.4311 Acc: 0.7717\n",
      "Epoch 15/25 | Train Loss: 0.3415 Acc: 0.8402 | Val Loss: 0.4411 Acc: 0.7580\n",
      "Epoch 16/25 | Train Loss: 0.3575 Acc: 0.8265 | Val Loss: 0.4800 Acc: 0.7489\n",
      "Epoch 17/25 | Train Loss: 0.3440 Acc: 0.8311 | Val Loss: 0.4433 Acc: 0.7534\n",
      "Epoch 18/25 | Train Loss: 0.3441 Acc: 0.8447 | Val Loss: 0.4699 Acc: 0.7580\n",
      "Epoch 19/25 | Train Loss: 0.3383 Acc: 0.8311 | Val Loss: 0.4550 Acc: 0.7489\n",
      "Epoch 20/25 | Train Loss: 0.3633 Acc: 0.8219 | Val Loss: 0.4638 Acc: 0.7808\n",
      "Epoch 21/25 | Train Loss: 0.3669 Acc: 0.8311 | Val Loss: 0.4428 Acc: 0.7626\n",
      "Epoch 22/25 | Train Loss: 0.3277 Acc: 0.8539 | Val Loss: 0.4785 Acc: 0.7489\n",
      "Epoch 23/25 | Train Loss: 0.3465 Acc: 0.8356 | Val Loss: 0.4645 Acc: 0.7626\n",
      "Epoch 24/25 | Train Loss: 0.3461 Acc: 0.8265 | Val Loss: 0.4591 Acc: 0.7717\n",
      "Epoch 25/25 | Train Loss: 0.3981 Acc: 0.7945 | Val Loss: 0.4325 Acc: 0.7580\n",
      "Fold 10 Test Accuracy: 0.7443\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8009, Std Dev = 0.0464\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/oversampled_dataset/VAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/oversampled_dataset/VAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T14:34:54.562628Z",
     "start_time": "2025-05-18T13:18:38.358496Z"
    }
   },
   "id": "339b494660715202",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Syntethic dataset (negative class only made from generated images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d584694220a9823"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ab0e479319d2a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T05:30:01.540388Z",
     "start_time": "2025-05-18T05:30:01.525220Z"
    }
   },
   "id": "4236292aeee6eb5a",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.2726 Acc: 0.8767 | Val Loss: 0.0956 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0280 Acc: 1.0000 | Val Loss: 0.0093 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0031 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0140 Acc: 0.9954 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0132 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0007 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.2200 Acc: 0.9132 | Val Loss: 0.0676 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0247 Acc: 0.9954 | Val Loss: 0.0085 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0264 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0198 Acc: 0.9909 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0060 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0312 Acc: 0.9817 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0088 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 1.0000\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.3141 Acc: 0.8858 | Val Loss: 0.2268 Acc: 0.9498\n",
      "Epoch 2/25 | Train Loss: 0.0270 Acc: 1.0000 | Val Loss: 0.0121 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0073 Acc: 1.0000 | Val Loss: 0.0040 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0213 Acc: 0.9909 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0158 Acc: 1.0000 | Val Loss: 0.0044 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0149 Acc: 0.9954 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0123 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0074 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 1.0000\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.3270 Acc: 0.8447 | Val Loss: 0.0958 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0064 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0324 Acc: 0.9954 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0023 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0076 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0122 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0242 Acc: 0.9863 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0090 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0127 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0313 Acc: 0.9909 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.3339 Acc: 0.8265 | Val Loss: 0.0685 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0241 Acc: 1.0000 | Val Loss: 0.0107 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0282 Acc: 0.9954 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0082 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0097 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 1.0000\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.2898 Acc: 0.8539 | Val Loss: 0.1650 Acc: 0.9909\n",
      "Epoch 2/25 | Train Loss: 0.0170 Acc: 1.0000 | Val Loss: 0.0212 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0284 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0102 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0153 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 1.0000\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.3460 Acc: 0.8356 | Val Loss: 0.0851 Acc: 0.9954\n",
      "Epoch 2/25 | Train Loss: 0.0191 Acc: 1.0000 | Val Loss: 0.0044 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0206 Acc: 0.9909 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0108 Acc: 1.0000 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0103 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0090 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0137 Acc: 0.9954 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.3080 Acc: 0.8676 | Val Loss: 0.0785 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0404 Acc: 0.9954 | Val Loss: 0.0031 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3343 Acc: 0.8493 | Val Loss: 0.0785 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0347 Acc: 0.9954 | Val Loss: 0.0085 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0109 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0255 Acc: 0.9863 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0146 Acc: 0.9954 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0073 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0100 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0305 Acc: 0.9863 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 1.0000\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.3599 Acc: 0.8082 | Val Loss: 0.0908 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0109 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0146 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0104 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0092 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0069 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5×2 CV results: Mean Acc = 1.0000, Std Dev = 0.0000\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T06:23:32.513883Z",
     "start_time": "2025-05-18T05:30:01.542396Z"
    }
   },
   "id": "1da07ad0daad1554",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "497806a17aea1d08"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/GAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T06:23:32.531583Z",
     "start_time": "2025-05-18T06:23:32.514892Z"
    }
   },
   "id": "70dee764acdfa018",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.2561 Acc: 0.8767 | Val Loss: 0.0378 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0172 Acc: 0.9954 | Val Loss: 0.0041 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0023 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0218 Acc: 0.9909 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.2520 Acc: 0.9315 | Val Loss: 0.0573 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0071 Acc: 1.0000 | Val Loss: 0.0062 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0210 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0103 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0071 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0005 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 1.0000\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.2041 Acc: 0.9406 | Val Loss: 0.0734 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0150 Acc: 1.0000 | Val Loss: 0.0029 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0113 Acc: 0.9954 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0081 Acc: 0.9954 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 1.0000\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.2824 Acc: 0.8584 | Val Loss: 0.0506 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0308 Acc: 0.9909 | Val Loss: 0.0047 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0186 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0134 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0278 Acc: 0.9909 | Val Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0250 Acc: 0.9909 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.1864 Acc: 0.9680 | Val Loss: 0.0449 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0318 Acc: 1.0000 | Val Loss: 0.0041 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0117 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0088 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0007 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 1.0000\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.2036 Acc: 0.9498 | Val Loss: 0.0760 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0068 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0100 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0056 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0253 Acc: 0.9909 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0207 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0071 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 1.0000\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.1505 Acc: 0.9817 | Val Loss: 0.0789 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0235 Acc: 1.0000 | Val Loss: 0.0030 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0312 Acc: 0.9909 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0176 Acc: 0.9909 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0117 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0081 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0045 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.2866 Acc: 0.8630 | Val Loss: 0.0445 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0041 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0079 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0105 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0111 Acc: 0.9954 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3227 Acc: 0.8082 | Val Loss: 0.0419 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0418 Acc: 0.9909 | Val Loss: 0.0041 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0085 Acc: 0.9954 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0038 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 1.0000\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.2755 Acc: 0.8539 | Val Loss: 0.1037 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0081 Acc: 1.0000 | Val Loss: 0.0037 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0130 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0142 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0085 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5×2 CV results: Mean Acc = 1.0000, Std Dev = 0.0000\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/GAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/GAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T07:16:38.320981Z",
     "start_time": "2025-05-18T06:23:32.532591Z"
    }
   },
   "id": "ef55ca576c8a7482",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80a67814c1f2e3e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T14:34:54.574802Z",
     "start_time": "2025-05-18T14:34:54.565027Z"
    }
   },
   "id": "f7b7b351d2e2435",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.3896 Acc: 0.8037 | Val Loss: 0.0933 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0291 Acc: 1.0000 | Val Loss: 0.0050 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0356 Acc: 0.9909 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0183 Acc: 0.9954 | Val Loss: 0.0037 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0164 Acc: 0.9954 | Val Loss: 0.0191 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0077 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0091 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0056 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0086 Acc: 0.9954 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0131 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0084 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.3403 Acc: 0.8493 | Val Loss: 0.0729 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0352 Acc: 1.0000 | Val Loss: 0.0149 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0078 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0026 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0501 Acc: 0.9726 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0155 Acc: 0.9909 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0241 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0191 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0104 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 0.9863\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.3590 Acc: 0.8447 | Val Loss: 0.1136 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0366 Acc: 1.0000 | Val Loss: 0.0069 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0117 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0701 Acc: 0.9680 | Val Loss: 0.0059 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0170 Acc: 0.9954 | Val Loss: 0.0029 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0059 Acc: 1.0000 | Val Loss: 0.0051 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0053 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 0.9954\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.3403 Acc: 0.8630 | Val Loss: 0.0945 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0314 Acc: 1.0000 | Val Loss: 0.0088 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0025 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0019 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0028 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0238 Acc: 0.9954 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0090 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0273 Acc: 0.9863 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0260 Acc: 0.9909 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0100 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.3381 Acc: 0.8630 | Val Loss: 0.1809 Acc: 0.9863\n",
      "Epoch 2/25 | Train Loss: 0.0221 Acc: 1.0000 | Val Loss: 0.0273 Acc: 0.9954\n",
      "Epoch 3/25 | Train Loss: 0.0228 Acc: 0.9954 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0082 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0096 Acc: 0.9954 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0090 Acc: 0.9954 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0141 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 1.0000\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.3743 Acc: 0.8402 | Val Loss: 0.1135 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0520 Acc: 0.9909 | Val Loss: 0.0096 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0237 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0068 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0144 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0124 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0059 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 1.0000\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.3053 Acc: 0.8995 | Val Loss: 0.0741 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0912 Acc: 0.9772 | Val Loss: 0.0039 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0160 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0070 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0117 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0218 Acc: 0.9863 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0102 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0143 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0120 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.3281 Acc: 0.8767 | Val Loss: 0.0789 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0788 Acc: 0.9726 | Val Loss: 0.0076 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0093 Acc: 1.0000 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0023 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0151 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0116 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0074 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0087 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0092 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0087 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0391 Acc: 0.9863 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0192 Acc: 0.9954 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0143 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3810 Acc: 0.8311 | Val Loss: 0.0821 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0403 Acc: 0.9954 | Val Loss: 0.0037 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0119 Acc: 1.0000 | Val Loss: 0.0026 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0181 Acc: 0.9954 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0021 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0111 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0027 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0106 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0153 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 1.0000\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.3095 Acc: 0.8767 | Val Loss: 0.0919 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0415 Acc: 1.0000 | Val Loss: 0.0122 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0123 Acc: 1.0000 | Val Loss: 0.0036 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0062 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0212 Acc: 0.9954 | Val Loss: 0.0409 Acc: 0.9863\n",
      "Epoch 7/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0086 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0156 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0088 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0070 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0220 Acc: 0.9954 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0118 Acc: 0.9909 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0045 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0056 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0080 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.9982, Std Dev = 0.0044\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T15:28:48.417453Z",
     "start_time": "2025-05-18T14:34:54.576809Z"
    }
   },
   "id": "2e0987154c90646c",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b2f6414f6b32247"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/VAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[sample_folder], max_per_dir=219, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T15:28:48.439463Z",
     "start_time": "2025-05-18T15:28:48.419463Z"
    }
   },
   "id": "45dcf8ee2dd545e8",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.2747 Acc: 0.8904 | Val Loss: 0.0642 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0092 Acc: 1.0000 | Val Loss: 0.0101 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0058 Acc: 1.0000 | Val Loss: 0.0036 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0137 Acc: 0.9954 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0015 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0311 Acc: 0.9863 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0089 Acc: 0.9954 | Val Loss: 0.0018 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0065 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0040 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0129 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0036 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0321 Acc: 0.9909 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 1.0000\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.3295 Acc: 0.8539 | Val Loss: 0.0598 Acc: 0.9909\n",
      "Epoch 2/25 | Train Loss: 0.0302 Acc: 1.0000 | Val Loss: 0.0081 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0076 Acc: 1.0000 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0099 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0093 Acc: 0.9954 | Val Loss: 0.0016 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0074 Acc: 0.9954 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0007 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0141 Acc: 0.9954 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 2 Test Accuracy: 1.0000\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.2893 Acc: 0.8630 | Val Loss: 0.0438 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0384 Acc: 0.9909 | Val Loss: 0.0041 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0103 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0030 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0138 Acc: 0.9954 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Fold 3 Test Accuracy: 1.0000\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.2576 Acc: 0.9087 | Val Loss: 0.0549 Acc: 0.9954\n",
      "Epoch 2/25 | Train Loss: 0.0493 Acc: 0.9909 | Val Loss: 0.0069 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0122 Acc: 0.9954 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0041 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0045 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0115 Acc: 0.9954 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0236 Acc: 0.9954 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Fold 4 Test Accuracy: 1.0000\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.3664 Acc: 0.8356 | Val Loss: 0.0965 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0248 Acc: 1.0000 | Val Loss: 0.0132 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0064 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0020 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0046 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0081 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0241 Acc: 0.9909 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0063 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Fold 5 Test Accuracy: 0.9863\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.2335 Acc: 0.9087 | Val Loss: 0.0818 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0174 Acc: 1.0000 | Val Loss: 0.0069 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0265 Acc: 0.9909 | Val Loss: 0.0022 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0051 Acc: 1.0000 | Val Loss: 0.0012 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0304 Acc: 0.9863 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0028 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0054 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0072 Acc: 0.9954 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0079 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0085 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0055 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0094 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Fold 6 Test Accuracy: 0.9954\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.2228 Acc: 0.9361 | Val Loss: 0.0728 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0210 Acc: 1.0000 | Val Loss: 0.0153 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0123 Acc: 1.0000 | Val Loss: 0.0046 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0048 Acc: 1.0000 | Val Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0075 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0029 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0072 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0032 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0043 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0037 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0079 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0403 Acc: 0.9817 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Fold 7 Test Accuracy: 1.0000\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.2737 Acc: 0.8995 | Val Loss: 0.1153 Acc: 0.9772\n",
      "Epoch 2/25 | Train Loss: 0.0231 Acc: 1.0000 | Val Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0180 Acc: 1.0000 | Val Loss: 0.0062 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0044 Acc: 1.0000 | Val Loss: 0.0014 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0057 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0264 Acc: 0.9863 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0065 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0189 Acc: 0.9954 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0069 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0007 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0018 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0022 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0067 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0011 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0052 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Fold 8 Test Accuracy: 1.0000\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.3019 Acc: 0.9041 | Val Loss: 0.0689 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0125 Acc: 1.0000 | Val Loss: 0.0058 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0060 Acc: 1.0000 | Val Loss: 0.0024 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0049 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0132 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0013 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0009 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0027 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0074 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0338 Acc: 0.9817 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0079 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0060 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0066 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0010 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0031 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0039 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0021 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0035 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0061 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0047 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Fold 9 Test Accuracy: 0.9909\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.4421 Acc: 0.7717 | Val Loss: 0.0371 Acc: 1.0000\n",
      "Epoch 2/25 | Train Loss: 0.0362 Acc: 1.0000 | Val Loss: 0.0082 Acc: 1.0000\n",
      "Epoch 3/25 | Train Loss: 0.0164 Acc: 0.9954 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 4/25 | Train Loss: 0.0017 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 5/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 6/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 7/25 | Train Loss: 0.0083 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 8/25 | Train Loss: 0.0050 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 9/25 | Train Loss: 0.0012 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 10/25 | Train Loss: 0.0042 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 11/25 | Train Loss: 0.0024 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 12/25 | Train Loss: 0.0008 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 13/25 | Train Loss: 0.0014 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 14/25 | Train Loss: 0.0033 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 15/25 | Train Loss: 0.0019 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 16/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 17/25 | Train Loss: 0.0020 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 18/25 | Train Loss: 0.0700 Acc: 0.9680 | Val Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 19/25 | Train Loss: 0.0080 Acc: 0.9954 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 20/25 | Train Loss: 0.0023 Acc: 1.0000 | Val Loss: 0.0008 Acc: 1.0000\n",
      "Epoch 21/25 | Train Loss: 0.0034 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 22/25 | Train Loss: 0.0016 Acc: 1.0000 | Val Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 23/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 24/25 | Train Loss: 0.0013 Acc: 1.0000 | Val Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 25/25 | Train Loss: 0.0025 Acc: 1.0000 | Val Loss: 0.0004 Acc: 1.0000\n",
      "Fold 10 Test Accuracy: 1.0000\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.9973, Std Dev = 0.0049\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/synthetic_dataset/VAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/synthetic_dataset/VAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T16:22:26.788833Z",
     "start_time": "2025-05-18T15:28:48.440975Z"
    }
   },
   "id": "375b0f4f6e575111",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Over-oversampled dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a22bc78da939e67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ab2e653f59225c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNGAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T09:03:00.001267Z",
     "start_time": "2025-05-18T09:02:59.986146Z"
    }
   },
   "id": "7e11a8a1d8aeb8cc",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.4942 Acc: 0.7482 | Val Loss: 0.4767 Acc: 0.7993\n",
      "Epoch 2/25 | Train Loss: 0.4869 Acc: 0.7409 | Val Loss: 0.4522 Acc: 0.7153\n",
      "Epoch 3/25 | Train Loss: 0.3837 Acc: 0.8066 | Val Loss: 0.5400 Acc: 0.6642\n",
      "Epoch 4/25 | Train Loss: 0.4839 Acc: 0.7263 | Val Loss: 0.4458 Acc: 0.6642\n",
      "Epoch 5/25 | Train Loss: 0.4508 Acc: 0.7591 | Val Loss: 0.3456 Acc: 0.8394\n",
      "Epoch 6/25 | Train Loss: 0.3910 Acc: 0.8248 | Val Loss: 0.3947 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.3735 Acc: 0.8431 | Val Loss: 0.4566 Acc: 0.6861\n",
      "Epoch 8/25 | Train Loss: 0.4287 Acc: 0.7190 | Val Loss: 0.3816 Acc: 0.7956\n",
      "Epoch 9/25 | Train Loss: 0.3653 Acc: 0.8139 | Val Loss: 0.3381 Acc: 0.8358\n",
      "Epoch 10/25 | Train Loss: 0.3448 Acc: 0.8066 | Val Loss: 0.3607 Acc: 0.7956\n",
      "Epoch 11/25 | Train Loss: 0.3717 Acc: 0.8175 | Val Loss: 0.3289 Acc: 0.8358\n",
      "Epoch 12/25 | Train Loss: 0.3529 Acc: 0.8139 | Val Loss: 0.3549 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3260 Acc: 0.8504 | Val Loss: 0.3346 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.3185 Acc: 0.8613 | Val Loss: 0.3549 Acc: 0.8321\n",
      "Epoch 15/25 | Train Loss: 0.3066 Acc: 0.8796 | Val Loss: 0.3540 Acc: 0.8175\n",
      "Epoch 16/25 | Train Loss: 0.2965 Acc: 0.8759 | Val Loss: 0.3346 Acc: 0.8577\n",
      "Epoch 17/25 | Train Loss: 0.3293 Acc: 0.8577 | Val Loss: 0.3476 Acc: 0.8321\n",
      "Epoch 18/25 | Train Loss: 0.3386 Acc: 0.8467 | Val Loss: 0.3273 Acc: 0.8212\n",
      "Epoch 19/25 | Train Loss: 0.3483 Acc: 0.8504 | Val Loss: 0.3380 Acc: 0.8066\n",
      "Epoch 20/25 | Train Loss: 0.3233 Acc: 0.8723 | Val Loss: 0.3176 Acc: 0.8504\n",
      "Epoch 21/25 | Train Loss: 0.2945 Acc: 0.8832 | Val Loss: 0.3120 Acc: 0.8504\n",
      "Epoch 22/25 | Train Loss: 0.3333 Acc: 0.8431 | Val Loss: 0.3403 Acc: 0.8431\n",
      "Epoch 23/25 | Train Loss: 0.3025 Acc: 0.8723 | Val Loss: 0.3473 Acc: 0.8394\n",
      "Epoch 24/25 | Train Loss: 0.3156 Acc: 0.8504 | Val Loss: 0.3301 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.3449 Acc: 0.8394 | Val Loss: 0.3309 Acc: 0.8431\n",
      "Fold 1 Test Accuracy: 0.8467\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.6129 Acc: 0.6569 | Val Loss: 0.5563 Acc: 0.6350\n",
      "Epoch 2/25 | Train Loss: 0.5788 Acc: 0.6898 | Val Loss: 0.3772 Acc: 0.8248\n",
      "Epoch 3/25 | Train Loss: 0.6517 Acc: 0.7299 | Val Loss: 0.3700 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.5849 Acc: 0.7737 | Val Loss: 0.4922 Acc: 0.7701\n",
      "Epoch 5/25 | Train Loss: 0.4477 Acc: 0.7993 | Val Loss: 0.3836 Acc: 0.8139\n",
      "Epoch 6/25 | Train Loss: 0.4050 Acc: 0.7847 | Val Loss: 0.3952 Acc: 0.8029\n",
      "Epoch 7/25 | Train Loss: 0.4747 Acc: 0.7591 | Val Loss: 0.3933 Acc: 0.8029\n",
      "Epoch 8/25 | Train Loss: 0.3918 Acc: 0.8175 | Val Loss: 0.3942 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3827 Acc: 0.7956 | Val Loss: 0.4005 Acc: 0.8029\n",
      "Epoch 10/25 | Train Loss: 0.3549 Acc: 0.8285 | Val Loss: 0.3541 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3778 Acc: 0.8029 | Val Loss: 0.3681 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3617 Acc: 0.8212 | Val Loss: 0.3910 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3853 Acc: 0.7956 | Val Loss: 0.3646 Acc: 0.8066\n",
      "Epoch 14/25 | Train Loss: 0.3675 Acc: 0.8175 | Val Loss: 0.3640 Acc: 0.8139\n",
      "Epoch 15/25 | Train Loss: 0.3470 Acc: 0.8248 | Val Loss: 0.3750 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.3724 Acc: 0.7993 | Val Loss: 0.3732 Acc: 0.8139\n",
      "Epoch 17/25 | Train Loss: 0.3843 Acc: 0.8066 | Val Loss: 0.3558 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3608 Acc: 0.8212 | Val Loss: 0.3591 Acc: 0.8175\n",
      "Epoch 19/25 | Train Loss: 0.4010 Acc: 0.7883 | Val Loss: 0.3521 Acc: 0.8102\n",
      "Epoch 20/25 | Train Loss: 0.3781 Acc: 0.8175 | Val Loss: 0.3660 Acc: 0.8139\n",
      "Epoch 21/25 | Train Loss: 0.3702 Acc: 0.8066 | Val Loss: 0.3673 Acc: 0.8139\n",
      "Epoch 22/25 | Train Loss: 0.3792 Acc: 0.8029 | Val Loss: 0.3547 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3728 Acc: 0.8212 | Val Loss: 0.3585 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.3733 Acc: 0.8066 | Val Loss: 0.3686 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.3619 Acc: 0.8066 | Val Loss: 0.3722 Acc: 0.8066\n",
      "Fold 2 Test Accuracy: 0.8029\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5548 Acc: 0.7044 | Val Loss: 0.4236 Acc: 0.8102\n",
      "Epoch 2/25 | Train Loss: 0.4508 Acc: 0.7774 | Val Loss: 0.6651 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4113 Acc: 0.7883 | Val Loss: 0.4188 Acc: 0.7701\n",
      "Epoch 4/25 | Train Loss: 0.3829 Acc: 0.8102 | Val Loss: 0.4539 Acc: 0.7007\n",
      "Epoch 5/25 | Train Loss: 0.3765 Acc: 0.8248 | Val Loss: 0.3927 Acc: 0.7445\n",
      "Epoch 6/25 | Train Loss: 0.3537 Acc: 0.8175 | Val Loss: 0.5170 Acc: 0.8066\n",
      "Epoch 7/25 | Train Loss: 0.4060 Acc: 0.7737 | Val Loss: 0.4173 Acc: 0.8102\n",
      "Epoch 8/25 | Train Loss: 0.4211 Acc: 0.8029 | Val Loss: 0.3732 Acc: 0.8066\n",
      "Epoch 9/25 | Train Loss: 0.3745 Acc: 0.8358 | Val Loss: 0.3693 Acc: 0.7993\n",
      "Epoch 10/25 | Train Loss: 0.3398 Acc: 0.8285 | Val Loss: 0.3505 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3138 Acc: 0.8540 | Val Loss: 0.3431 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3295 Acc: 0.8540 | Val Loss: 0.3692 Acc: 0.8175\n",
      "Epoch 13/25 | Train Loss: 0.2821 Acc: 0.8650 | Val Loss: 0.3389 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.2719 Acc: 0.8796 | Val Loss: 0.3336 Acc: 0.8212\n",
      "Epoch 15/25 | Train Loss: 0.2727 Acc: 0.8759 | Val Loss: 0.3123 Acc: 0.8394\n",
      "Epoch 16/25 | Train Loss: 0.2770 Acc: 0.8686 | Val Loss: 0.3225 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.2839 Acc: 0.8869 | Val Loss: 0.3396 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.2782 Acc: 0.8759 | Val Loss: 0.3356 Acc: 0.8358\n",
      "Epoch 19/25 | Train Loss: 0.3244 Acc: 0.8467 | Val Loss: 0.3299 Acc: 0.8504\n",
      "Epoch 20/25 | Train Loss: 0.2715 Acc: 0.8832 | Val Loss: 0.3105 Acc: 0.8431\n",
      "Epoch 21/25 | Train Loss: 0.2841 Acc: 0.8686 | Val Loss: 0.3231 Acc: 0.8175\n",
      "Epoch 22/25 | Train Loss: 0.2787 Acc: 0.8796 | Val Loss: 0.3270 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.2919 Acc: 0.8832 | Val Loss: 0.3182 Acc: 0.8358\n",
      "Epoch 24/25 | Train Loss: 0.3016 Acc: 0.8504 | Val Loss: 0.3056 Acc: 0.8504\n",
      "Epoch 25/25 | Train Loss: 0.3050 Acc: 0.8540 | Val Loss: 0.3189 Acc: 0.8504\n",
      "Fold 3 Test Accuracy: 0.8467\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5348 Acc: 0.7153 | Val Loss: 0.4500 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.5139 Acc: 0.7153 | Val Loss: 0.4264 Acc: 0.7628\n",
      "Epoch 3/25 | Train Loss: 0.3751 Acc: 0.7956 | Val Loss: 0.3781 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.4094 Acc: 0.7883 | Val Loss: 0.3708 Acc: 0.8102\n",
      "Epoch 5/25 | Train Loss: 0.5036 Acc: 0.7993 | Val Loss: 0.5336 Acc: 0.6788\n",
      "Epoch 6/25 | Train Loss: 0.3633 Acc: 0.8358 | Val Loss: 0.4084 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.3440 Acc: 0.8358 | Val Loss: 0.3586 Acc: 0.8139\n",
      "Epoch 8/25 | Train Loss: 0.3608 Acc: 0.8285 | Val Loss: 0.3328 Acc: 0.8321\n",
      "Epoch 9/25 | Train Loss: 0.3723 Acc: 0.8358 | Val Loss: 0.3247 Acc: 0.8504\n",
      "Epoch 10/25 | Train Loss: 0.3694 Acc: 0.8394 | Val Loss: 0.3351 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3181 Acc: 0.8431 | Val Loss: 0.3720 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.3305 Acc: 0.8467 | Val Loss: 0.3205 Acc: 0.8431\n",
      "Epoch 13/25 | Train Loss: 0.3211 Acc: 0.8467 | Val Loss: 0.3041 Acc: 0.8650\n",
      "Epoch 14/25 | Train Loss: 0.3229 Acc: 0.8394 | Val Loss: 0.3360 Acc: 0.8139\n",
      "Epoch 15/25 | Train Loss: 0.3138 Acc: 0.8577 | Val Loss: 0.3332 Acc: 0.8321\n",
      "Epoch 16/25 | Train Loss: 0.3369 Acc: 0.8358 | Val Loss: 0.3046 Acc: 0.8613\n",
      "Epoch 17/25 | Train Loss: 0.3275 Acc: 0.8504 | Val Loss: 0.3210 Acc: 0.8613\n",
      "Epoch 18/25 | Train Loss: 0.2857 Acc: 0.8686 | Val Loss: 0.3257 Acc: 0.8321\n",
      "Epoch 19/25 | Train Loss: 0.2834 Acc: 0.8796 | Val Loss: 0.3209 Acc: 0.8431\n",
      "Epoch 20/25 | Train Loss: 0.2908 Acc: 0.8577 | Val Loss: 0.3109 Acc: 0.8540\n",
      "Epoch 21/25 | Train Loss: 0.3291 Acc: 0.8431 | Val Loss: 0.3219 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.2911 Acc: 0.8650 | Val Loss: 0.3286 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.2802 Acc: 0.8723 | Val Loss: 0.3318 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.2884 Acc: 0.8613 | Val Loss: 0.3314 Acc: 0.8394\n",
      "Epoch 25/25 | Train Loss: 0.2968 Acc: 0.8540 | Val Loss: 0.3145 Acc: 0.8394\n",
      "Fold 4 Test Accuracy: 0.8577\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.6469 Acc: 0.6496 | Val Loss: 0.4629 Acc: 0.8029\n",
      "Epoch 2/25 | Train Loss: 0.4287 Acc: 0.7555 | Val Loss: 0.4223 Acc: 0.8066\n",
      "Epoch 3/25 | Train Loss: 0.4282 Acc: 0.7664 | Val Loss: 0.5532 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4849 Acc: 0.7591 | Val Loss: 0.4124 Acc: 0.7847\n",
      "Epoch 5/25 | Train Loss: 0.5204 Acc: 0.8102 | Val Loss: 0.6646 Acc: 0.6387\n",
      "Epoch 6/25 | Train Loss: 0.4537 Acc: 0.7737 | Val Loss: 0.4421 Acc: 0.7336\n",
      "Epoch 7/25 | Train Loss: 0.4210 Acc: 0.7847 | Val Loss: 0.4154 Acc: 0.7810\n",
      "Epoch 8/25 | Train Loss: 0.3688 Acc: 0.8175 | Val Loss: 0.3853 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3566 Acc: 0.8212 | Val Loss: 0.3386 Acc: 0.8285\n",
      "Epoch 10/25 | Train Loss: 0.3413 Acc: 0.8358 | Val Loss: 0.3402 Acc: 0.8248\n",
      "Epoch 11/25 | Train Loss: 0.3484 Acc: 0.8102 | Val Loss: 0.3662 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.3342 Acc: 0.8248 | Val Loss: 0.3513 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.3254 Acc: 0.8431 | Val Loss: 0.3581 Acc: 0.8321\n",
      "Epoch 14/25 | Train Loss: 0.3515 Acc: 0.8394 | Val Loss: 0.3574 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.3250 Acc: 0.8394 | Val Loss: 0.3784 Acc: 0.7883\n",
      "Epoch 16/25 | Train Loss: 0.3379 Acc: 0.8285 | Val Loss: 0.3555 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.3718 Acc: 0.8139 | Val Loss: 0.3597 Acc: 0.8248\n",
      "Epoch 18/25 | Train Loss: 0.3426 Acc: 0.8394 | Val Loss: 0.3683 Acc: 0.8066\n",
      "Epoch 19/25 | Train Loss: 0.3128 Acc: 0.8577 | Val Loss: 0.3596 Acc: 0.8285\n",
      "Epoch 20/25 | Train Loss: 0.3358 Acc: 0.8285 | Val Loss: 0.3639 Acc: 0.8358\n",
      "Epoch 21/25 | Train Loss: 0.3358 Acc: 0.8212 | Val Loss: 0.3467 Acc: 0.8248\n",
      "Epoch 22/25 | Train Loss: 0.3381 Acc: 0.8285 | Val Loss: 0.3471 Acc: 0.8504\n",
      "Epoch 23/25 | Train Loss: 0.3305 Acc: 0.8394 | Val Loss: 0.3376 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.3470 Acc: 0.8285 | Val Loss: 0.3273 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3406 Acc: 0.8358 | Val Loss: 0.3626 Acc: 0.8248\n",
      "Fold 5 Test Accuracy: 0.7993\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5378 Acc: 0.7226 | Val Loss: 0.4185 Acc: 0.8066\n",
      "Epoch 2/25 | Train Loss: 0.4670 Acc: 0.7701 | Val Loss: 0.4385 Acc: 0.7993\n",
      "Epoch 3/25 | Train Loss: 0.4807 Acc: 0.7555 | Val Loss: 0.3775 Acc: 0.8212\n",
      "Epoch 4/25 | Train Loss: 0.4626 Acc: 0.7737 | Val Loss: 0.3440 Acc: 0.8212\n",
      "Epoch 5/25 | Train Loss: 0.3817 Acc: 0.8102 | Val Loss: 0.6593 Acc: 0.6569\n",
      "Epoch 6/25 | Train Loss: 0.4254 Acc: 0.7847 | Val Loss: 0.3887 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.4272 Acc: 0.8029 | Val Loss: 0.4658 Acc: 0.7044\n",
      "Epoch 8/25 | Train Loss: 0.4568 Acc: 0.7409 | Val Loss: 0.3743 Acc: 0.8212\n",
      "Epoch 9/25 | Train Loss: 0.3859 Acc: 0.7920 | Val Loss: 0.3616 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3370 Acc: 0.8212 | Val Loss: 0.3445 Acc: 0.8248\n",
      "Epoch 11/25 | Train Loss: 0.3427 Acc: 0.8248 | Val Loss: 0.3665 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3287 Acc: 0.8577 | Val Loss: 0.3871 Acc: 0.7883\n",
      "Epoch 13/25 | Train Loss: 0.3185 Acc: 0.8540 | Val Loss: 0.3615 Acc: 0.8139\n",
      "Epoch 14/25 | Train Loss: 0.3393 Acc: 0.8504 | Val Loss: 0.4028 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3412 Acc: 0.8285 | Val Loss: 0.3857 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3110 Acc: 0.8504 | Val Loss: 0.3910 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3121 Acc: 0.8723 | Val Loss: 0.3527 Acc: 0.8285\n",
      "Epoch 18/25 | Train Loss: 0.3129 Acc: 0.8686 | Val Loss: 0.3492 Acc: 0.8394\n",
      "Epoch 19/25 | Train Loss: 0.3474 Acc: 0.8358 | Val Loss: 0.3681 Acc: 0.8175\n",
      "Epoch 20/25 | Train Loss: 0.3007 Acc: 0.8650 | Val Loss: 0.3719 Acc: 0.8139\n",
      "Epoch 21/25 | Train Loss: 0.3269 Acc: 0.8613 | Val Loss: 0.3585 Acc: 0.7993\n",
      "Epoch 22/25 | Train Loss: 0.3176 Acc: 0.8358 | Val Loss: 0.3506 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3174 Acc: 0.8431 | Val Loss: 0.3763 Acc: 0.8212\n",
      "Epoch 24/25 | Train Loss: 0.3281 Acc: 0.8431 | Val Loss: 0.3666 Acc: 0.8175\n",
      "Epoch 25/25 | Train Loss: 0.3424 Acc: 0.8431 | Val Loss: 0.3650 Acc: 0.8212\n",
      "Fold 6 Test Accuracy: 0.8285\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.5621 Acc: 0.6898 | Val Loss: 0.3681 Acc: 0.8212\n",
      "Epoch 2/25 | Train Loss: 0.4888 Acc: 0.7336 | Val Loss: 0.4843 Acc: 0.6861\n",
      "Epoch 3/25 | Train Loss: 0.4197 Acc: 0.7993 | Val Loss: 0.4540 Acc: 0.7153\n",
      "Epoch 4/25 | Train Loss: 0.4103 Acc: 0.7847 | Val Loss: 0.6431 Acc: 0.7299\n",
      "Epoch 5/25 | Train Loss: 0.6105 Acc: 0.7044 | Val Loss: 0.3520 Acc: 0.8175\n",
      "Epoch 6/25 | Train Loss: 0.4199 Acc: 0.7701 | Val Loss: 0.3155 Acc: 0.8431\n",
      "Epoch 7/25 | Train Loss: 0.4219 Acc: 0.8248 | Val Loss: 0.4505 Acc: 0.7737\n",
      "Epoch 8/25 | Train Loss: 0.4162 Acc: 0.7737 | Val Loss: 0.3147 Acc: 0.8540\n",
      "Epoch 9/25 | Train Loss: 0.3518 Acc: 0.8358 | Val Loss: 0.2863 Acc: 0.8723\n",
      "Epoch 10/25 | Train Loss: 0.3270 Acc: 0.8504 | Val Loss: 0.3067 Acc: 0.8613\n",
      "Epoch 11/25 | Train Loss: 0.3053 Acc: 0.8650 | Val Loss: 0.3203 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3293 Acc: 0.8467 | Val Loss: 0.3062 Acc: 0.8540\n",
      "Epoch 13/25 | Train Loss: 0.2851 Acc: 0.8796 | Val Loss: 0.2900 Acc: 0.8540\n",
      "Epoch 14/25 | Train Loss: 0.3103 Acc: 0.8248 | Val Loss: 0.2686 Acc: 0.8650\n",
      "Epoch 15/25 | Train Loss: 0.2993 Acc: 0.8796 | Val Loss: 0.2830 Acc: 0.8613\n",
      "Epoch 16/25 | Train Loss: 0.2774 Acc: 0.8650 | Val Loss: 0.2896 Acc: 0.8577\n",
      "Epoch 17/25 | Train Loss: 0.3147 Acc: 0.8540 | Val Loss: 0.2778 Acc: 0.8686\n",
      "Epoch 18/25 | Train Loss: 0.2937 Acc: 0.8759 | Val Loss: 0.2769 Acc: 0.8686\n",
      "Epoch 19/25 | Train Loss: 0.2651 Acc: 0.8942 | Val Loss: 0.2827 Acc: 0.8613\n",
      "Epoch 20/25 | Train Loss: 0.2981 Acc: 0.8686 | Val Loss: 0.2836 Acc: 0.8650\n",
      "Epoch 21/25 | Train Loss: 0.2971 Acc: 0.8467 | Val Loss: 0.2712 Acc: 0.8577\n",
      "Epoch 22/25 | Train Loss: 0.3298 Acc: 0.8394 | Val Loss: 0.2761 Acc: 0.8796\n",
      "Epoch 23/25 | Train Loss: 0.3022 Acc: 0.8686 | Val Loss: 0.2829 Acc: 0.8613\n",
      "Epoch 24/25 | Train Loss: 0.2746 Acc: 0.8650 | Val Loss: 0.2898 Acc: 0.8540\n",
      "Epoch 25/25 | Train Loss: 0.2827 Acc: 0.8686 | Val Loss: 0.2846 Acc: 0.8577\n",
      "Fold 7 Test Accuracy: 0.8613\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.4539 Acc: 0.7555 | Val Loss: 0.4482 Acc: 0.7409\n",
      "Epoch 2/25 | Train Loss: 0.4158 Acc: 0.7883 | Val Loss: 0.4524 Acc: 0.7445\n",
      "Epoch 3/25 | Train Loss: 0.3814 Acc: 0.7956 | Val Loss: 0.4504 Acc: 0.7810\n",
      "Epoch 4/25 | Train Loss: 0.3530 Acc: 0.8321 | Val Loss: 0.5834 Acc: 0.7664\n",
      "Epoch 5/25 | Train Loss: 0.3678 Acc: 0.8139 | Val Loss: 0.5455 Acc: 0.7774\n",
      "Epoch 6/25 | Train Loss: 0.3862 Acc: 0.8321 | Val Loss: 0.4145 Acc: 0.7701\n",
      "Epoch 7/25 | Train Loss: 0.3367 Acc: 0.8102 | Val Loss: 0.4037 Acc: 0.8029\n",
      "Epoch 8/25 | Train Loss: 0.3452 Acc: 0.8540 | Val Loss: 0.3619 Acc: 0.8102\n",
      "Epoch 9/25 | Train Loss: 0.3218 Acc: 0.8394 | Val Loss: 0.3680 Acc: 0.7883\n",
      "Epoch 10/25 | Train Loss: 0.3276 Acc: 0.8467 | Val Loss: 0.3918 Acc: 0.7847\n",
      "Epoch 11/25 | Train Loss: 0.2821 Acc: 0.8650 | Val Loss: 0.3595 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.2785 Acc: 0.8686 | Val Loss: 0.3510 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.2958 Acc: 0.8759 | Val Loss: 0.3684 Acc: 0.8029\n",
      "Epoch 14/25 | Train Loss: 0.2693 Acc: 0.8905 | Val Loss: 0.3710 Acc: 0.7920\n",
      "Epoch 15/25 | Train Loss: 0.2772 Acc: 0.8686 | Val Loss: 0.3680 Acc: 0.7920\n",
      "Epoch 16/25 | Train Loss: 0.2826 Acc: 0.8869 | Val Loss: 0.3333 Acc: 0.8212\n",
      "Epoch 17/25 | Train Loss: 0.2531 Acc: 0.8723 | Val Loss: 0.3540 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.2702 Acc: 0.8759 | Val Loss: 0.3461 Acc: 0.8285\n",
      "Epoch 19/25 | Train Loss: 0.2831 Acc: 0.8504 | Val Loss: 0.3764 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.2843 Acc: 0.8686 | Val Loss: 0.3555 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.2626 Acc: 0.8942 | Val Loss: 0.3338 Acc: 0.8394\n",
      "Epoch 22/25 | Train Loss: 0.2607 Acc: 0.8869 | Val Loss: 0.3310 Acc: 0.8321\n",
      "Epoch 23/25 | Train Loss: 0.2976 Acc: 0.8577 | Val Loss: 0.3288 Acc: 0.8248\n",
      "Epoch 24/25 | Train Loss: 0.2683 Acc: 0.8650 | Val Loss: 0.3224 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.2633 Acc: 0.8759 | Val Loss: 0.3573 Acc: 0.8212\n",
      "Fold 8 Test Accuracy: 0.8248\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5400 Acc: 0.6898 | Val Loss: 0.4228 Acc: 0.7701\n",
      "Epoch 2/25 | Train Loss: 0.5173 Acc: 0.7701 | Val Loss: 0.5215 Acc: 0.7883\n",
      "Epoch 3/25 | Train Loss: 0.5479 Acc: 0.7518 | Val Loss: 0.4687 Acc: 0.8139\n",
      "Epoch 4/25 | Train Loss: 0.4255 Acc: 0.7737 | Val Loss: 0.4033 Acc: 0.7628\n",
      "Epoch 5/25 | Train Loss: 0.4900 Acc: 0.7482 | Val Loss: 0.5480 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.5169 Acc: 0.7299 | Val Loss: 0.5328 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.5508 Acc: 0.7993 | Val Loss: 0.5325 Acc: 0.6387\n",
      "Epoch 8/25 | Train Loss: 0.4309 Acc: 0.7445 | Val Loss: 0.3763 Acc: 0.7956\n",
      "Epoch 9/25 | Train Loss: 0.3574 Acc: 0.8285 | Val Loss: 0.3699 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3246 Acc: 0.8540 | Val Loss: 0.3604 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3418 Acc: 0.8285 | Val Loss: 0.3448 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3181 Acc: 0.8650 | Val Loss: 0.3747 Acc: 0.7956\n",
      "Epoch 13/25 | Train Loss: 0.3009 Acc: 0.8431 | Val Loss: 0.3715 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3074 Acc: 0.8759 | Val Loss: 0.3282 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3128 Acc: 0.8650 | Val Loss: 0.3380 Acc: 0.8285\n",
      "Epoch 16/25 | Train Loss: 0.3140 Acc: 0.8686 | Val Loss: 0.3536 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.2979 Acc: 0.8723 | Val Loss: 0.3528 Acc: 0.7956\n",
      "Epoch 18/25 | Train Loss: 0.2910 Acc: 0.8796 | Val Loss: 0.3560 Acc: 0.8467\n",
      "Epoch 19/25 | Train Loss: 0.3054 Acc: 0.8613 | Val Loss: 0.3617 Acc: 0.8066\n",
      "Epoch 20/25 | Train Loss: 0.3029 Acc: 0.8723 | Val Loss: 0.3512 Acc: 0.8431\n",
      "Epoch 21/25 | Train Loss: 0.2990 Acc: 0.8613 | Val Loss: 0.3290 Acc: 0.8431\n",
      "Epoch 22/25 | Train Loss: 0.2935 Acc: 0.8577 | Val Loss: 0.3573 Acc: 0.8139\n",
      "Epoch 23/25 | Train Loss: 0.2967 Acc: 0.8504 | Val Loss: 0.3400 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2824 Acc: 0.8905 | Val Loss: 0.3463 Acc: 0.8139\n",
      "Epoch 25/25 | Train Loss: 0.3043 Acc: 0.8577 | Val Loss: 0.3434 Acc: 0.8175\n",
      "Fold 9 Test Accuracy: 0.8431\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5525 Acc: 0.7190 | Val Loss: 0.4245 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.4325 Acc: 0.7883 | Val Loss: 0.5856 Acc: 0.7993\n",
      "Epoch 3/25 | Train Loss: 0.4503 Acc: 0.7701 | Val Loss: 0.4044 Acc: 0.7737\n",
      "Epoch 4/25 | Train Loss: 0.4250 Acc: 0.7847 | Val Loss: 0.4339 Acc: 0.8029\n",
      "Epoch 5/25 | Train Loss: 0.3976 Acc: 0.8102 | Val Loss: 0.3707 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.3626 Acc: 0.8102 | Val Loss: 0.5161 Acc: 0.7007\n",
      "Epoch 7/25 | Train Loss: 0.3709 Acc: 0.7883 | Val Loss: 0.5132 Acc: 0.7993\n",
      "Epoch 8/25 | Train Loss: 0.4229 Acc: 0.8066 | Val Loss: 0.4478 Acc: 0.7993\n",
      "Epoch 9/25 | Train Loss: 0.3639 Acc: 0.8248 | Val Loss: 0.3568 Acc: 0.8321\n",
      "Epoch 10/25 | Train Loss: 0.3321 Acc: 0.8650 | Val Loss: 0.3360 Acc: 0.8358\n",
      "Epoch 11/25 | Train Loss: 0.3386 Acc: 0.8431 | Val Loss: 0.3441 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3127 Acc: 0.8650 | Val Loss: 0.3346 Acc: 0.8431\n",
      "Epoch 13/25 | Train Loss: 0.3282 Acc: 0.8504 | Val Loss: 0.3363 Acc: 0.8358\n",
      "Epoch 14/25 | Train Loss: 0.3298 Acc: 0.8540 | Val Loss: 0.3264 Acc: 0.8358\n",
      "Epoch 15/25 | Train Loss: 0.3035 Acc: 0.8540 | Val Loss: 0.3315 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3035 Acc: 0.8613 | Val Loss: 0.3333 Acc: 0.8358\n",
      "Epoch 17/25 | Train Loss: 0.3187 Acc: 0.8504 | Val Loss: 0.3620 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.3072 Acc: 0.8467 | Val Loss: 0.3346 Acc: 0.8321\n",
      "Epoch 19/25 | Train Loss: 0.3024 Acc: 0.8796 | Val Loss: 0.3551 Acc: 0.8248\n",
      "Epoch 20/25 | Train Loss: 0.3087 Acc: 0.8504 | Val Loss: 0.3307 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3040 Acc: 0.8723 | Val Loss: 0.3337 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.3143 Acc: 0.8431 | Val Loss: 0.3389 Acc: 0.8394\n",
      "Epoch 23/25 | Train Loss: 0.3003 Acc: 0.8504 | Val Loss: 0.3246 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.3004 Acc: 0.8577 | Val Loss: 0.3384 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3038 Acc: 0.8650 | Val Loss: 0.3249 Acc: 0.8467\n",
      "Fold 10 Test Accuracy: 0.8467\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8358, Std Dev = 0.0214\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/CNNGAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T10:20:45.010596Z",
     "start_time": "2025-05-18T09:03:00.002273Z"
    }
   },
   "id": "b366861a88b8de03",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238d7a292d6b60d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/GAN'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T10:20:45.028172Z",
     "start_time": "2025-05-18T10:20:45.011603Z"
    }
   },
   "id": "23bd9e274c78db2d",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.6216 Acc: 0.6606 | Val Loss: 0.4223 Acc: 0.7883\n",
      "Epoch 2/25 | Train Loss: 0.4998 Acc: 0.7664 | Val Loss: 0.4532 Acc: 0.7920\n",
      "Epoch 3/25 | Train Loss: 0.4545 Acc: 0.7810 | Val Loss: 0.6603 Acc: 0.6058\n",
      "Epoch 4/25 | Train Loss: 0.4017 Acc: 0.7628 | Val Loss: 0.4893 Acc: 0.7701\n",
      "Epoch 5/25 | Train Loss: 0.4174 Acc: 0.7847 | Val Loss: 0.4245 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.4428 Acc: 0.7628 | Val Loss: 0.4347 Acc: 0.7993\n",
      "Epoch 7/25 | Train Loss: 0.4816 Acc: 0.8175 | Val Loss: 0.4063 Acc: 0.7482\n",
      "Epoch 8/25 | Train Loss: 0.4088 Acc: 0.7701 | Val Loss: 0.3956 Acc: 0.8066\n",
      "Epoch 9/25 | Train Loss: 0.3456 Acc: 0.8066 | Val Loss: 0.4103 Acc: 0.7920\n",
      "Epoch 10/25 | Train Loss: 0.3368 Acc: 0.8358 | Val Loss: 0.3564 Acc: 0.8175\n",
      "Epoch 11/25 | Train Loss: 0.3295 Acc: 0.8504 | Val Loss: 0.3723 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.3443 Acc: 0.8394 | Val Loss: 0.3677 Acc: 0.8066\n",
      "Epoch 13/25 | Train Loss: 0.3351 Acc: 0.8285 | Val Loss: 0.3466 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3209 Acc: 0.8650 | Val Loss: 0.3453 Acc: 0.8285\n",
      "Epoch 15/25 | Train Loss: 0.3032 Acc: 0.8650 | Val Loss: 0.3443 Acc: 0.8175\n",
      "Epoch 16/25 | Train Loss: 0.3073 Acc: 0.8650 | Val Loss: 0.3318 Acc: 0.8285\n",
      "Epoch 17/25 | Train Loss: 0.3167 Acc: 0.8504 | Val Loss: 0.3547 Acc: 0.8066\n",
      "Epoch 18/25 | Train Loss: 0.2875 Acc: 0.8613 | Val Loss: 0.3317 Acc: 0.8248\n",
      "Epoch 19/25 | Train Loss: 0.3197 Acc: 0.8504 | Val Loss: 0.3366 Acc: 0.8467\n",
      "Epoch 20/25 | Train Loss: 0.3252 Acc: 0.8248 | Val Loss: 0.3434 Acc: 0.8358\n",
      "Epoch 21/25 | Train Loss: 0.3204 Acc: 0.8467 | Val Loss: 0.3251 Acc: 0.8504\n",
      "Epoch 22/25 | Train Loss: 0.3014 Acc: 0.8540 | Val Loss: 0.3412 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.3258 Acc: 0.8285 | Val Loss: 0.3215 Acc: 0.8248\n",
      "Epoch 24/25 | Train Loss: 0.3130 Acc: 0.8394 | Val Loss: 0.3467 Acc: 0.8066\n",
      "Epoch 25/25 | Train Loss: 0.2946 Acc: 0.8467 | Val Loss: 0.3563 Acc: 0.8248\n",
      "Fold 1 Test Accuracy: 0.8212\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.5723 Acc: 0.7044 | Val Loss: 0.4023 Acc: 0.8321\n",
      "Epoch 2/25 | Train Loss: 0.4728 Acc: 0.7591 | Val Loss: 0.4399 Acc: 0.7299\n",
      "Epoch 3/25 | Train Loss: 0.3902 Acc: 0.8102 | Val Loss: 0.4214 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.3821 Acc: 0.7993 | Val Loss: 0.4171 Acc: 0.7774\n",
      "Epoch 5/25 | Train Loss: 0.3531 Acc: 0.8285 | Val Loss: 0.3707 Acc: 0.8285\n",
      "Epoch 6/25 | Train Loss: 0.3913 Acc: 0.7847 | Val Loss: 0.3726 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.3721 Acc: 0.8248 | Val Loss: 0.4128 Acc: 0.7664\n",
      "Epoch 8/25 | Train Loss: 0.3122 Acc: 0.8467 | Val Loss: 0.3166 Acc: 0.8467\n",
      "Epoch 9/25 | Train Loss: 0.2884 Acc: 0.8905 | Val Loss: 0.2999 Acc: 0.8723\n",
      "Epoch 10/25 | Train Loss: 0.3307 Acc: 0.8358 | Val Loss: 0.2853 Acc: 0.8759\n",
      "Epoch 11/25 | Train Loss: 0.3170 Acc: 0.8467 | Val Loss: 0.2642 Acc: 0.8796\n",
      "Epoch 12/25 | Train Loss: 0.2793 Acc: 0.8832 | Val Loss: 0.2808 Acc: 0.8650\n",
      "Epoch 13/25 | Train Loss: 0.2797 Acc: 0.8650 | Val Loss: 0.2769 Acc: 0.8613\n",
      "Epoch 14/25 | Train Loss: 0.2722 Acc: 0.8869 | Val Loss: 0.2947 Acc: 0.8650\n",
      "Epoch 15/25 | Train Loss: 0.2498 Acc: 0.8869 | Val Loss: 0.2658 Acc: 0.8942\n",
      "Epoch 16/25 | Train Loss: 0.2333 Acc: 0.8942 | Val Loss: 0.2708 Acc: 0.8759\n",
      "Epoch 17/25 | Train Loss: 0.2547 Acc: 0.8796 | Val Loss: 0.2659 Acc: 0.8978\n",
      "Epoch 18/25 | Train Loss: 0.2258 Acc: 0.9088 | Val Loss: 0.2785 Acc: 0.8613\n",
      "Epoch 19/25 | Train Loss: 0.3201 Acc: 0.8686 | Val Loss: 0.2591 Acc: 0.8832\n",
      "Epoch 20/25 | Train Loss: 0.2536 Acc: 0.8905 | Val Loss: 0.2595 Acc: 0.8796\n",
      "Epoch 21/25 | Train Loss: 0.2578 Acc: 0.8723 | Val Loss: 0.2436 Acc: 0.8869\n",
      "Epoch 22/25 | Train Loss: 0.2452 Acc: 0.8978 | Val Loss: 0.2504 Acc: 0.8832\n",
      "Epoch 23/25 | Train Loss: 0.2303 Acc: 0.8978 | Val Loss: 0.2658 Acc: 0.8723\n",
      "Epoch 24/25 | Train Loss: 0.2428 Acc: 0.8905 | Val Loss: 0.2347 Acc: 0.8942\n",
      "Epoch 25/25 | Train Loss: 0.2703 Acc: 0.8796 | Val Loss: 0.2821 Acc: 0.8686\n",
      "Fold 2 Test Accuracy: 0.8759\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5717 Acc: 0.7117 | Val Loss: 0.4744 Acc: 0.7920\n",
      "Epoch 2/25 | Train Loss: 0.6909 Acc: 0.7226 | Val Loss: 0.4839 Acc: 0.8029\n",
      "Epoch 3/25 | Train Loss: 0.4316 Acc: 0.7920 | Val Loss: 0.4674 Acc: 0.8066\n",
      "Epoch 4/25 | Train Loss: 0.4608 Acc: 0.7737 | Val Loss: 0.4431 Acc: 0.8139\n",
      "Epoch 5/25 | Train Loss: 0.4519 Acc: 0.7774 | Val Loss: 0.3571 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.3892 Acc: 0.8248 | Val Loss: 0.4831 Acc: 0.7263\n",
      "Epoch 7/25 | Train Loss: 0.3818 Acc: 0.8285 | Val Loss: 0.4630 Acc: 0.8248\n",
      "Epoch 8/25 | Train Loss: 0.3039 Acc: 0.8723 | Val Loss: 0.3660 Acc: 0.8212\n",
      "Epoch 9/25 | Train Loss: 0.3064 Acc: 0.8577 | Val Loss: 0.3531 Acc: 0.8029\n",
      "Epoch 10/25 | Train Loss: 0.2963 Acc: 0.8723 | Val Loss: 0.3396 Acc: 0.8321\n",
      "Epoch 11/25 | Train Loss: 0.2802 Acc: 0.8905 | Val Loss: 0.3122 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.2521 Acc: 0.8942 | Val Loss: 0.3527 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.2853 Acc: 0.8723 | Val Loss: 0.3116 Acc: 0.8467\n",
      "Epoch 14/25 | Train Loss: 0.2926 Acc: 0.8650 | Val Loss: 0.3543 Acc: 0.8358\n",
      "Epoch 15/25 | Train Loss: 0.2750 Acc: 0.8796 | Val Loss: 0.3185 Acc: 0.8285\n",
      "Epoch 16/25 | Train Loss: 0.2587 Acc: 0.8978 | Val Loss: 0.3465 Acc: 0.8467\n",
      "Epoch 17/25 | Train Loss: 0.2573 Acc: 0.8832 | Val Loss: 0.3010 Acc: 0.8467\n",
      "Epoch 18/25 | Train Loss: 0.2433 Acc: 0.9088 | Val Loss: 0.3033 Acc: 0.8358\n",
      "Epoch 19/25 | Train Loss: 0.2776 Acc: 0.8796 | Val Loss: 0.2879 Acc: 0.8613\n",
      "Epoch 20/25 | Train Loss: 0.2371 Acc: 0.9015 | Val Loss: 0.3147 Acc: 0.8467\n",
      "Epoch 21/25 | Train Loss: 0.2913 Acc: 0.8832 | Val Loss: 0.3120 Acc: 0.8577\n",
      "Epoch 22/25 | Train Loss: 0.2208 Acc: 0.9051 | Val Loss: 0.3171 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.2777 Acc: 0.8686 | Val Loss: 0.2867 Acc: 0.8759\n",
      "Epoch 24/25 | Train Loss: 0.2693 Acc: 0.8686 | Val Loss: 0.3283 Acc: 0.8431\n",
      "Epoch 25/25 | Train Loss: 0.2737 Acc: 0.8540 | Val Loss: 0.3057 Acc: 0.8358\n",
      "Fold 3 Test Accuracy: 0.8358\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5062 Acc: 0.7336 | Val Loss: 0.4570 Acc: 0.7883\n",
      "Epoch 2/25 | Train Loss: 0.4642 Acc: 0.7774 | Val Loss: 0.4503 Acc: 0.7117\n",
      "Epoch 3/25 | Train Loss: 0.5202 Acc: 0.7591 | Val Loss: 0.4095 Acc: 0.7883\n",
      "Epoch 4/25 | Train Loss: 0.5943 Acc: 0.7920 | Val Loss: 0.8095 Acc: 0.6204\n",
      "Epoch 5/25 | Train Loss: 0.5683 Acc: 0.7847 | Val Loss: 0.4244 Acc: 0.7372\n",
      "Epoch 6/25 | Train Loss: 0.3870 Acc: 0.7810 | Val Loss: 0.3570 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.3405 Acc: 0.8212 | Val Loss: 0.4139 Acc: 0.8175\n",
      "Epoch 8/25 | Train Loss: 0.3482 Acc: 0.8650 | Val Loss: 0.3329 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3356 Acc: 0.8285 | Val Loss: 0.3640 Acc: 0.8066\n",
      "Epoch 10/25 | Train Loss: 0.3197 Acc: 0.8504 | Val Loss: 0.3499 Acc: 0.8139\n",
      "Epoch 11/25 | Train Loss: 0.3205 Acc: 0.8394 | Val Loss: 0.3496 Acc: 0.8212\n",
      "Epoch 12/25 | Train Loss: 0.3490 Acc: 0.8467 | Val Loss: 0.3442 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.2955 Acc: 0.8467 | Val Loss: 0.3152 Acc: 0.8504\n",
      "Epoch 14/25 | Train Loss: 0.3048 Acc: 0.8504 | Val Loss: 0.3347 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3110 Acc: 0.8540 | Val Loss: 0.3549 Acc: 0.8394\n",
      "Epoch 16/25 | Train Loss: 0.2715 Acc: 0.8759 | Val Loss: 0.3212 Acc: 0.8467\n",
      "Epoch 17/25 | Train Loss: 0.2889 Acc: 0.8650 | Val Loss: 0.3384 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3184 Acc: 0.8285 | Val Loss: 0.3327 Acc: 0.8212\n",
      "Epoch 19/25 | Train Loss: 0.3029 Acc: 0.8613 | Val Loss: 0.3218 Acc: 0.8394\n",
      "Epoch 20/25 | Train Loss: 0.3028 Acc: 0.8504 | Val Loss: 0.3137 Acc: 0.8504\n",
      "Epoch 21/25 | Train Loss: 0.2899 Acc: 0.8723 | Val Loss: 0.3425 Acc: 0.8212\n",
      "Epoch 22/25 | Train Loss: 0.3028 Acc: 0.8467 | Val Loss: 0.3180 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.2964 Acc: 0.8686 | Val Loss: 0.3254 Acc: 0.8321\n",
      "Epoch 24/25 | Train Loss: 0.3225 Acc: 0.8504 | Val Loss: 0.3193 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3097 Acc: 0.8504 | Val Loss: 0.3194 Acc: 0.8248\n",
      "Fold 4 Test Accuracy: 0.8431\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.4624 Acc: 0.7628 | Val Loss: 0.5097 Acc: 0.8029\n",
      "Epoch 2/25 | Train Loss: 0.4678 Acc: 0.7518 | Val Loss: 0.4316 Acc: 0.7956\n",
      "Epoch 3/25 | Train Loss: 0.4606 Acc: 0.7591 | Val Loss: 0.3931 Acc: 0.7956\n",
      "Epoch 4/25 | Train Loss: 0.5001 Acc: 0.7555 | Val Loss: 0.3882 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4260 Acc: 0.7920 | Val Loss: 0.4336 Acc: 0.7299\n",
      "Epoch 6/25 | Train Loss: 0.4345 Acc: 0.7445 | Val Loss: 0.3640 Acc: 0.8175\n",
      "Epoch 7/25 | Train Loss: 0.3915 Acc: 0.7774 | Val Loss: 0.3654 Acc: 0.7847\n",
      "Epoch 8/25 | Train Loss: 0.3388 Acc: 0.8394 | Val Loss: 0.3387 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3102 Acc: 0.8650 | Val Loss: 0.3462 Acc: 0.8139\n",
      "Epoch 10/25 | Train Loss: 0.3295 Acc: 0.8504 | Val Loss: 0.3507 Acc: 0.8175\n",
      "Epoch 11/25 | Train Loss: 0.2913 Acc: 0.8577 | Val Loss: 0.3291 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.2956 Acc: 0.8686 | Val Loss: 0.3253 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.2822 Acc: 0.8832 | Val Loss: 0.3354 Acc: 0.8066\n",
      "Epoch 14/25 | Train Loss: 0.3514 Acc: 0.8029 | Val Loss: 0.2968 Acc: 0.8504\n",
      "Epoch 15/25 | Train Loss: 0.2824 Acc: 0.8759 | Val Loss: 0.2991 Acc: 0.8686\n",
      "Epoch 16/25 | Train Loss: 0.3194 Acc: 0.8358 | Val Loss: 0.3329 Acc: 0.8212\n",
      "Epoch 17/25 | Train Loss: 0.3164 Acc: 0.8285 | Val Loss: 0.3430 Acc: 0.8431\n",
      "Epoch 18/25 | Train Loss: 0.3000 Acc: 0.8540 | Val Loss: 0.3003 Acc: 0.8504\n",
      "Epoch 19/25 | Train Loss: 0.3137 Acc: 0.8394 | Val Loss: 0.3517 Acc: 0.8285\n",
      "Epoch 20/25 | Train Loss: 0.2599 Acc: 0.8905 | Val Loss: 0.3082 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.2814 Acc: 0.8723 | Val Loss: 0.3143 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.3134 Acc: 0.8394 | Val Loss: 0.3007 Acc: 0.8577\n",
      "Epoch 23/25 | Train Loss: 0.2726 Acc: 0.8686 | Val Loss: 0.3263 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.2896 Acc: 0.8613 | Val Loss: 0.3363 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3008 Acc: 0.8394 | Val Loss: 0.3192 Acc: 0.8613\n",
      "Fold 5 Test Accuracy: 0.8431\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5485 Acc: 0.6715 | Val Loss: 0.5045 Acc: 0.7774\n",
      "Epoch 2/25 | Train Loss: 0.5215 Acc: 0.7482 | Val Loss: 0.5120 Acc: 0.8066\n",
      "Epoch 3/25 | Train Loss: 0.4408 Acc: 0.7956 | Val Loss: 0.6734 Acc: 0.7993\n",
      "Epoch 4/25 | Train Loss: 0.4626 Acc: 0.7737 | Val Loss: 0.4107 Acc: 0.7774\n",
      "Epoch 5/25 | Train Loss: 0.4135 Acc: 0.7883 | Val Loss: 0.3669 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.4081 Acc: 0.7956 | Val Loss: 0.3948 Acc: 0.7883\n",
      "Epoch 7/25 | Train Loss: 0.3820 Acc: 0.8066 | Val Loss: 0.3534 Acc: 0.8248\n",
      "Epoch 8/25 | Train Loss: 0.3602 Acc: 0.8248 | Val Loss: 0.3648 Acc: 0.8321\n",
      "Epoch 9/25 | Train Loss: 0.3208 Acc: 0.8358 | Val Loss: 0.3709 Acc: 0.7810\n",
      "Epoch 10/25 | Train Loss: 0.3077 Acc: 0.8504 | Val Loss: 0.3782 Acc: 0.7920\n",
      "Epoch 11/25 | Train Loss: 0.3451 Acc: 0.8285 | Val Loss: 0.3688 Acc: 0.7883\n",
      "Epoch 12/25 | Train Loss: 0.3245 Acc: 0.8358 | Val Loss: 0.4083 Acc: 0.7993\n",
      "Epoch 13/25 | Train Loss: 0.3385 Acc: 0.8248 | Val Loss: 0.4230 Acc: 0.7883\n",
      "Epoch 14/25 | Train Loss: 0.3546 Acc: 0.8175 | Val Loss: 0.3556 Acc: 0.8285\n",
      "Epoch 15/25 | Train Loss: 0.3010 Acc: 0.8723 | Val Loss: 0.3471 Acc: 0.8139\n",
      "Epoch 16/25 | Train Loss: 0.2887 Acc: 0.8942 | Val Loss: 0.3474 Acc: 0.8504\n",
      "Epoch 17/25 | Train Loss: 0.3284 Acc: 0.8321 | Val Loss: 0.3593 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3064 Acc: 0.8650 | Val Loss: 0.3262 Acc: 0.8139\n",
      "Epoch 19/25 | Train Loss: 0.2856 Acc: 0.8613 | Val Loss: 0.3465 Acc: 0.8102\n",
      "Epoch 20/25 | Train Loss: 0.3093 Acc: 0.8467 | Val Loss: 0.3634 Acc: 0.8066\n",
      "Epoch 21/25 | Train Loss: 0.3191 Acc: 0.8358 | Val Loss: 0.3148 Acc: 0.8431\n",
      "Epoch 22/25 | Train Loss: 0.2960 Acc: 0.8759 | Val Loss: 0.3315 Acc: 0.8394\n",
      "Epoch 23/25 | Train Loss: 0.2941 Acc: 0.8832 | Val Loss: 0.3377 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.2780 Acc: 0.8650 | Val Loss: 0.3243 Acc: 0.8467\n",
      "Epoch 25/25 | Train Loss: 0.3058 Acc: 0.8504 | Val Loss: 0.3361 Acc: 0.8358\n",
      "Fold 6 Test Accuracy: 0.8321\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.7050 Acc: 0.6533 | Val Loss: 0.3771 Acc: 0.7883\n",
      "Epoch 2/25 | Train Loss: 0.5136 Acc: 0.7263 | Val Loss: 0.4018 Acc: 0.7993\n",
      "Epoch 3/25 | Train Loss: 0.4935 Acc: 0.7591 | Val Loss: 0.4226 Acc: 0.8175\n",
      "Epoch 4/25 | Train Loss: 0.5373 Acc: 0.7737 | Val Loss: 1.1369 Acc: 0.6168\n",
      "Epoch 5/25 | Train Loss: 0.5150 Acc: 0.7080 | Val Loss: 0.3247 Acc: 0.8467\n",
      "Epoch 6/25 | Train Loss: 0.4101 Acc: 0.7664 | Val Loss: 0.3053 Acc: 0.8540\n",
      "Epoch 7/25 | Train Loss: 0.3377 Acc: 0.8285 | Val Loss: 0.2987 Acc: 0.8540\n",
      "Epoch 8/25 | Train Loss: 0.3638 Acc: 0.8248 | Val Loss: 0.2611 Acc: 0.9051\n",
      "Epoch 9/25 | Train Loss: 0.3358 Acc: 0.8504 | Val Loss: 0.2992 Acc: 0.8723\n",
      "Epoch 10/25 | Train Loss: 0.2956 Acc: 0.8577 | Val Loss: 0.3189 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3192 Acc: 0.8467 | Val Loss: 0.2457 Acc: 0.9088\n",
      "Epoch 12/25 | Train Loss: 0.2759 Acc: 0.8759 | Val Loss: 0.2628 Acc: 0.8869\n",
      "Epoch 13/25 | Train Loss: 0.2854 Acc: 0.8577 | Val Loss: 0.2720 Acc: 0.8650\n",
      "Epoch 14/25 | Train Loss: 0.2725 Acc: 0.8905 | Val Loss: 0.3137 Acc: 0.8431\n",
      "Epoch 15/25 | Train Loss: 0.2960 Acc: 0.8540 | Val Loss: 0.2876 Acc: 0.8394\n",
      "Epoch 16/25 | Train Loss: 0.2523 Acc: 0.8978 | Val Loss: 0.2932 Acc: 0.8431\n",
      "Epoch 17/25 | Train Loss: 0.2787 Acc: 0.8723 | Val Loss: 0.2379 Acc: 0.8905\n",
      "Epoch 18/25 | Train Loss: 0.2692 Acc: 0.8723 | Val Loss: 0.2471 Acc: 0.8978\n",
      "Epoch 19/25 | Train Loss: 0.2652 Acc: 0.8796 | Val Loss: 0.2696 Acc: 0.8723\n",
      "Epoch 20/25 | Train Loss: 0.2630 Acc: 0.8905 | Val Loss: 0.2843 Acc: 0.8577\n",
      "Epoch 21/25 | Train Loss: 0.2573 Acc: 0.8942 | Val Loss: 0.2403 Acc: 0.9088\n",
      "Epoch 22/25 | Train Loss: 0.2906 Acc: 0.8577 | Val Loss: 0.2457 Acc: 0.8723\n",
      "Epoch 23/25 | Train Loss: 0.2876 Acc: 0.8942 | Val Loss: 0.2690 Acc: 0.8723\n",
      "Epoch 24/25 | Train Loss: 0.2606 Acc: 0.8905 | Val Loss: 0.2568 Acc: 0.8905\n",
      "Epoch 25/25 | Train Loss: 0.2769 Acc: 0.8832 | Val Loss: 0.2442 Acc: 0.8832\n",
      "Fold 7 Test Accuracy: 0.8832\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.5392 Acc: 0.7336 | Val Loss: 0.7887 Acc: 0.7664\n",
      "Epoch 2/25 | Train Loss: 0.4627 Acc: 0.7956 | Val Loss: 0.4968 Acc: 0.7701\n",
      "Epoch 3/25 | Train Loss: 0.4747 Acc: 0.7664 | Val Loss: 0.4696 Acc: 0.7774\n",
      "Epoch 4/25 | Train Loss: 0.4154 Acc: 0.7774 | Val Loss: 0.4354 Acc: 0.7847\n",
      "Epoch 5/25 | Train Loss: 0.4127 Acc: 0.8066 | Val Loss: 0.4671 Acc: 0.6861\n",
      "Epoch 6/25 | Train Loss: 0.3667 Acc: 0.8175 | Val Loss: 0.6114 Acc: 0.7664\n",
      "Epoch 7/25 | Train Loss: 0.4800 Acc: 0.7737 | Val Loss: 0.4593 Acc: 0.7445\n",
      "Epoch 8/25 | Train Loss: 0.4285 Acc: 0.7847 | Val Loss: 0.4361 Acc: 0.7409\n",
      "Epoch 9/25 | Train Loss: 0.3396 Acc: 0.8285 | Val Loss: 0.4233 Acc: 0.7628\n",
      "Epoch 10/25 | Train Loss: 0.3504 Acc: 0.8248 | Val Loss: 0.4290 Acc: 0.7737\n",
      "Epoch 11/25 | Train Loss: 0.3335 Acc: 0.8504 | Val Loss: 0.4143 Acc: 0.7591\n",
      "Epoch 12/25 | Train Loss: 0.3699 Acc: 0.8285 | Val Loss: 0.3930 Acc: 0.7810\n",
      "Epoch 13/25 | Train Loss: 0.3459 Acc: 0.8212 | Val Loss: 0.4285 Acc: 0.7628\n",
      "Epoch 14/25 | Train Loss: 0.3557 Acc: 0.8321 | Val Loss: 0.4403 Acc: 0.7664\n",
      "Epoch 15/25 | Train Loss: 0.3394 Acc: 0.8358 | Val Loss: 0.4408 Acc: 0.7701\n",
      "Epoch 16/25 | Train Loss: 0.3581 Acc: 0.8358 | Val Loss: 0.4326 Acc: 0.7664\n",
      "Epoch 17/25 | Train Loss: 0.3382 Acc: 0.8285 | Val Loss: 0.4389 Acc: 0.7628\n",
      "Epoch 18/25 | Train Loss: 0.3278 Acc: 0.8394 | Val Loss: 0.4337 Acc: 0.7628\n",
      "Epoch 19/25 | Train Loss: 0.3173 Acc: 0.8394 | Val Loss: 0.4152 Acc: 0.7701\n",
      "Epoch 20/25 | Train Loss: 0.3296 Acc: 0.8358 | Val Loss: 0.4305 Acc: 0.7701\n",
      "Epoch 21/25 | Train Loss: 0.3165 Acc: 0.8431 | Val Loss: 0.4241 Acc: 0.7701\n",
      "Epoch 22/25 | Train Loss: 0.3305 Acc: 0.8394 | Val Loss: 0.4355 Acc: 0.7628\n",
      "Epoch 23/25 | Train Loss: 0.3217 Acc: 0.8358 | Val Loss: 0.4348 Acc: 0.7664\n",
      "Epoch 24/25 | Train Loss: 0.3228 Acc: 0.8431 | Val Loss: 0.4213 Acc: 0.7664\n",
      "Epoch 25/25 | Train Loss: 0.3367 Acc: 0.8431 | Val Loss: 0.4224 Acc: 0.7701\n",
      "Fold 8 Test Accuracy: 0.7847\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5525 Acc: 0.7153 | Val Loss: 0.4829 Acc: 0.6606\n",
      "Epoch 2/25 | Train Loss: 0.5268 Acc: 0.7336 | Val Loss: 1.0551 Acc: 0.5985\n",
      "Epoch 3/25 | Train Loss: 0.5101 Acc: 0.7555 | Val Loss: 0.9678 Acc: 0.5985\n",
      "Epoch 4/25 | Train Loss: 0.4480 Acc: 0.7810 | Val Loss: 0.4050 Acc: 0.7263\n",
      "Epoch 5/25 | Train Loss: 0.5013 Acc: 0.7810 | Val Loss: 0.3807 Acc: 0.7810\n",
      "Epoch 6/25 | Train Loss: 0.3826 Acc: 0.8175 | Val Loss: 0.4413 Acc: 0.8248\n",
      "Epoch 7/25 | Train Loss: 0.3239 Acc: 0.8358 | Val Loss: 0.4189 Acc: 0.7336\n",
      "Epoch 8/25 | Train Loss: 0.2900 Acc: 0.8686 | Val Loss: 0.3684 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.2975 Acc: 0.8723 | Val Loss: 0.3330 Acc: 0.8431\n",
      "Epoch 10/25 | Train Loss: 0.2662 Acc: 0.8650 | Val Loss: 0.3921 Acc: 0.8175\n",
      "Epoch 11/25 | Train Loss: 0.2557 Acc: 0.9015 | Val Loss: 0.3588 Acc: 0.8394\n",
      "Epoch 12/25 | Train Loss: 0.2852 Acc: 0.8650 | Val Loss: 0.3552 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.2772 Acc: 0.8978 | Val Loss: 0.3698 Acc: 0.8212\n",
      "Epoch 14/25 | Train Loss: 0.2820 Acc: 0.8796 | Val Loss: 0.3314 Acc: 0.8431\n",
      "Epoch 15/25 | Train Loss: 0.2794 Acc: 0.8905 | Val Loss: 0.3281 Acc: 0.8358\n",
      "Epoch 16/25 | Train Loss: 0.2426 Acc: 0.9088 | Val Loss: 0.3307 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.2875 Acc: 0.8796 | Val Loss: 0.3267 Acc: 0.8358\n",
      "Epoch 18/25 | Train Loss: 0.2346 Acc: 0.9015 | Val Loss: 0.3084 Acc: 0.8394\n",
      "Epoch 19/25 | Train Loss: 0.2634 Acc: 0.8942 | Val Loss: 0.3842 Acc: 0.8212\n",
      "Epoch 20/25 | Train Loss: 0.2673 Acc: 0.8686 | Val Loss: 0.3273 Acc: 0.8248\n",
      "Epoch 21/25 | Train Loss: 0.2494 Acc: 0.8942 | Val Loss: 0.3713 Acc: 0.8358\n",
      "Epoch 22/25 | Train Loss: 0.2424 Acc: 0.9015 | Val Loss: 0.3111 Acc: 0.8686\n",
      "Epoch 23/25 | Train Loss: 0.2599 Acc: 0.8759 | Val Loss: 0.3264 Acc: 0.8504\n",
      "Epoch 24/25 | Train Loss: 0.2599 Acc: 0.8942 | Val Loss: 0.3555 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.2609 Acc: 0.8759 | Val Loss: 0.3226 Acc: 0.8540\n",
      "Fold 9 Test Accuracy: 0.8139\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.4958 Acc: 0.7117 | Val Loss: 0.4116 Acc: 0.8102\n",
      "Epoch 2/25 | Train Loss: 0.3867 Acc: 0.7993 | Val Loss: 0.5392 Acc: 0.6606\n",
      "Epoch 3/25 | Train Loss: 0.4273 Acc: 0.7737 | Val Loss: 0.4472 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4640 Acc: 0.7883 | Val Loss: 0.3777 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4250 Acc: 0.7701 | Val Loss: 0.3884 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.3643 Acc: 0.8175 | Val Loss: 0.4586 Acc: 0.7810\n",
      "Epoch 7/25 | Train Loss: 0.3905 Acc: 0.7993 | Val Loss: 0.3626 Acc: 0.8029\n",
      "Epoch 8/25 | Train Loss: 0.3848 Acc: 0.7555 | Val Loss: 0.3694 Acc: 0.7810\n",
      "Epoch 9/25 | Train Loss: 0.3213 Acc: 0.8358 | Val Loss: 0.3534 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3401 Acc: 0.8394 | Val Loss: 0.3750 Acc: 0.8066\n",
      "Epoch 11/25 | Train Loss: 0.3343 Acc: 0.8394 | Val Loss: 0.3692 Acc: 0.8212\n",
      "Epoch 12/25 | Train Loss: 0.3191 Acc: 0.8431 | Val Loss: 0.3488 Acc: 0.8248\n",
      "Epoch 13/25 | Train Loss: 0.3355 Acc: 0.8467 | Val Loss: 0.3424 Acc: 0.8139\n",
      "Epoch 14/25 | Train Loss: 0.3156 Acc: 0.8321 | Val Loss: 0.3688 Acc: 0.7993\n",
      "Epoch 15/25 | Train Loss: 0.3118 Acc: 0.8613 | Val Loss: 0.3766 Acc: 0.7883\n",
      "Epoch 16/25 | Train Loss: 0.3297 Acc: 0.8467 | Val Loss: 0.3518 Acc: 0.8321\n",
      "Epoch 17/25 | Train Loss: 0.3185 Acc: 0.8650 | Val Loss: 0.3231 Acc: 0.8431\n",
      "Epoch 18/25 | Train Loss: 0.3199 Acc: 0.8467 | Val Loss: 0.3433 Acc: 0.8321\n",
      "Epoch 19/25 | Train Loss: 0.3247 Acc: 0.8321 | Val Loss: 0.3427 Acc: 0.8431\n",
      "Epoch 20/25 | Train Loss: 0.3369 Acc: 0.8358 | Val Loss: 0.3389 Acc: 0.8248\n",
      "Epoch 21/25 | Train Loss: 0.3140 Acc: 0.8686 | Val Loss: 0.3428 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3165 Acc: 0.8285 | Val Loss: 0.3541 Acc: 0.8212\n",
      "Epoch 23/25 | Train Loss: 0.3253 Acc: 0.8431 | Val Loss: 0.3364 Acc: 0.8394\n",
      "Epoch 24/25 | Train Loss: 0.3311 Acc: 0.8394 | Val Loss: 0.3306 Acc: 0.8248\n",
      "Epoch 25/25 | Train Loss: 0.3262 Acc: 0.8394 | Val Loss: 0.3507 Acc: 0.8321\n",
      "Fold 10 Test Accuracy: 0.8102\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8343, Std Dev = 0.0297\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/GAN/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/GAN/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T11:39:19.275513Z",
     "start_time": "2025-05-18T10:20:45.030181Z"
    }
   },
   "id": "8a62703cd072ecfe",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CNNVAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f700fbe891b185be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/CNNVAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T16:22:26.804131Z",
     "start_time": "2025-05-18T16:22:26.790843Z"
    }
   },
   "id": "314d5ad019d8954c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.5229 Acc: 0.7226 | Val Loss: 0.4320 Acc: 0.7774\n",
      "Epoch 2/25 | Train Loss: 0.4052 Acc: 0.7664 | Val Loss: 0.6136 Acc: 0.6496\n",
      "Epoch 3/25 | Train Loss: 0.4967 Acc: 0.7482 | Val Loss: 0.4206 Acc: 0.7956\n",
      "Epoch 4/25 | Train Loss: 0.4338 Acc: 0.8066 | Val Loss: 0.4717 Acc: 0.7628\n",
      "Epoch 5/25 | Train Loss: 0.5014 Acc: 0.7664 | Val Loss: 0.4394 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.3703 Acc: 0.8321 | Val Loss: 0.4600 Acc: 0.7117\n",
      "Epoch 7/25 | Train Loss: 0.3997 Acc: 0.7956 | Val Loss: 0.4127 Acc: 0.7664\n",
      "Epoch 8/25 | Train Loss: 0.3319 Acc: 0.8577 | Val Loss: 0.4073 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3398 Acc: 0.8248 | Val Loss: 0.3912 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3524 Acc: 0.8285 | Val Loss: 0.3782 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3281 Acc: 0.8686 | Val Loss: 0.3679 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3496 Acc: 0.8248 | Val Loss: 0.3618 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.3210 Acc: 0.8504 | Val Loss: 0.3708 Acc: 0.8139\n",
      "Epoch 14/25 | Train Loss: 0.3331 Acc: 0.8431 | Val Loss: 0.3843 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.3232 Acc: 0.8358 | Val Loss: 0.3639 Acc: 0.8066\n",
      "Epoch 16/25 | Train Loss: 0.3336 Acc: 0.8394 | Val Loss: 0.3417 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3415 Acc: 0.8321 | Val Loss: 0.3772 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3103 Acc: 0.8540 | Val Loss: 0.3849 Acc: 0.8102\n",
      "Epoch 19/25 | Train Loss: 0.3421 Acc: 0.8285 | Val Loss: 0.3614 Acc: 0.8175\n",
      "Epoch 20/25 | Train Loss: 0.2997 Acc: 0.8650 | Val Loss: 0.3573 Acc: 0.8066\n",
      "Epoch 21/25 | Train Loss: 0.3119 Acc: 0.8577 | Val Loss: 0.3666 Acc: 0.8029\n",
      "Epoch 22/25 | Train Loss: 0.3321 Acc: 0.8285 | Val Loss: 0.3591 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3584 Acc: 0.8577 | Val Loss: 0.3413 Acc: 0.8248\n",
      "Epoch 24/25 | Train Loss: 0.3465 Acc: 0.8431 | Val Loss: 0.3826 Acc: 0.7993\n",
      "Epoch 25/25 | Train Loss: 0.3201 Acc: 0.8467 | Val Loss: 0.3510 Acc: 0.8175\n",
      "Fold 1 Test Accuracy: 0.8212\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.5238 Acc: 0.7226 | Val Loss: 0.4456 Acc: 0.8139\n",
      "Epoch 2/25 | Train Loss: 0.4925 Acc: 0.7518 | Val Loss: 0.5085 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4618 Acc: 0.7774 | Val Loss: 0.3852 Acc: 0.8212\n",
      "Epoch 4/25 | Train Loss: 0.4341 Acc: 0.7555 | Val Loss: 0.3645 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4432 Acc: 0.7956 | Val Loss: 0.3751 Acc: 0.8102\n",
      "Epoch 6/25 | Train Loss: 0.4215 Acc: 0.7883 | Val Loss: 0.4046 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.4396 Acc: 0.7336 | Val Loss: 0.3377 Acc: 0.8285\n",
      "Epoch 8/25 | Train Loss: 0.3428 Acc: 0.8285 | Val Loss: 0.3281 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3482 Acc: 0.7920 | Val Loss: 0.3308 Acc: 0.8248\n",
      "Epoch 10/25 | Train Loss: 0.3337 Acc: 0.8321 | Val Loss: 0.3374 Acc: 0.8394\n",
      "Epoch 11/25 | Train Loss: 0.3166 Acc: 0.8321 | Val Loss: 0.3256 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.3194 Acc: 0.8431 | Val Loss: 0.3058 Acc: 0.8577\n",
      "Epoch 13/25 | Train Loss: 0.3286 Acc: 0.8285 | Val Loss: 0.3201 Acc: 0.8467\n",
      "Epoch 14/25 | Train Loss: 0.3192 Acc: 0.8431 | Val Loss: 0.3378 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3043 Acc: 0.8540 | Val Loss: 0.3187 Acc: 0.8321\n",
      "Epoch 16/25 | Train Loss: 0.3062 Acc: 0.8650 | Val Loss: 0.2980 Acc: 0.8723\n",
      "Epoch 17/25 | Train Loss: 0.2968 Acc: 0.8577 | Val Loss: 0.3127 Acc: 0.8358\n",
      "Epoch 18/25 | Train Loss: 0.3132 Acc: 0.8467 | Val Loss: 0.3141 Acc: 0.8358\n",
      "Epoch 19/25 | Train Loss: 0.3104 Acc: 0.8613 | Val Loss: 0.3100 Acc: 0.8540\n",
      "Epoch 20/25 | Train Loss: 0.3088 Acc: 0.8431 | Val Loss: 0.2977 Acc: 0.8650\n",
      "Epoch 21/25 | Train Loss: 0.2926 Acc: 0.8431 | Val Loss: 0.3205 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3256 Acc: 0.8650 | Val Loss: 0.3163 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.2960 Acc: 0.8759 | Val Loss: 0.2886 Acc: 0.8759\n",
      "Epoch 24/25 | Train Loss: 0.2857 Acc: 0.8869 | Val Loss: 0.3299 Acc: 0.8504\n",
      "Epoch 25/25 | Train Loss: 0.3241 Acc: 0.8394 | Val Loss: 0.3024 Acc: 0.8577\n",
      "Fold 2 Test Accuracy: 0.8540\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5157 Acc: 0.7409 | Val Loss: 0.7756 Acc: 0.6204\n",
      "Epoch 2/25 | Train Loss: 0.4508 Acc: 0.7628 | Val Loss: 0.4272 Acc: 0.7920\n",
      "Epoch 3/25 | Train Loss: 0.4949 Acc: 0.7810 | Val Loss: 0.4345 Acc: 0.7847\n",
      "Epoch 4/25 | Train Loss: 0.5009 Acc: 0.7226 | Val Loss: 0.5409 Acc: 0.8066\n",
      "Epoch 5/25 | Train Loss: 0.4699 Acc: 0.7810 | Val Loss: 0.3786 Acc: 0.7847\n",
      "Epoch 6/25 | Train Loss: 0.3717 Acc: 0.8102 | Val Loss: 0.3572 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.3755 Acc: 0.7920 | Val Loss: 0.3737 Acc: 0.7956\n",
      "Epoch 8/25 | Train Loss: 0.3611 Acc: 0.8102 | Val Loss: 0.3658 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.3450 Acc: 0.8394 | Val Loss: 0.3497 Acc: 0.8175\n",
      "Epoch 10/25 | Train Loss: 0.3554 Acc: 0.8029 | Val Loss: 0.3419 Acc: 0.8321\n",
      "Epoch 11/25 | Train Loss: 0.3387 Acc: 0.8431 | Val Loss: 0.3770 Acc: 0.8285\n",
      "Epoch 12/25 | Train Loss: 0.3320 Acc: 0.8248 | Val Loss: 0.3479 Acc: 0.8175\n",
      "Epoch 13/25 | Train Loss: 0.3197 Acc: 0.8540 | Val Loss: 0.3379 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3414 Acc: 0.8248 | Val Loss: 0.3372 Acc: 0.8431\n",
      "Epoch 15/25 | Train Loss: 0.3229 Acc: 0.8358 | Val Loss: 0.3581 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.3058 Acc: 0.8650 | Val Loss: 0.3312 Acc: 0.8431\n",
      "Epoch 17/25 | Train Loss: 0.3083 Acc: 0.8504 | Val Loss: 0.3457 Acc: 0.8212\n",
      "Epoch 18/25 | Train Loss: 0.3200 Acc: 0.8285 | Val Loss: 0.3370 Acc: 0.8431\n",
      "Epoch 19/25 | Train Loss: 0.3254 Acc: 0.8504 | Val Loss: 0.3443 Acc: 0.8248\n",
      "Epoch 20/25 | Train Loss: 0.3490 Acc: 0.8248 | Val Loss: 0.3361 Acc: 0.8467\n",
      "Epoch 21/25 | Train Loss: 0.3389 Acc: 0.8358 | Val Loss: 0.3400 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3287 Acc: 0.8394 | Val Loss: 0.3273 Acc: 0.8467\n",
      "Epoch 23/25 | Train Loss: 0.3356 Acc: 0.8248 | Val Loss: 0.3433 Acc: 0.8285\n",
      "Epoch 24/25 | Train Loss: 0.3419 Acc: 0.8212 | Val Loss: 0.3441 Acc: 0.8394\n",
      "Epoch 25/25 | Train Loss: 0.3011 Acc: 0.8577 | Val Loss: 0.3243 Acc: 0.8248\n",
      "Fold 3 Test Accuracy: 0.8212\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5115 Acc: 0.7080 | Val Loss: 0.5320 Acc: 0.6387\n",
      "Epoch 2/25 | Train Loss: 0.4424 Acc: 0.7737 | Val Loss: 0.4811 Acc: 0.7117\n",
      "Epoch 3/25 | Train Loss: 0.4079 Acc: 0.7993 | Val Loss: 0.4420 Acc: 0.7409\n",
      "Epoch 4/25 | Train Loss: 0.4349 Acc: 0.7993 | Val Loss: 0.5259 Acc: 0.6934\n",
      "Epoch 5/25 | Train Loss: 0.4259 Acc: 0.7810 | Val Loss: 0.4306 Acc: 0.7555\n",
      "Epoch 6/25 | Train Loss: 0.4826 Acc: 0.7409 | Val Loss: 0.3876 Acc: 0.8139\n",
      "Epoch 7/25 | Train Loss: 0.4065 Acc: 0.7883 | Val Loss: 0.4090 Acc: 0.7591\n",
      "Epoch 8/25 | Train Loss: 0.3746 Acc: 0.8102 | Val Loss: 0.3880 Acc: 0.7883\n",
      "Epoch 9/25 | Train Loss: 0.3922 Acc: 0.7920 | Val Loss: 0.3825 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3838 Acc: 0.8029 | Val Loss: 0.3772 Acc: 0.8102\n",
      "Epoch 11/25 | Train Loss: 0.3715 Acc: 0.8139 | Val Loss: 0.3589 Acc: 0.8175\n",
      "Epoch 12/25 | Train Loss: 0.3597 Acc: 0.8212 | Val Loss: 0.3701 Acc: 0.8066\n",
      "Epoch 13/25 | Train Loss: 0.3575 Acc: 0.8285 | Val Loss: 0.3748 Acc: 0.7956\n",
      "Epoch 14/25 | Train Loss: 0.3476 Acc: 0.8139 | Val Loss: 0.3675 Acc: 0.8102\n",
      "Epoch 15/25 | Train Loss: 0.3508 Acc: 0.8394 | Val Loss: 0.3650 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.3557 Acc: 0.8248 | Val Loss: 0.3722 Acc: 0.8029\n",
      "Epoch 17/25 | Train Loss: 0.3563 Acc: 0.8321 | Val Loss: 0.3660 Acc: 0.8175\n",
      "Epoch 18/25 | Train Loss: 0.3506 Acc: 0.8540 | Val Loss: 0.3433 Acc: 0.8139\n",
      "Epoch 19/25 | Train Loss: 0.3631 Acc: 0.8066 | Val Loss: 0.3795 Acc: 0.8029\n",
      "Epoch 20/25 | Train Loss: 0.3527 Acc: 0.8285 | Val Loss: 0.3472 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3671 Acc: 0.8175 | Val Loss: 0.3713 Acc: 0.8066\n",
      "Epoch 22/25 | Train Loss: 0.3842 Acc: 0.8066 | Val Loss: 0.3572 Acc: 0.8285\n",
      "Epoch 23/25 | Train Loss: 0.3510 Acc: 0.8321 | Val Loss: 0.3411 Acc: 0.8066\n",
      "Epoch 24/25 | Train Loss: 0.3729 Acc: 0.8102 | Val Loss: 0.3548 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3392 Acc: 0.8321 | Val Loss: 0.3773 Acc: 0.8102\n",
      "Fold 4 Test Accuracy: 0.8139\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.5943 Acc: 0.6642 | Val Loss: 0.4295 Acc: 0.8102\n",
      "Epoch 2/25 | Train Loss: 0.4297 Acc: 0.7810 | Val Loss: 0.7049 Acc: 0.6022\n",
      "Epoch 3/25 | Train Loss: 0.5107 Acc: 0.7372 | Val Loss: 0.4157 Acc: 0.8175\n",
      "Epoch 4/25 | Train Loss: 0.4736 Acc: 0.7117 | Val Loss: 0.4361 Acc: 0.7956\n",
      "Epoch 5/25 | Train Loss: 0.4499 Acc: 0.7263 | Val Loss: 0.5513 Acc: 0.8029\n",
      "Epoch 6/25 | Train Loss: 0.4705 Acc: 0.7555 | Val Loss: 0.5007 Acc: 0.6752\n",
      "Epoch 7/25 | Train Loss: 0.3592 Acc: 0.8066 | Val Loss: 0.3842 Acc: 0.8066\n",
      "Epoch 8/25 | Train Loss: 0.3598 Acc: 0.8029 | Val Loss: 0.3507 Acc: 0.8102\n",
      "Epoch 9/25 | Train Loss: 0.3280 Acc: 0.8394 | Val Loss: 0.3507 Acc: 0.8321\n",
      "Epoch 10/25 | Train Loss: 0.3253 Acc: 0.8467 | Val Loss: 0.3565 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3474 Acc: 0.8394 | Val Loss: 0.3517 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3651 Acc: 0.8029 | Val Loss: 0.3759 Acc: 0.8102\n",
      "Epoch 13/25 | Train Loss: 0.3361 Acc: 0.8321 | Val Loss: 0.3724 Acc: 0.8102\n",
      "Epoch 14/25 | Train Loss: 0.3193 Acc: 0.8285 | Val Loss: 0.3564 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3267 Acc: 0.8431 | Val Loss: 0.3463 Acc: 0.8139\n",
      "Epoch 16/25 | Train Loss: 0.2944 Acc: 0.8613 | Val Loss: 0.3352 Acc: 0.8139\n",
      "Epoch 17/25 | Train Loss: 0.3381 Acc: 0.8358 | Val Loss: 0.3397 Acc: 0.8066\n",
      "Epoch 18/25 | Train Loss: 0.3237 Acc: 0.8577 | Val Loss: 0.3248 Acc: 0.8285\n",
      "Epoch 19/25 | Train Loss: 0.3210 Acc: 0.8394 | Val Loss: 0.3500 Acc: 0.8139\n",
      "Epoch 20/25 | Train Loss: 0.3211 Acc: 0.8394 | Val Loss: 0.3425 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3113 Acc: 0.8431 | Val Loss: 0.3613 Acc: 0.7956\n",
      "Epoch 22/25 | Train Loss: 0.3235 Acc: 0.8394 | Val Loss: 0.3320 Acc: 0.8321\n",
      "Epoch 23/25 | Train Loss: 0.3198 Acc: 0.8358 | Val Loss: 0.3516 Acc: 0.8212\n",
      "Epoch 24/25 | Train Loss: 0.3112 Acc: 0.8358 | Val Loss: 0.3533 Acc: 0.8102\n",
      "Epoch 25/25 | Train Loss: 0.3204 Acc: 0.8431 | Val Loss: 0.3492 Acc: 0.8139\n",
      "Fold 5 Test Accuracy: 0.8066\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5850 Acc: 0.6825 | Val Loss: 0.4141 Acc: 0.8248\n",
      "Epoch 2/25 | Train Loss: 0.4522 Acc: 0.7774 | Val Loss: 0.4737 Acc: 0.6971\n",
      "Epoch 3/25 | Train Loss: 0.4603 Acc: 0.7555 | Val Loss: 0.5238 Acc: 0.6241\n",
      "Epoch 4/25 | Train Loss: 0.4423 Acc: 0.7701 | Val Loss: 0.4372 Acc: 0.7445\n",
      "Epoch 5/25 | Train Loss: 0.4244 Acc: 0.7883 | Val Loss: 0.4805 Acc: 0.7956\n",
      "Epoch 6/25 | Train Loss: 0.4403 Acc: 0.7847 | Val Loss: 0.3781 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.4184 Acc: 0.7810 | Val Loss: 0.4153 Acc: 0.8212\n",
      "Epoch 8/25 | Train Loss: 0.3749 Acc: 0.8139 | Val Loss: 0.4412 Acc: 0.7810\n",
      "Epoch 9/25 | Train Loss: 0.3540 Acc: 0.8321 | Val Loss: 0.3736 Acc: 0.7883\n",
      "Epoch 10/25 | Train Loss: 0.3661 Acc: 0.8066 | Val Loss: 0.3907 Acc: 0.7920\n",
      "Epoch 11/25 | Train Loss: 0.3589 Acc: 0.8212 | Val Loss: 0.3938 Acc: 0.7956\n",
      "Epoch 12/25 | Train Loss: 0.3809 Acc: 0.7993 | Val Loss: 0.3839 Acc: 0.7737\n",
      "Epoch 13/25 | Train Loss: 0.3813 Acc: 0.8102 | Val Loss: 0.3720 Acc: 0.8066\n",
      "Epoch 14/25 | Train Loss: 0.3537 Acc: 0.8358 | Val Loss: 0.3925 Acc: 0.8029\n",
      "Epoch 15/25 | Train Loss: 0.3451 Acc: 0.8467 | Val Loss: 0.4034 Acc: 0.8066\n",
      "Epoch 16/25 | Train Loss: 0.3495 Acc: 0.8175 | Val Loss: 0.4028 Acc: 0.7920\n",
      "Epoch 17/25 | Train Loss: 0.3434 Acc: 0.8248 | Val Loss: 0.3830 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3538 Acc: 0.8321 | Val Loss: 0.3741 Acc: 0.7993\n",
      "Epoch 19/25 | Train Loss: 0.3419 Acc: 0.8285 | Val Loss: 0.3805 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.3440 Acc: 0.8139 | Val Loss: 0.3973 Acc: 0.8066\n",
      "Epoch 21/25 | Train Loss: 0.3291 Acc: 0.8248 | Val Loss: 0.3707 Acc: 0.8066\n",
      "Epoch 22/25 | Train Loss: 0.3478 Acc: 0.8212 | Val Loss: 0.3943 Acc: 0.7847\n",
      "Epoch 23/25 | Train Loss: 0.3361 Acc: 0.8394 | Val Loss: 0.3632 Acc: 0.8175\n",
      "Epoch 24/25 | Train Loss: 0.3407 Acc: 0.8394 | Val Loss: 0.3841 Acc: 0.7883\n",
      "Epoch 25/25 | Train Loss: 0.3338 Acc: 0.8358 | Val Loss: 0.3768 Acc: 0.8066\n",
      "Fold 6 Test Accuracy: 0.7993\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.5899 Acc: 0.6861 | Val Loss: 0.8139 Acc: 0.5985\n",
      "Epoch 2/25 | Train Loss: 0.5595 Acc: 0.7372 | Val Loss: 0.5833 Acc: 0.6679\n",
      "Epoch 3/25 | Train Loss: 0.4568 Acc: 0.7409 | Val Loss: 0.3507 Acc: 0.8066\n",
      "Epoch 4/25 | Train Loss: 0.4189 Acc: 0.7628 | Val Loss: 0.3870 Acc: 0.8358\n",
      "Epoch 5/25 | Train Loss: 0.4402 Acc: 0.7628 | Val Loss: 0.3695 Acc: 0.7956\n",
      "Epoch 6/25 | Train Loss: 0.4108 Acc: 0.8139 | Val Loss: 0.3993 Acc: 0.7664\n",
      "Epoch 7/25 | Train Loss: 0.4351 Acc: 0.7518 | Val Loss: 0.4483 Acc: 0.7226\n",
      "Epoch 8/25 | Train Loss: 0.3571 Acc: 0.8394 | Val Loss: 0.3289 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3176 Acc: 0.8467 | Val Loss: 0.2929 Acc: 0.8686\n",
      "Epoch 10/25 | Train Loss: 0.3136 Acc: 0.8394 | Val Loss: 0.2701 Acc: 0.8723\n",
      "Epoch 11/25 | Train Loss: 0.3156 Acc: 0.8285 | Val Loss: 0.3069 Acc: 0.8394\n",
      "Epoch 12/25 | Train Loss: 0.3048 Acc: 0.8577 | Val Loss: 0.2563 Acc: 0.8759\n",
      "Epoch 13/25 | Train Loss: 0.2953 Acc: 0.8759 | Val Loss: 0.2947 Acc: 0.8504\n",
      "Epoch 14/25 | Train Loss: 0.2637 Acc: 0.9015 | Val Loss: 0.2402 Acc: 0.8905\n",
      "Epoch 15/25 | Train Loss: 0.3054 Acc: 0.8394 | Val Loss: 0.2833 Acc: 0.8613\n",
      "Epoch 16/25 | Train Loss: 0.2736 Acc: 0.8796 | Val Loss: 0.2727 Acc: 0.8723\n",
      "Epoch 17/25 | Train Loss: 0.2797 Acc: 0.8796 | Val Loss: 0.2652 Acc: 0.8723\n",
      "Epoch 18/25 | Train Loss: 0.2731 Acc: 0.8869 | Val Loss: 0.2552 Acc: 0.8978\n",
      "Epoch 19/25 | Train Loss: 0.2673 Acc: 0.8759 | Val Loss: 0.2517 Acc: 0.8723\n",
      "Epoch 20/25 | Train Loss: 0.2799 Acc: 0.8759 | Val Loss: 0.2385 Acc: 0.8978\n",
      "Epoch 21/25 | Train Loss: 0.2725 Acc: 0.8686 | Val Loss: 0.2621 Acc: 0.8942\n",
      "Epoch 22/25 | Train Loss: 0.2636 Acc: 0.8869 | Val Loss: 0.2450 Acc: 0.8832\n",
      "Epoch 23/25 | Train Loss: 0.2962 Acc: 0.8796 | Val Loss: 0.2645 Acc: 0.8832\n",
      "Epoch 24/25 | Train Loss: 0.2915 Acc: 0.8686 | Val Loss: 0.2588 Acc: 0.8650\n",
      "Epoch 25/25 | Train Loss: 0.2799 Acc: 0.8613 | Val Loss: 0.2582 Acc: 0.8832\n",
      "Fold 7 Test Accuracy: 0.8796\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.5088 Acc: 0.7080 | Val Loss: 0.4851 Acc: 0.6934\n",
      "Epoch 2/25 | Train Loss: 0.3794 Acc: 0.7883 | Val Loss: 0.5258 Acc: 0.7372\n",
      "Epoch 3/25 | Train Loss: 0.3981 Acc: 0.8102 | Val Loss: 0.4378 Acc: 0.7263\n",
      "Epoch 4/25 | Train Loss: 0.3766 Acc: 0.8175 | Val Loss: 0.4820 Acc: 0.7591\n",
      "Epoch 5/25 | Train Loss: 0.4222 Acc: 0.8139 | Val Loss: 0.3946 Acc: 0.8212\n",
      "Epoch 6/25 | Train Loss: 0.3668 Acc: 0.8467 | Val Loss: 0.3669 Acc: 0.8175\n",
      "Epoch 7/25 | Train Loss: 0.4022 Acc: 0.8029 | Val Loss: 0.3997 Acc: 0.7810\n",
      "Epoch 8/25 | Train Loss: 0.3098 Acc: 0.8467 | Val Loss: 0.3772 Acc: 0.8066\n",
      "Epoch 9/25 | Train Loss: 0.3420 Acc: 0.8321 | Val Loss: 0.3644 Acc: 0.7956\n",
      "Epoch 10/25 | Train Loss: 0.3054 Acc: 0.8650 | Val Loss: 0.4246 Acc: 0.7847\n",
      "Epoch 11/25 | Train Loss: 0.3362 Acc: 0.8540 | Val Loss: 0.3571 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.3084 Acc: 0.8686 | Val Loss: 0.4016 Acc: 0.7847\n",
      "Epoch 13/25 | Train Loss: 0.2869 Acc: 0.8650 | Val Loss: 0.3934 Acc: 0.7810\n",
      "Epoch 14/25 | Train Loss: 0.2995 Acc: 0.8577 | Val Loss: 0.3843 Acc: 0.7993\n",
      "Epoch 15/25 | Train Loss: 0.2754 Acc: 0.8723 | Val Loss: 0.3480 Acc: 0.8102\n",
      "Epoch 16/25 | Train Loss: 0.2762 Acc: 0.8869 | Val Loss: 0.3543 Acc: 0.8175\n",
      "Epoch 17/25 | Train Loss: 0.2708 Acc: 0.8686 | Val Loss: 0.3582 Acc: 0.8102\n",
      "Epoch 18/25 | Train Loss: 0.2752 Acc: 0.8759 | Val Loss: 0.3836 Acc: 0.7956\n",
      "Epoch 19/25 | Train Loss: 0.2695 Acc: 0.8942 | Val Loss: 0.3773 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.2702 Acc: 0.8978 | Val Loss: 0.3812 Acc: 0.8212\n",
      "Epoch 21/25 | Train Loss: 0.3040 Acc: 0.8540 | Val Loss: 0.3722 Acc: 0.8029\n",
      "Epoch 22/25 | Train Loss: 0.2756 Acc: 0.8723 | Val Loss: 0.3673 Acc: 0.7810\n",
      "Epoch 23/25 | Train Loss: 0.2685 Acc: 0.8869 | Val Loss: 0.3905 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2693 Acc: 0.8869 | Val Loss: 0.3832 Acc: 0.7920\n",
      "Epoch 25/25 | Train Loss: 0.2825 Acc: 0.8759 | Val Loss: 0.3576 Acc: 0.8102\n",
      "Fold 8 Test Accuracy: 0.7993\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5337 Acc: 0.7153 | Val Loss: 0.7934 Acc: 0.5949\n",
      "Epoch 2/25 | Train Loss: 0.5189 Acc: 0.7482 | Val Loss: 0.4222 Acc: 0.8102\n",
      "Epoch 3/25 | Train Loss: 0.4232 Acc: 0.8029 | Val Loss: 0.3766 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4676 Acc: 0.7591 | Val Loss: 0.4072 Acc: 0.7883\n",
      "Epoch 5/25 | Train Loss: 0.4112 Acc: 0.7810 | Val Loss: 0.3781 Acc: 0.8175\n",
      "Epoch 6/25 | Train Loss: 0.3486 Acc: 0.8248 | Val Loss: 0.3807 Acc: 0.8248\n",
      "Epoch 7/25 | Train Loss: 0.3780 Acc: 0.7993 | Val Loss: 0.4476 Acc: 0.7482\n",
      "Epoch 8/25 | Train Loss: 0.3287 Acc: 0.8394 | Val Loss: 0.3444 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.2971 Acc: 0.8577 | Val Loss: 0.3317 Acc: 0.8504\n",
      "Epoch 10/25 | Train Loss: 0.3227 Acc: 0.8358 | Val Loss: 0.3271 Acc: 0.8175\n",
      "Epoch 11/25 | Train Loss: 0.2798 Acc: 0.8723 | Val Loss: 0.3134 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.2800 Acc: 0.8613 | Val Loss: 0.3138 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.2755 Acc: 0.8759 | Val Loss: 0.3506 Acc: 0.8175\n",
      "Epoch 14/25 | Train Loss: 0.2900 Acc: 0.8650 | Val Loss: 0.3233 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.2717 Acc: 0.8869 | Val Loss: 0.3236 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.2757 Acc: 0.8796 | Val Loss: 0.3215 Acc: 0.8285\n",
      "Epoch 17/25 | Train Loss: 0.2685 Acc: 0.8723 | Val Loss: 0.3214 Acc: 0.8504\n",
      "Epoch 18/25 | Train Loss: 0.2910 Acc: 0.8504 | Val Loss: 0.3093 Acc: 0.8577\n",
      "Epoch 19/25 | Train Loss: 0.2628 Acc: 0.8942 | Val Loss: 0.3099 Acc: 0.8577\n",
      "Epoch 20/25 | Train Loss: 0.2680 Acc: 0.8686 | Val Loss: 0.3284 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.2481 Acc: 0.8942 | Val Loss: 0.3250 Acc: 0.8358\n",
      "Epoch 22/25 | Train Loss: 0.2963 Acc: 0.8686 | Val Loss: 0.3098 Acc: 0.8394\n",
      "Epoch 23/25 | Train Loss: 0.2375 Acc: 0.8978 | Val Loss: 0.3404 Acc: 0.8467\n",
      "Epoch 24/25 | Train Loss: 0.2685 Acc: 0.8905 | Val Loss: 0.2906 Acc: 0.8540\n",
      "Epoch 25/25 | Train Loss: 0.2957 Acc: 0.8759 | Val Loss: 0.3627 Acc: 0.8139\n",
      "Fold 9 Test Accuracy: 0.8394\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5259 Acc: 0.7044 | Val Loss: 0.5077 Acc: 0.6679\n",
      "Epoch 2/25 | Train Loss: 0.4946 Acc: 0.7810 | Val Loss: 0.4711 Acc: 0.7080\n",
      "Epoch 3/25 | Train Loss: 0.4242 Acc: 0.7810 | Val Loss: 0.4685 Acc: 0.8029\n",
      "Epoch 4/25 | Train Loss: 0.4849 Acc: 0.7810 | Val Loss: 0.4490 Acc: 0.8102\n",
      "Epoch 5/25 | Train Loss: 0.4172 Acc: 0.7956 | Val Loss: 0.4375 Acc: 0.7263\n",
      "Epoch 6/25 | Train Loss: 0.4006 Acc: 0.7993 | Val Loss: 0.4210 Acc: 0.7956\n",
      "Epoch 7/25 | Train Loss: 0.4832 Acc: 0.7591 | Val Loss: 0.5054 Acc: 0.7993\n",
      "Epoch 8/25 | Train Loss: 0.4492 Acc: 0.8139 | Val Loss: 0.4413 Acc: 0.7993\n",
      "Epoch 9/25 | Train Loss: 0.3814 Acc: 0.8285 | Val Loss: 0.3844 Acc: 0.8139\n",
      "Epoch 10/25 | Train Loss: 0.3345 Acc: 0.8431 | Val Loss: 0.3622 Acc: 0.8139\n",
      "Epoch 11/25 | Train Loss: 0.3212 Acc: 0.8577 | Val Loss: 0.3889 Acc: 0.8066\n",
      "Epoch 12/25 | Train Loss: 0.3227 Acc: 0.8613 | Val Loss: 0.3688 Acc: 0.8029\n",
      "Epoch 13/25 | Train Loss: 0.3236 Acc: 0.8504 | Val Loss: 0.3605 Acc: 0.8248\n",
      "Epoch 14/25 | Train Loss: 0.3278 Acc: 0.8139 | Val Loss: 0.3547 Acc: 0.8175\n",
      "Epoch 15/25 | Train Loss: 0.3243 Acc: 0.8358 | Val Loss: 0.3378 Acc: 0.8358\n",
      "Epoch 16/25 | Train Loss: 0.3157 Acc: 0.8431 | Val Loss: 0.3553 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3227 Acc: 0.8504 | Val Loss: 0.3808 Acc: 0.8139\n",
      "Epoch 18/25 | Train Loss: 0.3091 Acc: 0.8467 | Val Loss: 0.3588 Acc: 0.8066\n",
      "Epoch 19/25 | Train Loss: 0.3462 Acc: 0.8321 | Val Loss: 0.3449 Acc: 0.8321\n",
      "Epoch 20/25 | Train Loss: 0.3421 Acc: 0.8358 | Val Loss: 0.3282 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.3192 Acc: 0.8540 | Val Loss: 0.3322 Acc: 0.8212\n",
      "Epoch 22/25 | Train Loss: 0.3285 Acc: 0.8212 | Val Loss: 0.3745 Acc: 0.8066\n",
      "Epoch 23/25 | Train Loss: 0.3219 Acc: 0.8431 | Val Loss: 0.3541 Acc: 0.8285\n",
      "Epoch 24/25 | Train Loss: 0.3104 Acc: 0.8431 | Val Loss: 0.3317 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3381 Acc: 0.8358 | Val Loss: 0.3655 Acc: 0.8175\n",
      "Fold 10 Test Accuracy: 0.8066\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8241, Std Dev = 0.0262\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/CNNVAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T17:42:01.006555Z",
     "start_time": "2025-05-18T16:22:26.805137Z"
    }
   },
   "id": "17aece9678433731",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### VAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27474d16c7841d1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_folder = 'generated_images/VAE'\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=[neg_folder, sample_folder], max_per_dir=220, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T17:42:01.036343Z",
     "start_time": "2025-05-18T17:42:01.012023Z"
    }
   },
   "id": "f7e2782f22bb81cf",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.5329 Acc: 0.7737 | Val Loss: 0.4609 Acc: 0.6788\n",
      "Epoch 2/25 | Train Loss: 0.5106 Acc: 0.7263 | Val Loss: 0.5414 Acc: 0.6314\n",
      "Epoch 3/25 | Train Loss: 0.5527 Acc: 0.7263 | Val Loss: 0.4384 Acc: 0.6861\n",
      "Epoch 4/25 | Train Loss: 0.3941 Acc: 0.7810 | Val Loss: 0.3955 Acc: 0.7847\n",
      "Epoch 5/25 | Train Loss: 0.4355 Acc: 0.7920 | Val Loss: 0.4531 Acc: 0.7920\n",
      "Epoch 6/25 | Train Loss: 0.3794 Acc: 0.8139 | Val Loss: 0.5224 Acc: 0.7993\n",
      "Epoch 7/25 | Train Loss: 0.3602 Acc: 0.8212 | Val Loss: 0.3843 Acc: 0.8175\n",
      "Epoch 8/25 | Train Loss: 0.3452 Acc: 0.8212 | Val Loss: 0.3575 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3219 Acc: 0.8358 | Val Loss: 0.3499 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3499 Acc: 0.8285 | Val Loss: 0.3399 Acc: 0.8212\n",
      "Epoch 11/25 | Train Loss: 0.3342 Acc: 0.8394 | Val Loss: 0.3581 Acc: 0.8212\n",
      "Epoch 12/25 | Train Loss: 0.3015 Acc: 0.8504 | Val Loss: 0.3319 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.3132 Acc: 0.8504 | Val Loss: 0.3209 Acc: 0.8431\n",
      "Epoch 14/25 | Train Loss: 0.3155 Acc: 0.8577 | Val Loss: 0.3246 Acc: 0.8102\n",
      "Epoch 15/25 | Train Loss: 0.2992 Acc: 0.8504 | Val Loss: 0.3269 Acc: 0.8285\n",
      "Epoch 16/25 | Train Loss: 0.3086 Acc: 0.8540 | Val Loss: 0.3111 Acc: 0.8394\n",
      "Epoch 17/25 | Train Loss: 0.3199 Acc: 0.8394 | Val Loss: 0.3216 Acc: 0.8248\n",
      "Epoch 18/25 | Train Loss: 0.3197 Acc: 0.8285 | Val Loss: 0.3197 Acc: 0.8248\n",
      "Epoch 19/25 | Train Loss: 0.3058 Acc: 0.8540 | Val Loss: 0.3334 Acc: 0.7956\n",
      "Epoch 20/25 | Train Loss: 0.2737 Acc: 0.8832 | Val Loss: 0.3108 Acc: 0.8285\n",
      "Epoch 21/25 | Train Loss: 0.2970 Acc: 0.8504 | Val Loss: 0.3047 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.3056 Acc: 0.8540 | Val Loss: 0.3349 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.2880 Acc: 0.8723 | Val Loss: 0.3128 Acc: 0.8504\n",
      "Epoch 24/25 | Train Loss: 0.3155 Acc: 0.8504 | Val Loss: 0.3256 Acc: 0.8467\n",
      "Epoch 25/25 | Train Loss: 0.3041 Acc: 0.8504 | Val Loss: 0.3286 Acc: 0.8394\n",
      "Fold 1 Test Accuracy: 0.8175\n",
      "===== Fold 2 =====\n",
      "Epoch 1/25 | Train Loss: 0.5278 Acc: 0.7226 | Val Loss: 0.4508 Acc: 0.7299\n",
      "Epoch 2/25 | Train Loss: 0.4481 Acc: 0.7445 | Val Loss: 0.4126 Acc: 0.7920\n",
      "Epoch 3/25 | Train Loss: 0.4668 Acc: 0.7445 | Val Loss: 0.3983 Acc: 0.8102\n",
      "Epoch 4/25 | Train Loss: 0.4470 Acc: 0.7883 | Val Loss: 0.3716 Acc: 0.8139\n",
      "Epoch 5/25 | Train Loss: 0.4326 Acc: 0.7555 | Val Loss: 0.3613 Acc: 0.8102\n",
      "Epoch 6/25 | Train Loss: 0.3943 Acc: 0.7883 | Val Loss: 0.3638 Acc: 0.8102\n",
      "Epoch 7/25 | Train Loss: 0.3867 Acc: 0.7956 | Val Loss: 0.3388 Acc: 0.8358\n",
      "Epoch 8/25 | Train Loss: 0.4095 Acc: 0.8029 | Val Loss: 0.3482 Acc: 0.8285\n",
      "Epoch 9/25 | Train Loss: 0.3891 Acc: 0.8102 | Val Loss: 0.3551 Acc: 0.8285\n",
      "Epoch 10/25 | Train Loss: 0.3432 Acc: 0.8431 | Val Loss: 0.3471 Acc: 0.8394\n",
      "Epoch 11/25 | Train Loss: 0.3563 Acc: 0.8139 | Val Loss: 0.3471 Acc: 0.8321\n",
      "Epoch 12/25 | Train Loss: 0.3295 Acc: 0.8358 | Val Loss: 0.3275 Acc: 0.8321\n",
      "Epoch 13/25 | Train Loss: 0.3642 Acc: 0.8139 | Val Loss: 0.3281 Acc: 0.8248\n",
      "Epoch 14/25 | Train Loss: 0.3899 Acc: 0.7993 | Val Loss: 0.3528 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3580 Acc: 0.8102 | Val Loss: 0.3292 Acc: 0.8431\n",
      "Epoch 16/25 | Train Loss: 0.3499 Acc: 0.8212 | Val Loss: 0.3494 Acc: 0.8248\n",
      "Epoch 17/25 | Train Loss: 0.3454 Acc: 0.8212 | Val Loss: 0.3269 Acc: 0.8540\n",
      "Epoch 18/25 | Train Loss: 0.3486 Acc: 0.8285 | Val Loss: 0.3370 Acc: 0.8212\n",
      "Epoch 19/25 | Train Loss: 0.3444 Acc: 0.8066 | Val Loss: 0.3600 Acc: 0.8102\n",
      "Epoch 20/25 | Train Loss: 0.3483 Acc: 0.8540 | Val Loss: 0.3275 Acc: 0.8577\n",
      "Epoch 21/25 | Train Loss: 0.3446 Acc: 0.8431 | Val Loss: 0.3309 Acc: 0.8431\n",
      "Epoch 22/25 | Train Loss: 0.3575 Acc: 0.8175 | Val Loss: 0.3288 Acc: 0.8540\n",
      "Epoch 23/25 | Train Loss: 0.3466 Acc: 0.8248 | Val Loss: 0.3278 Acc: 0.8504\n",
      "Epoch 24/25 | Train Loss: 0.3420 Acc: 0.8066 | Val Loss: 0.3555 Acc: 0.8358\n",
      "Epoch 25/25 | Train Loss: 0.3471 Acc: 0.8066 | Val Loss: 0.3412 Acc: 0.8358\n",
      "Fold 2 Test Accuracy: 0.8212\n",
      "===== Fold 3 =====\n",
      "Epoch 1/25 | Train Loss: 0.5039 Acc: 0.7044 | Val Loss: 0.4921 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.5320 Acc: 0.7372 | Val Loss: 0.5939 Acc: 0.6241\n",
      "Epoch 3/25 | Train Loss: 0.4656 Acc: 0.7737 | Val Loss: 0.3821 Acc: 0.8248\n",
      "Epoch 4/25 | Train Loss: 0.4553 Acc: 0.7555 | Val Loss: 0.3995 Acc: 0.8066\n",
      "Epoch 5/25 | Train Loss: 0.5155 Acc: 0.7737 | Val Loss: 0.3808 Acc: 0.7737\n",
      "Epoch 6/25 | Train Loss: 0.3453 Acc: 0.8285 | Val Loss: 0.3734 Acc: 0.7847\n",
      "Epoch 7/25 | Train Loss: 0.3378 Acc: 0.8285 | Val Loss: 0.4824 Acc: 0.8102\n",
      "Epoch 8/25 | Train Loss: 0.3395 Acc: 0.8248 | Val Loss: 0.3640 Acc: 0.8321\n",
      "Epoch 9/25 | Train Loss: 0.2920 Acc: 0.8650 | Val Loss: 0.3108 Acc: 0.8394\n",
      "Epoch 10/25 | Train Loss: 0.2859 Acc: 0.8358 | Val Loss: 0.3217 Acc: 0.8321\n",
      "Epoch 11/25 | Train Loss: 0.2583 Acc: 0.8759 | Val Loss: 0.3341 Acc: 0.8248\n",
      "Epoch 12/25 | Train Loss: 0.2641 Acc: 0.8905 | Val Loss: 0.4598 Acc: 0.8139\n",
      "Epoch 13/25 | Train Loss: 0.2983 Acc: 0.8650 | Val Loss: 0.3801 Acc: 0.8285\n",
      "Epoch 14/25 | Train Loss: 0.2907 Acc: 0.8686 | Val Loss: 0.3442 Acc: 0.8321\n",
      "Epoch 15/25 | Train Loss: 0.3157 Acc: 0.8504 | Val Loss: 0.3190 Acc: 0.8540\n",
      "Epoch 16/25 | Train Loss: 0.2841 Acc: 0.8759 | Val Loss: 0.2979 Acc: 0.8650\n",
      "Epoch 17/25 | Train Loss: 0.2790 Acc: 0.8869 | Val Loss: 0.3226 Acc: 0.8504\n",
      "Epoch 18/25 | Train Loss: 0.2756 Acc: 0.8759 | Val Loss: 0.3341 Acc: 0.8540\n",
      "Epoch 19/25 | Train Loss: 0.2843 Acc: 0.8686 | Val Loss: 0.3194 Acc: 0.8577\n",
      "Epoch 20/25 | Train Loss: 0.2726 Acc: 0.8759 | Val Loss: 0.3019 Acc: 0.8613\n",
      "Epoch 21/25 | Train Loss: 0.2326 Acc: 0.9270 | Val Loss: 0.3118 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.2511 Acc: 0.8978 | Val Loss: 0.2979 Acc: 0.8723\n",
      "Epoch 23/25 | Train Loss: 0.2629 Acc: 0.8759 | Val Loss: 0.2978 Acc: 0.8540\n",
      "Epoch 24/25 | Train Loss: 0.2781 Acc: 0.8723 | Val Loss: 0.3252 Acc: 0.8431\n",
      "Epoch 25/25 | Train Loss: 0.2652 Acc: 0.8832 | Val Loss: 0.2943 Acc: 0.8504\n",
      "Fold 3 Test Accuracy: 0.8577\n",
      "===== Fold 4 =====\n",
      "Epoch 1/25 | Train Loss: 0.5739 Acc: 0.6788 | Val Loss: 0.4214 Acc: 0.8029\n",
      "Epoch 2/25 | Train Loss: 0.4694 Acc: 0.7372 | Val Loss: 0.4630 Acc: 0.7664\n",
      "Epoch 3/25 | Train Loss: 0.4883 Acc: 0.7774 | Val Loss: 0.6147 Acc: 0.6058\n",
      "Epoch 4/25 | Train Loss: 0.4322 Acc: 0.7555 | Val Loss: 0.4128 Acc: 0.7153\n",
      "Epoch 5/25 | Train Loss: 0.4257 Acc: 0.7847 | Val Loss: 0.3796 Acc: 0.7993\n",
      "Epoch 6/25 | Train Loss: 0.4546 Acc: 0.7409 | Val Loss: 0.3801 Acc: 0.7993\n",
      "Epoch 7/25 | Train Loss: 0.3535 Acc: 0.8248 | Val Loss: 0.3603 Acc: 0.8285\n",
      "Epoch 8/25 | Train Loss: 0.3476 Acc: 0.8066 | Val Loss: 0.3624 Acc: 0.7920\n",
      "Epoch 9/25 | Train Loss: 0.3348 Acc: 0.8175 | Val Loss: 0.3605 Acc: 0.8102\n",
      "Epoch 10/25 | Train Loss: 0.3464 Acc: 0.8175 | Val Loss: 0.3694 Acc: 0.7993\n",
      "Epoch 11/25 | Train Loss: 0.3548 Acc: 0.8139 | Val Loss: 0.3316 Acc: 0.8540\n",
      "Epoch 12/25 | Train Loss: 0.3638 Acc: 0.8321 | Val Loss: 0.3573 Acc: 0.8102\n",
      "Epoch 13/25 | Train Loss: 0.3411 Acc: 0.8139 | Val Loss: 0.3391 Acc: 0.8285\n",
      "Epoch 14/25 | Train Loss: 0.3419 Acc: 0.8358 | Val Loss: 0.3466 Acc: 0.8285\n",
      "Epoch 15/25 | Train Loss: 0.3424 Acc: 0.8248 | Val Loss: 0.3328 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.3398 Acc: 0.8358 | Val Loss: 0.3465 Acc: 0.8029\n",
      "Epoch 17/25 | Train Loss: 0.3246 Acc: 0.8431 | Val Loss: 0.3366 Acc: 0.8285\n",
      "Epoch 18/25 | Train Loss: 0.3428 Acc: 0.8321 | Val Loss: 0.3355 Acc: 0.8175\n",
      "Epoch 19/25 | Train Loss: 0.3310 Acc: 0.8358 | Val Loss: 0.3371 Acc: 0.8212\n",
      "Epoch 20/25 | Train Loss: 0.3247 Acc: 0.8285 | Val Loss: 0.3474 Acc: 0.8285\n",
      "Epoch 21/25 | Train Loss: 0.3393 Acc: 0.8248 | Val Loss: 0.3333 Acc: 0.8175\n",
      "Epoch 22/25 | Train Loss: 0.3390 Acc: 0.8394 | Val Loss: 0.3463 Acc: 0.8102\n",
      "Epoch 23/25 | Train Loss: 0.3118 Acc: 0.8394 | Val Loss: 0.3371 Acc: 0.8285\n",
      "Epoch 24/25 | Train Loss: 0.3307 Acc: 0.8358 | Val Loss: 0.3448 Acc: 0.8321\n",
      "Epoch 25/25 | Train Loss: 0.3398 Acc: 0.8285 | Val Loss: 0.3260 Acc: 0.8431\n",
      "Fold 4 Test Accuracy: 0.8066\n",
      "===== Fold 5 =====\n",
      "Epoch 1/25 | Train Loss: 0.6029 Acc: 0.6971 | Val Loss: 0.5228 Acc: 0.7299\n",
      "Epoch 2/25 | Train Loss: 0.6357 Acc: 0.7117 | Val Loss: 0.5109 Acc: 0.6277\n",
      "Epoch 3/25 | Train Loss: 0.4479 Acc: 0.7701 | Val Loss: 0.4083 Acc: 0.7883\n",
      "Epoch 4/25 | Train Loss: 0.4340 Acc: 0.7847 | Val Loss: 0.3984 Acc: 0.7847\n",
      "Epoch 5/25 | Train Loss: 0.4386 Acc: 0.7190 | Val Loss: 0.3898 Acc: 0.8102\n",
      "Epoch 6/25 | Train Loss: 0.4433 Acc: 0.7920 | Val Loss: 0.4364 Acc: 0.7372\n",
      "Epoch 7/25 | Train Loss: 0.4384 Acc: 0.7482 | Val Loss: 0.4008 Acc: 0.7737\n",
      "Epoch 8/25 | Train Loss: 0.4031 Acc: 0.7920 | Val Loss: 0.3953 Acc: 0.8029\n",
      "Epoch 9/25 | Train Loss: 0.3866 Acc: 0.8175 | Val Loss: 0.3868 Acc: 0.8102\n",
      "Epoch 10/25 | Train Loss: 0.3818 Acc: 0.8029 | Val Loss: 0.3941 Acc: 0.7993\n",
      "Epoch 11/25 | Train Loss: 0.3839 Acc: 0.8066 | Val Loss: 0.3871 Acc: 0.8139\n",
      "Epoch 12/25 | Train Loss: 0.3904 Acc: 0.7847 | Val Loss: 0.3880 Acc: 0.7993\n",
      "Epoch 13/25 | Train Loss: 0.3910 Acc: 0.7920 | Val Loss: 0.3926 Acc: 0.7956\n",
      "Epoch 14/25 | Train Loss: 0.3980 Acc: 0.8212 | Val Loss: 0.3920 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.3849 Acc: 0.8102 | Val Loss: 0.3724 Acc: 0.8175\n",
      "Epoch 16/25 | Train Loss: 0.3809 Acc: 0.8066 | Val Loss: 0.3859 Acc: 0.8066\n",
      "Epoch 17/25 | Train Loss: 0.3576 Acc: 0.8102 | Val Loss: 0.3813 Acc: 0.8029\n",
      "Epoch 18/25 | Train Loss: 0.3596 Acc: 0.8066 | Val Loss: 0.3947 Acc: 0.8029\n",
      "Epoch 19/25 | Train Loss: 0.3753 Acc: 0.8102 | Val Loss: 0.3812 Acc: 0.7993\n",
      "Epoch 20/25 | Train Loss: 0.3914 Acc: 0.8029 | Val Loss: 0.3673 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3839 Acc: 0.8102 | Val Loss: 0.3777 Acc: 0.7993\n",
      "Epoch 22/25 | Train Loss: 0.3669 Acc: 0.8066 | Val Loss: 0.3590 Acc: 0.7993\n",
      "Epoch 23/25 | Train Loss: 0.3825 Acc: 0.8029 | Val Loss: 0.3714 Acc: 0.8066\n",
      "Epoch 24/25 | Train Loss: 0.3634 Acc: 0.8066 | Val Loss: 0.3763 Acc: 0.8102\n",
      "Epoch 25/25 | Train Loss: 0.3630 Acc: 0.8029 | Val Loss: 0.3791 Acc: 0.8175\n",
      "Fold 5 Test Accuracy: 0.8029\n",
      "===== Fold 6 =====\n",
      "Epoch 1/25 | Train Loss: 0.5227 Acc: 0.7153 | Val Loss: 0.4347 Acc: 0.7263\n",
      "Epoch 2/25 | Train Loss: 0.4664 Acc: 0.7774 | Val Loss: 0.4186 Acc: 0.7336\n",
      "Epoch 3/25 | Train Loss: 0.4253 Acc: 0.7993 | Val Loss: 0.4775 Acc: 0.6752\n",
      "Epoch 4/25 | Train Loss: 0.3864 Acc: 0.8066 | Val Loss: 0.4496 Acc: 0.7226\n",
      "Epoch 5/25 | Train Loss: 0.4689 Acc: 0.7555 | Val Loss: 0.4098 Acc: 0.7299\n",
      "Epoch 6/25 | Train Loss: 0.3707 Acc: 0.8029 | Val Loss: 0.4048 Acc: 0.7591\n",
      "Epoch 7/25 | Train Loss: 0.3871 Acc: 0.7591 | Val Loss: 0.3884 Acc: 0.8212\n",
      "Epoch 8/25 | Train Loss: 0.3836 Acc: 0.7993 | Val Loss: 0.3722 Acc: 0.8102\n",
      "Epoch 9/25 | Train Loss: 0.3718 Acc: 0.8139 | Val Loss: 0.3554 Acc: 0.8066\n",
      "Epoch 10/25 | Train Loss: 0.3582 Acc: 0.8102 | Val Loss: 0.3577 Acc: 0.8139\n",
      "Epoch 11/25 | Train Loss: 0.3392 Acc: 0.8248 | Val Loss: 0.3454 Acc: 0.8212\n",
      "Epoch 12/25 | Train Loss: 0.3483 Acc: 0.8285 | Val Loss: 0.3643 Acc: 0.8285\n",
      "Epoch 13/25 | Train Loss: 0.3447 Acc: 0.8285 | Val Loss: 0.3288 Acc: 0.8358\n",
      "Epoch 14/25 | Train Loss: 0.3281 Acc: 0.8394 | Val Loss: 0.3318 Acc: 0.8248\n",
      "Epoch 15/25 | Train Loss: 0.3299 Acc: 0.8431 | Val Loss: 0.3597 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3332 Acc: 0.8358 | Val Loss: 0.3297 Acc: 0.8321\n",
      "Epoch 17/25 | Train Loss: 0.3332 Acc: 0.8431 | Val Loss: 0.3325 Acc: 0.8321\n",
      "Epoch 18/25 | Train Loss: 0.3449 Acc: 0.8102 | Val Loss: 0.3227 Acc: 0.8467\n",
      "Epoch 19/25 | Train Loss: 0.3100 Acc: 0.8431 | Val Loss: 0.3357 Acc: 0.8394\n",
      "Epoch 20/25 | Train Loss: 0.3139 Acc: 0.8431 | Val Loss: 0.3477 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.3433 Acc: 0.8175 | Val Loss: 0.3260 Acc: 0.8467\n",
      "Epoch 22/25 | Train Loss: 0.3357 Acc: 0.8248 | Val Loss: 0.3304 Acc: 0.8248\n",
      "Epoch 23/25 | Train Loss: 0.3257 Acc: 0.8285 | Val Loss: 0.3351 Acc: 0.8212\n",
      "Epoch 24/25 | Train Loss: 0.3111 Acc: 0.8650 | Val Loss: 0.3359 Acc: 0.8212\n",
      "Epoch 25/25 | Train Loss: 0.3246 Acc: 0.8467 | Val Loss: 0.3301 Acc: 0.8321\n",
      "Fold 6 Test Accuracy: 0.8431\n",
      "===== Fold 7 =====\n",
      "Epoch 1/25 | Train Loss: 0.5940 Acc: 0.6569 | Val Loss: 0.4445 Acc: 0.7226\n",
      "Epoch 2/25 | Train Loss: 0.4795 Acc: 0.7336 | Val Loss: 0.3631 Acc: 0.8066\n",
      "Epoch 3/25 | Train Loss: 0.4759 Acc: 0.7226 | Val Loss: 0.3857 Acc: 0.8467\n",
      "Epoch 4/25 | Train Loss: 0.4666 Acc: 0.7372 | Val Loss: 0.3560 Acc: 0.8248\n",
      "Epoch 5/25 | Train Loss: 0.4477 Acc: 0.7628 | Val Loss: 0.3467 Acc: 0.8358\n",
      "Epoch 6/25 | Train Loss: 0.4550 Acc: 0.7445 | Val Loss: 0.3463 Acc: 0.8467\n",
      "Epoch 7/25 | Train Loss: 0.5291 Acc: 0.7810 | Val Loss: 0.3331 Acc: 0.8248\n",
      "Epoch 8/25 | Train Loss: 0.3438 Acc: 0.8358 | Val Loss: 0.3116 Acc: 0.8467\n",
      "Epoch 9/25 | Train Loss: 0.3519 Acc: 0.8431 | Val Loss: 0.3014 Acc: 0.8723\n",
      "Epoch 10/25 | Train Loss: 0.3394 Acc: 0.8139 | Val Loss: 0.2991 Acc: 0.8613\n",
      "Epoch 11/25 | Train Loss: 0.3196 Acc: 0.8394 | Val Loss: 0.2981 Acc: 0.8650\n",
      "Epoch 12/25 | Train Loss: 0.3757 Acc: 0.8102 | Val Loss: 0.3046 Acc: 0.8431\n",
      "Epoch 13/25 | Train Loss: 0.3045 Acc: 0.8613 | Val Loss: 0.2872 Acc: 0.8613\n",
      "Epoch 14/25 | Train Loss: 0.3246 Acc: 0.8613 | Val Loss: 0.3009 Acc: 0.8394\n",
      "Epoch 15/25 | Train Loss: 0.3457 Acc: 0.8175 | Val Loss: 0.3108 Acc: 0.8723\n",
      "Epoch 16/25 | Train Loss: 0.3189 Acc: 0.8431 | Val Loss: 0.3076 Acc: 0.8321\n",
      "Epoch 17/25 | Train Loss: 0.3188 Acc: 0.8686 | Val Loss: 0.3302 Acc: 0.8577\n",
      "Epoch 18/25 | Train Loss: 0.3262 Acc: 0.8577 | Val Loss: 0.2974 Acc: 0.8394\n",
      "Epoch 19/25 | Train Loss: 0.3137 Acc: 0.8540 | Val Loss: 0.3183 Acc: 0.8285\n",
      "Epoch 20/25 | Train Loss: 0.3065 Acc: 0.8504 | Val Loss: 0.3055 Acc: 0.8394\n",
      "Epoch 21/25 | Train Loss: 0.3189 Acc: 0.8431 | Val Loss: 0.2830 Acc: 0.8723\n",
      "Epoch 22/25 | Train Loss: 0.3523 Acc: 0.8212 | Val Loss: 0.2842 Acc: 0.8650\n",
      "Epoch 23/25 | Train Loss: 0.3637 Acc: 0.8029 | Val Loss: 0.2893 Acc: 0.8686\n",
      "Epoch 24/25 | Train Loss: 0.3264 Acc: 0.8248 | Val Loss: 0.2807 Acc: 0.8613\n",
      "Epoch 25/25 | Train Loss: 0.3182 Acc: 0.8613 | Val Loss: 0.2920 Acc: 0.8504\n",
      "Fold 7 Test Accuracy: 0.8650\n",
      "===== Fold 8 =====\n",
      "Epoch 1/25 | Train Loss: 0.4904 Acc: 0.7628 | Val Loss: 0.4620 Acc: 0.7007\n",
      "Epoch 2/25 | Train Loss: 0.3420 Acc: 0.8139 | Val Loss: 0.4663 Acc: 0.7226\n",
      "Epoch 3/25 | Train Loss: 0.3749 Acc: 0.8139 | Val Loss: 0.4528 Acc: 0.7591\n",
      "Epoch 4/25 | Train Loss: 0.3456 Acc: 0.8394 | Val Loss: 0.4112 Acc: 0.7810\n",
      "Epoch 5/25 | Train Loss: 0.4131 Acc: 0.7847 | Val Loss: 0.4302 Acc: 0.7774\n",
      "Epoch 6/25 | Train Loss: 0.3128 Acc: 0.8650 | Val Loss: 0.4327 Acc: 0.7701\n",
      "Epoch 7/25 | Train Loss: 0.3785 Acc: 0.8175 | Val Loss: 0.4053 Acc: 0.7701\n",
      "Epoch 8/25 | Train Loss: 0.3355 Acc: 0.8431 | Val Loss: 0.3989 Acc: 0.7956\n",
      "Epoch 9/25 | Train Loss: 0.3237 Acc: 0.8613 | Val Loss: 0.4137 Acc: 0.7774\n",
      "Epoch 10/25 | Train Loss: 0.3488 Acc: 0.8321 | Val Loss: 0.4049 Acc: 0.7737\n",
      "Epoch 11/25 | Train Loss: 0.2917 Acc: 0.8358 | Val Loss: 0.3969 Acc: 0.7847\n",
      "Epoch 12/25 | Train Loss: 0.2875 Acc: 0.8577 | Val Loss: 0.3938 Acc: 0.7993\n",
      "Epoch 13/25 | Train Loss: 0.3145 Acc: 0.8577 | Val Loss: 0.3751 Acc: 0.7774\n",
      "Epoch 14/25 | Train Loss: 0.3096 Acc: 0.8431 | Val Loss: 0.3938 Acc: 0.7883\n",
      "Epoch 15/25 | Train Loss: 0.2987 Acc: 0.8577 | Val Loss: 0.3995 Acc: 0.7847\n",
      "Epoch 16/25 | Train Loss: 0.2967 Acc: 0.8540 | Val Loss: 0.3762 Acc: 0.7993\n",
      "Epoch 17/25 | Train Loss: 0.3015 Acc: 0.8540 | Val Loss: 0.3892 Acc: 0.7956\n",
      "Epoch 18/25 | Train Loss: 0.3085 Acc: 0.8540 | Val Loss: 0.3662 Acc: 0.7956\n",
      "Epoch 19/25 | Train Loss: 0.3035 Acc: 0.8467 | Val Loss: 0.3914 Acc: 0.7920\n",
      "Epoch 20/25 | Train Loss: 0.2966 Acc: 0.8467 | Val Loss: 0.4011 Acc: 0.7847\n",
      "Epoch 21/25 | Train Loss: 0.2953 Acc: 0.8504 | Val Loss: 0.3816 Acc: 0.7883\n",
      "Epoch 22/25 | Train Loss: 0.2946 Acc: 0.8467 | Val Loss: 0.3760 Acc: 0.7883\n",
      "Epoch 23/25 | Train Loss: 0.2877 Acc: 0.8504 | Val Loss: 0.3794 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2866 Acc: 0.8540 | Val Loss: 0.3574 Acc: 0.8102\n",
      "Epoch 25/25 | Train Loss: 0.3025 Acc: 0.8540 | Val Loss: 0.3715 Acc: 0.7920\n",
      "Fold 8 Test Accuracy: 0.7993\n",
      "===== Fold 9 =====\n",
      "Epoch 1/25 | Train Loss: 0.5323 Acc: 0.7044 | Val Loss: 0.6757 Acc: 0.6095\n",
      "Epoch 2/25 | Train Loss: 0.4745 Acc: 0.7774 | Val Loss: 0.4319 Acc: 0.7482\n",
      "Epoch 3/25 | Train Loss: 0.4457 Acc: 0.7737 | Val Loss: 0.4866 Acc: 0.7993\n",
      "Epoch 4/25 | Train Loss: 0.4439 Acc: 0.7701 | Val Loss: 0.4756 Acc: 0.8066\n",
      "Epoch 5/25 | Train Loss: 0.4257 Acc: 0.7591 | Val Loss: 0.4430 Acc: 0.8066\n",
      "Epoch 6/25 | Train Loss: 0.4458 Acc: 0.7628 | Val Loss: 0.4301 Acc: 0.8029\n",
      "Epoch 7/25 | Train Loss: 0.4500 Acc: 0.8102 | Val Loss: 0.5627 Acc: 0.6496\n",
      "Epoch 8/25 | Train Loss: 0.3925 Acc: 0.7701 | Val Loss: 0.4062 Acc: 0.7482\n",
      "Epoch 9/25 | Train Loss: 0.3451 Acc: 0.8212 | Val Loss: 0.3638 Acc: 0.8212\n",
      "Epoch 10/25 | Train Loss: 0.3300 Acc: 0.8613 | Val Loss: 0.3454 Acc: 0.8321\n",
      "Epoch 11/25 | Train Loss: 0.3456 Acc: 0.8321 | Val Loss: 0.3662 Acc: 0.8102\n",
      "Epoch 12/25 | Train Loss: 0.2920 Acc: 0.8686 | Val Loss: 0.3934 Acc: 0.7701\n",
      "Epoch 13/25 | Train Loss: 0.3079 Acc: 0.8613 | Val Loss: 0.3908 Acc: 0.7956\n",
      "Epoch 14/25 | Train Loss: 0.2832 Acc: 0.8686 | Val Loss: 0.3185 Acc: 0.8394\n",
      "Epoch 15/25 | Train Loss: 0.3360 Acc: 0.8431 | Val Loss: 0.3703 Acc: 0.8248\n",
      "Epoch 16/25 | Train Loss: 0.3095 Acc: 0.8577 | Val Loss: 0.3981 Acc: 0.8029\n",
      "Epoch 17/25 | Train Loss: 0.2941 Acc: 0.8577 | Val Loss: 0.4005 Acc: 0.7956\n",
      "Epoch 18/25 | Train Loss: 0.3138 Acc: 0.8504 | Val Loss: 0.3790 Acc: 0.8175\n",
      "Epoch 19/25 | Train Loss: 0.2969 Acc: 0.8686 | Val Loss: 0.3423 Acc: 0.8175\n",
      "Epoch 20/25 | Train Loss: 0.2940 Acc: 0.8613 | Val Loss: 0.3498 Acc: 0.7920\n",
      "Epoch 21/25 | Train Loss: 0.2917 Acc: 0.8686 | Val Loss: 0.3564 Acc: 0.8321\n",
      "Epoch 22/25 | Train Loss: 0.3059 Acc: 0.8504 | Val Loss: 0.3735 Acc: 0.8029\n",
      "Epoch 23/25 | Train Loss: 0.2915 Acc: 0.8942 | Val Loss: 0.4086 Acc: 0.7993\n",
      "Epoch 24/25 | Train Loss: 0.2932 Acc: 0.8650 | Val Loss: 0.3607 Acc: 0.8066\n",
      "Epoch 25/25 | Train Loss: 0.2755 Acc: 0.8796 | Val Loss: 0.3655 Acc: 0.8102\n",
      "Fold 9 Test Accuracy: 0.8212\n",
      "===== Fold 10 =====\n",
      "Epoch 1/25 | Train Loss: 0.5312 Acc: 0.7226 | Val Loss: 0.3983 Acc: 0.7956\n",
      "Epoch 2/25 | Train Loss: 0.4791 Acc: 0.7628 | Val Loss: 0.4231 Acc: 0.7737\n",
      "Epoch 3/25 | Train Loss: 0.4075 Acc: 0.7701 | Val Loss: 0.4226 Acc: 0.7883\n",
      "Epoch 4/25 | Train Loss: 0.4516 Acc: 0.7591 | Val Loss: 0.4031 Acc: 0.8066\n",
      "Epoch 5/25 | Train Loss: 0.5066 Acc: 0.7445 | Val Loss: 0.3936 Acc: 0.7664\n",
      "Epoch 6/25 | Train Loss: 0.4858 Acc: 0.7591 | Val Loss: 0.3572 Acc: 0.8066\n",
      "Epoch 7/25 | Train Loss: 0.3910 Acc: 0.8139 | Val Loss: 0.4036 Acc: 0.7628\n",
      "Epoch 8/25 | Train Loss: 0.3362 Acc: 0.8285 | Val Loss: 0.3829 Acc: 0.8175\n",
      "Epoch 9/25 | Train Loss: 0.3437 Acc: 0.8321 | Val Loss: 0.3891 Acc: 0.8175\n",
      "Epoch 10/25 | Train Loss: 0.3676 Acc: 0.8467 | Val Loss: 0.3637 Acc: 0.8285\n",
      "Epoch 11/25 | Train Loss: 0.3439 Acc: 0.8686 | Val Loss: 0.3723 Acc: 0.8029\n",
      "Epoch 12/25 | Train Loss: 0.3317 Acc: 0.8358 | Val Loss: 0.3556 Acc: 0.8358\n",
      "Epoch 13/25 | Train Loss: 0.3252 Acc: 0.8650 | Val Loss: 0.3687 Acc: 0.8212\n",
      "Epoch 14/25 | Train Loss: 0.3383 Acc: 0.8504 | Val Loss: 0.3621 Acc: 0.8066\n",
      "Epoch 15/25 | Train Loss: 0.2913 Acc: 0.8978 | Val Loss: 0.3624 Acc: 0.8212\n",
      "Epoch 16/25 | Train Loss: 0.3217 Acc: 0.8394 | Val Loss: 0.3685 Acc: 0.8102\n",
      "Epoch 17/25 | Train Loss: 0.3377 Acc: 0.8285 | Val Loss: 0.3669 Acc: 0.8358\n",
      "Epoch 18/25 | Train Loss: 0.3219 Acc: 0.8613 | Val Loss: 0.3612 Acc: 0.8285\n",
      "Epoch 19/25 | Train Loss: 0.3216 Acc: 0.8321 | Val Loss: 0.3380 Acc: 0.8467\n",
      "Epoch 20/25 | Train Loss: 0.3142 Acc: 0.8504 | Val Loss: 0.3585 Acc: 0.8175\n",
      "Epoch 21/25 | Train Loss: 0.3265 Acc: 0.8431 | Val Loss: 0.3625 Acc: 0.8139\n",
      "Epoch 22/25 | Train Loss: 0.3278 Acc: 0.8431 | Val Loss: 0.3724 Acc: 0.8175\n",
      "Epoch 23/25 | Train Loss: 0.3162 Acc: 0.8358 | Val Loss: 0.3691 Acc: 0.8139\n",
      "Epoch 24/25 | Train Loss: 0.3140 Acc: 0.8650 | Val Loss: 0.3506 Acc: 0.8248\n",
      "Epoch 25/25 | Train Loss: 0.3131 Acc: 0.8686 | Val Loss: 0.3684 Acc: 0.8139\n",
      "Fold 10 Test Accuracy: 0.8285\n",
      "\n",
      "5×2 CV results: Mean Acc = 0.8263, Std Dev = 0.0225\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    trainer = ResNetTrainer()\n",
    "\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        val_loader=test_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_best_to=f\"res_net_model/over_oversampled_dataset/VAE/best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "    )\n",
    "\n",
    "    trainer.load_model(f\"res_net_model/over_oversampled_dataset/VAE/best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "    _, acc = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    all_fold_results.append(acc)\n",
    "\n",
    "mean_acc = np.mean(all_fold_results)\n",
    "std_acc  = np.std(all_fold_results, ddof=1)\n",
    "print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T19:03:16.686933Z",
     "start_time": "2025-05-18T17:42:01.037349Z"
    }
   },
   "id": "9e13a078e484153c",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffeb30fe25b0420b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def res_net_training(output_dir, oversampler=None, dataset_type='original', epochs=25, batch_size=16, n_splits=2, n_repeats=5, random_state=42):\n",
    "    path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "    # download or mount your Kaggle data however you like; suppose:\n",
    "    # path = \".../capsule-defects\"\n",
    "    pos_folder = os.path.join(path, \"capsule/positive\")\n",
    "    neg_folder = os.path.join(path, \"capsule/negative\")\n",
    "    \n",
    "    max_per_dir = None\n",
    "\n",
    "    if oversampler==None:\n",
    "        neg_dirs=[neg_folder]\n",
    "        save_to=\"\"\n",
    "    else:\n",
    "        sample_folder = f'generated_images/{oversampler}'\n",
    "        save_to=f\"{oversampler}/\"\n",
    "        if dataset_type == 'oversampled':\n",
    "            neg_dirs=[neg_folder, sample_folder]\n",
    "            max_per_dir=110\n",
    "        elif dataset_type == 'over_oversampled':\n",
    "            neg_dirs=[neg_folder, sample_folder]\n",
    "            max_per_dir=220\n",
    "        elif dataset_type == 'synthetic':\n",
    "            neg_dirs=[sample_folder]\n",
    "            max_per_dir=219\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset type.\")\n",
    "    \n",
    "    dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_dirs, max_per_dir=max_per_dir, transform=transform)\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "    rskf = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    all_fold_results = []\n",
    "    \n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "        print(f\"===== Fold {fold_idx} =====\")\n",
    "    \n",
    "        # Subset + DataLoader\n",
    "        train_ds = Subset(dataset, train_idx)\n",
    "        test_ds  = Subset(dataset, test_idx)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        trainer = ResNetTrainer()\n",
    "    \n",
    "        trainer.train(\n",
    "            train_loader,\n",
    "            val_loader=test_loader,\n",
    "            num_epochs=epochs,\n",
    "            save_best_to=f\"{output_dir}{dataset_type}_dataset/{save_to}best_fold_{fold_idx}.pth\" # UWAGA na path\n",
    "        )\n",
    "    \n",
    "        trainer.load_model(f\"{output_dir}{dataset_type}_dataset/{save_to}best_fold_{fold_idx}.pth\") # UWAGA na path\n",
    "        _, acc = trainer.validate(test_loader)\n",
    "        print(f\"Fold {fold_idx} Test Accuracy: {acc:.4f}\")\n",
    "    \n",
    "        all_fold_results.append(acc)\n",
    "    \n",
    "    mean_acc = np.mean(all_fold_results)\n",
    "    std_acc  = np.std(all_fold_results, ddof=1)\n",
    "    print(f\"\\n5×2 CV results: Mean Acc = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef2038393f71cf3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "oversamplers = [\n",
    "    'CNNGAN',\n",
    "    'GAN',\n",
    "    'CNNVAE',\n",
    "    'VAE'\n",
    "]\n",
    "\n",
    "dataset_types = [\n",
    "    'oversampled',\n",
    "    'synthetic',\n",
    "    'over_oversampled',\n",
    "]\n",
    "\n",
    "# original dataset\n",
    "res_net_training('res_net_model/')\n",
    "\n",
    "# other datasets\n",
    "for oversampler in oversamplers:\n",
    "    for dataset_type in dataset_types:\n",
    "        res_net_training('res_net_model/', oversampler=oversampler, dataset_type=dataset_type)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5cd4f801f099b02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
