{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "id": "b5b45ce66797d3e3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T19:07:03.494262Z",
     "start_time": "2025-05-28T19:06:59.651353Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import  RepeatedStratifiedKFold\n",
    "import kagglehub\n",
    "from ResNet34 import ResNetTrainer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from DatasetClasses import CapsuleDataset\n",
    "from wrappers import CNNVAEResNetEstimator,CNNVAEWrapper, CNNGANWrapper, CNNGANResNetEstimator"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawid/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No VAE checkpoint found.\n",
      "[INFO] No generator checkpoint found.\n",
      "[INFO] No discriminator checkpoint found.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "1f4ed156edf08d68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T19:07:03.547064Z",
     "start_time": "2025-05-28T19:07:03.495858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parametry modelu\n",
    "IMG_SIZE = 128\n",
    "CHANNELS = 3\n",
    "LATENT_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "# Konfiguracja\n",
    "result_dir = 'results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/GAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNGAN'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/VAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "result_dir = 'results/CNNVAE'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(f\"Training device: {device}\")"
   ],
   "id": "a9b7e818fa133d69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T19:07:03.568614Z",
     "start_time": "2025-05-28T19:07:03.547808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=2,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "d01b8d11acc24a78",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset preparing"
   ],
   "id": "d786c4936b0c30fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T19:07:04.857649Z",
     "start_time": "2025-05-28T19:07:03.569383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# your augmentations + normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05,0.05), scale=(1.1,1.15), fill=255),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])   \n",
    "\n",
    "path = kagglehub.dataset_download(\"tladilebohang/capsule-defects\")\n",
    "# download or mount your Kaggle data however you like; suppose:\n",
    "# path = \".../capsule-defects\"\n",
    "pos_folder = os.path.join(path, \"capsule/positive\")\n",
    "neg_folder = os.path.join(path, \"capsule/negative\")\n",
    "pos_len=len(glob.glob(os.path.join(pos_folder, \"*\")))\n",
    "print(pos_len)\n",
    "neg_len=len(glob.glob(os.path.join(neg_folder, \"*\")))\n",
    "print(neg_len)"
   ],
   "id": "f4f67cd272e76c4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "219\n",
      "109\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Eksperyment na datasecie bez oversamplingu"
   ],
   "id": "229a9ecbb8592ea8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:02:37.125127Z",
     "start_time": "2025-05-24T18:59:04.380062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "EPOCHS = 25\n",
    "\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())\n",
    "\n",
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    # Subset + DataLoader\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    test_ds  = Subset(dataset, test_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "    trainer = ResNetTrainer()\n",
    "    \n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "    )\n",
    "\n",
    "    f2, bal_acc, recall, specificity = trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f2_score': f2,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity\n",
    "    })\n",
    "\n",
    "#Zapis wyników z każdego folda\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('without_oversampling_cross_validation_results.csv', index=False)\n"
   ],
   "id": "f168eff86851b0f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "===== Fold 1 =====\n",
      "Epoch 1/25 | Train Loss: 0.7442 Acc: 0.6336\n",
      "Epoch 2/25 | Train Loss: 0.6194 Acc: 0.7023\n",
      "Epoch 3/25 | Train Loss: 0.6195 Acc: 0.6908\n",
      "Epoch 4/25 | Train Loss: 0.5723 Acc: 0.7176\n",
      "Epoch 5/25 | Train Loss: 0.4970 Acc: 0.7786\n",
      "Epoch 6/25 | Train Loss: 0.6026 Acc: 0.7290\n",
      "Epoch 7/25 | Train Loss: 0.5569 Acc: 0.7366\n",
      "Epoch 8/25 | Train Loss: 0.4960 Acc: 0.7672\n",
      "Epoch 9/25 | Train Loss: 0.4983 Acc: 0.7557\n",
      "Epoch 10/25 | Train Loss: 0.4775 Acc: 0.7672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m test_loader  \u001B[38;5;241m=\u001B[39m DataLoader(test_ds,  batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     21\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ResNetTrainer()\n\u001B[0;32m---> 23\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m f2, bal_acc, recall, specificity \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mvalidate(test_loader)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Test Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbal_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/ResNet34.py:82\u001B[0m, in \u001B[0;36mResNetTrainer.train\u001B[0;34m(self, train_loader, num_epochs)\u001B[0m\n\u001B[1;32m     79\u001B[0m inputs_resnet \u001B[38;5;241m=\u001B[39m (inputs \u001B[38;5;241m-\u001B[39m imagenet_mean) \u001B[38;5;241m/\u001B[39m imagenet_std\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 82\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_resnet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n\u001B[1;32m     84\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:273\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    270\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m    271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[0;32m--> 273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[1;32m    275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:102\u001B[0m, in \u001B[0;36mBasicBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     identity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample(x)\n\u001B[0;32m--> 102\u001B[0m out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m identity\n\u001B[1;32m    103\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNNVAE: Ekperyment "
   ],
   "id": "3b97200ae874d707"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:57:48.336343Z",
     "start_time": "2025-05-24T12:48:51.259682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "CLASSIFIER_EPOCHS = 25\n",
    "OVERSAMPLER_EPOCHS = 200\n",
    "\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())\n",
    "\n",
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "    \n",
    "    CnnVae = CNNVAEWrapper(dataset,device,BATCH_SIZE,OVERSAMPLER_EPOCHS)\n",
    "    \n",
    "    CnnVae.fit(train_idx)\n",
    "\n",
    "\n",
    "    estimator = CNNVAEResNetEstimator(\n",
    "        dataset=dataset,\n",
    "        vae_model=CnnVae.vae_model,\n",
    "        device=device,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classifier_epochs=CLASSIFIER_EPOCHS\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'mu_multiplier': [0.8, 1.2],\n",
    "        'logvar_multiplier': [0.5, 1.5],\n",
    "        'multiplier_generated_samples': [1/2,1, 2]\n",
    "    }\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=8, \n",
    "        cv=None,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(train_idx)\n",
    "\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    test_ds = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    f2, bal_acc, recall, specificity = best_estimator.trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f2_score': f2,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        **best_params  \n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Zapis wyników z każdego folda\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('CNNVAE_cross_validation_results.csv', index=False)\n",
    "    "
   ],
   "id": "d20b162d6135eac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "===== Fold 1 =====\n",
      "Train Epoch: 1 | Loss: 2304.4632\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/1 | Train Loss: 0.5439 Acc: 0.6841\n",
      "[CV] END logvar_multiplier=0.5, mu_multiplier=1.2, multiplier_generated_samples=2; total time=  54.1s\n",
      "Epoch 1/1 | Train Loss: 0.5428 Acc: 0.6736\n",
      "[CV] END logvar_multiplier=0.5, mu_multiplier=1.2, multiplier_generated_samples=2; total time=  54.5s\n",
      "Epoch 1/1 | Train Loss: 0.4985 Acc: 0.7526\n",
      "[CV] END logvar_multiplier=0.5, mu_multiplier=1.2, multiplier_generated_samples=2; total time=  56.1s\n",
      "Epoch 1/1 | Train Loss: 0.6003 Acc: 0.7580\n",
      "[CV] END logvar_multiplier=0.5, mu_multiplier=1.2, multiplier_generated_samples=2; total time=  48.9s\n",
      "Epoch 1/1 | Train Loss: 0.4814 Acc: 0.8107\n",
      "[CV] END logvar_multiplier=0.5, mu_multiplier=1.2, multiplier_generated_samples=2; total time=  43.4s\n",
      "Epoch 1/1 | Train Loss: 0.7441 Acc: 0.5652\n",
      "[CV] END logvar_multiplier=1.0, mu_multiplier=1.0, multiplier_generated_samples=0.5; total time=  40.4s\n",
      "Epoch 1/1 | Train Loss: 0.6404 Acc: 0.6047\n",
      "[CV] END logvar_multiplier=1.0, mu_multiplier=1.0, multiplier_generated_samples=0.5; total time=  40.1s\n",
      "Epoch 1/1 | Train Loss: 0.7743 Acc: 0.6024\n",
      "[CV] END logvar_multiplier=1.0, mu_multiplier=1.0, multiplier_generated_samples=0.5; total time=  41.2s\n",
      "Epoch 1/1 | Train Loss: 0.6172 Acc: 0.7119\n",
      "[CV] END logvar_multiplier=1.0, mu_multiplier=1.0, multiplier_generated_samples=0.5; total time=  40.1s\n",
      "Epoch 1/1 | Train Loss: 0.5543 Acc: 0.7982\n",
      "[CV] END logvar_multiplier=1.0, mu_multiplier=1.0, multiplier_generated_samples=0.5; total time=  38.2s\n",
      "Epoch 1/1 | Train Loss: 0.5298 Acc: 0.7133\n",
      "Fold 1 Test Accuracy: 0.5455\n",
      "===== Fold 2 =====\n",
      "Train Epoch: 1 | Loss: 17866.6649\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[124], line 52\u001B[0m\n\u001B[1;32m     35\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmu_multiplier\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m1.0\u001B[39m, \u001B[38;5;241m1.2\u001B[39m],\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogvar_multiplier\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1.0\u001B[39m, \u001B[38;5;241m1.5\u001B[39m],\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultiplier_generated_samples\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m     39\u001B[0m }\n\u001B[1;32m     42\u001B[0m random_search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[1;32m     43\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[1;32m     44\u001B[0m     param_distributions\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     50\u001B[0m )\n\u001B[0;32m---> 52\u001B[0m \u001B[43mrandom_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m best_estimator \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m     55\u001B[0m best_params \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1020\u001B[0m     )\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1024\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1951\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1950\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1951\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1953\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m   1954\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1955\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    967\u001B[0m         )\n\u001B[1;32m    968\u001B[0m     )\n\u001B[0;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:864\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_train \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 864\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    866\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/wrappers.py:65\u001B[0m, in \u001B[0;36mCNNVAEResNetEstimator.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     61\u001B[0m num_neg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_neg_only_dataset)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_neg_only_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_neg_only_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvae_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_similar_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_neg_only_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmu_multiplier\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmu_multiplier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_multiplier\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogvar_multiplier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_neg\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultiplier_generated_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_dir\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m     74\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size)),\n\u001B[1;32m     75\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[1;32m     76\u001B[0m ])\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# 2. Łączymy dwa zbiory  w jeden duży treningowy\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/CNNVAE.py:209\u001B[0m, in \u001B[0;36mCNNVAE.generate_similar_data\u001B[0;34m(self, data, mu_multiplier, log_multiplier, num_samples, output_dir)\u001B[0m\n\u001B[1;32m    206\u001B[0m repeated_batches \u001B[38;5;241m=\u001B[39m itertools\u001B[38;5;241m.\u001B[39mcycle(data)  \u001B[38;5;66;03m# nieskończone batche\u001B[39;00m\n\u001B[1;32m    207\u001B[0m small_images \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 209\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m repeated_batches:\n\u001B[1;32m    210\u001B[0m     images \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)) \u001B[38;5;28;01melse\u001B[39;00m batch  \u001B[38;5;66;03m# tylko obrazy\u001B[39;00m\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/DatasetClasses.py:60\u001B[0m, in \u001B[0;36mNegativeOnlySubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 60\u001B[0m     img, _\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/DatasetClasses.py:44\u001B[0m, in \u001B[0;36mCapsuleDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     43\u001B[0m     path, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[idx]\n\u001B[0;32m---> 44\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/PIL/Image.py:922\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mconvert\u001B[39m(\n\u001B[1;32m    875\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m    876\u001B[0m ):\n\u001B[1;32m    877\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 922\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    924\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    926\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:291\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    290\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[0;32m--> 291\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNNGAN: Eksperyment"
   ],
   "id": "7b8b541aeeed5a11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T10:49:56.346416Z",
     "start_time": "2025-05-26T17:49:33.845712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "CLASSIFIER_EPOCHS = 15\n",
    "OVERSAMPLER_EPOCHS = 250\n",
    "\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())\n",
    "\n",
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "\n",
    "    CnnGan = CNNGANWrapper(dataset,device,BATCH_SIZE,OVERSAMPLER_EPOCHS)\n",
    "\n",
    "    CnnGan.fit(train_idx)\n",
    "\n",
    "\n",
    "    estimator = CNNGANResNetEstimator(\n",
    "        dataset=dataset,\n",
    "        gan_model=CnnGan.gan_model,\n",
    "        device=device,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classifier_epochs=CLASSIFIER_EPOCHS\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'scale_factor': [0.5, 1, 1.5],  # Czynnik skalujący szum w GAN\n",
    "        'multiplier_generated_samples': [1/2,1, 2]\n",
    "    }\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=6,\n",
    "        cv=None,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(train_idx)\n",
    "\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    test_ds = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    f2, bal_acc, recall, specificity = best_estimator.trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f2_score': f2,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        **best_params\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Zapis wyników z każdego folda\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('CNNGAN_cross_validation_results.csv', index=False)"
   ],
   "id": "88a95d341f05f041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "===== Fold 1 =====\n",
      "Epoch 1: Generator Loss = 10.0291, Discriminator Loss = 8.6379\n",
      "Epoch 2: Generator Loss = 10.9063, Discriminator Loss = 8.2331\n",
      "Epoch 3: Generator Loss = 12.0662, Discriminator Loss = 6.6395\n",
      "Epoch 4: Generator Loss = 18.0754, Discriminator Loss = 5.1761\n",
      "Epoch 5: Generator Loss = 26.0159, Discriminator Loss = 4.0308\n",
      "Epoch 6: Generator Loss = 29.1323, Discriminator Loss = 5.6949\n",
      "Epoch 7: Generator Loss = 24.4812, Discriminator Loss = 4.9665\n",
      "Epoch 8: Generator Loss = 25.7317, Discriminator Loss = 5.7915\n",
      "Epoch 9: Generator Loss = 28.1827, Discriminator Loss = 6.6236\n",
      "Epoch 10: Generator Loss = 21.3980, Discriminator Loss = 7.9283\n",
      "Epoch 11: Generator Loss = 21.2976, Discriminator Loss = 7.0885\n",
      "Epoch 12: Generator Loss = 23.1601, Discriminator Loss = 8.2236\n",
      "Epoch 13: Generator Loss = 21.8036, Discriminator Loss = 8.0966\n",
      "Epoch 14: Generator Loss = 17.0830, Discriminator Loss = 8.8234\n",
      "Epoch 15: Generator Loss = 18.3067, Discriminator Loss = 7.6943\n",
      "Epoch 16: Generator Loss = 19.3916, Discriminator Loss = 9.0201\n",
      "Epoch 17: Generator Loss = 19.1948, Discriminator Loss = 7.7673\n",
      "Epoch 18: Generator Loss = 19.6304, Discriminator Loss = 7.6315\n",
      "Epoch 19: Generator Loss = 16.6848, Discriminator Loss = 8.2855\n",
      "Epoch 20: Generator Loss = 15.3524, Discriminator Loss = 10.4062\n",
      "Epoch 21: Generator Loss = 15.2399, Discriminator Loss = 8.7773\n",
      "Epoch 22: Generator Loss = 15.9068, Discriminator Loss = 9.2248\n",
      "Epoch 23: Generator Loss = 16.5853, Discriminator Loss = 7.4367\n",
      "Epoch 24: Generator Loss = 16.4656, Discriminator Loss = 9.1164\n",
      "Epoch 25: Generator Loss = 16.0853, Discriminator Loss = 7.9322\n",
      "Epoch 26: Generator Loss = 19.8512, Discriminator Loss = 8.3070\n",
      "Epoch 27: Generator Loss = 17.9042, Discriminator Loss = 8.3935\n",
      "Epoch 28: Generator Loss = 13.9610, Discriminator Loss = 7.5902\n",
      "Epoch 29: Generator Loss = 18.2591, Discriminator Loss = 8.7058\n",
      "Epoch 30: Generator Loss = 20.1528, Discriminator Loss = 8.1060\n",
      "Epoch 31: Generator Loss = 17.6074, Discriminator Loss = 7.4509\n",
      "Epoch 32: Generator Loss = 19.0472, Discriminator Loss = 9.4773\n",
      "Epoch 33: Generator Loss = 17.0378, Discriminator Loss = 8.7214\n",
      "Epoch 34: Generator Loss = 16.8691, Discriminator Loss = 8.9727\n",
      "Epoch 35: Generator Loss = 16.8889, Discriminator Loss = 8.1842\n",
      "Epoch 36: Generator Loss = 16.3469, Discriminator Loss = 8.3357\n",
      "Epoch 37: Generator Loss = 19.1201, Discriminator Loss = 7.2696\n",
      "Epoch 38: Generator Loss = 14.9506, Discriminator Loss = 6.3311\n",
      "Epoch 39: Generator Loss = 19.7297, Discriminator Loss = 10.8511\n",
      "Epoch 40: Generator Loss = 16.9679, Discriminator Loss = 7.0461\n",
      "Epoch 41: Generator Loss = 16.6209, Discriminator Loss = 9.1410\n",
      "Epoch 42: Generator Loss = 18.8333, Discriminator Loss = 7.1115\n",
      "Epoch 43: Generator Loss = 18.5253, Discriminator Loss = 8.1634\n",
      "Epoch 44: Generator Loss = 17.7648, Discriminator Loss = 7.1836\n",
      "Epoch 45: Generator Loss = 18.2112, Discriminator Loss = 7.2580\n",
      "Epoch 46: Generator Loss = 19.3766, Discriminator Loss = 12.2904\n",
      "Epoch 47: Generator Loss = 14.4784, Discriminator Loss = 8.1671\n",
      "Epoch 48: Generator Loss = 13.9406, Discriminator Loss = 7.4152\n",
      "Epoch 49: Generator Loss = 16.8369, Discriminator Loss = 7.9070\n",
      "Epoch 50: Generator Loss = 21.1283, Discriminator Loss = 8.0550\n",
      "Epoch 51: Generator Loss = 13.6527, Discriminator Loss = 7.3488\n",
      "Epoch 52: Generator Loss = 21.5009, Discriminator Loss = 7.5095\n",
      "Epoch 53: Generator Loss = 16.5334, Discriminator Loss = 8.2896\n",
      "Epoch 54: Generator Loss = 20.5549, Discriminator Loss = 6.4749\n",
      "Epoch 55: Generator Loss = 20.7750, Discriminator Loss = 5.7902\n",
      "Epoch 56: Generator Loss = 20.1530, Discriminator Loss = 9.1446\n",
      "Epoch 57: Generator Loss = 18.4661, Discriminator Loss = 7.9743\n",
      "Epoch 58: Generator Loss = 18.8500, Discriminator Loss = 6.2726\n",
      "Epoch 59: Generator Loss = 21.9329, Discriminator Loss = 8.1498\n",
      "Epoch 60: Generator Loss = 18.8770, Discriminator Loss = 7.7439\n",
      "Epoch 61: Generator Loss = 18.9866, Discriminator Loss = 6.7179\n",
      "Epoch 62: Generator Loss = 23.4924, Discriminator Loss = 6.1639\n",
      "Epoch 63: Generator Loss = 20.0505, Discriminator Loss = 5.3405\n",
      "Epoch 64: Generator Loss = 28.1419, Discriminator Loss = 4.7471\n",
      "Epoch 65: Generator Loss = 27.5872, Discriminator Loss = 4.3863\n",
      "Epoch 66: Generator Loss = 31.5909, Discriminator Loss = 4.1108\n",
      "Epoch 67: Generator Loss = 31.1339, Discriminator Loss = 11.3530\n",
      "Epoch 68: Generator Loss = 21.0987, Discriminator Loss = 7.0311\n",
      "Epoch 69: Generator Loss = 22.4194, Discriminator Loss = 6.0259\n",
      "Epoch 70: Generator Loss = 25.8576, Discriminator Loss = 6.8951\n",
      "Epoch 71: Generator Loss = 24.0843, Discriminator Loss = 5.1304\n",
      "Epoch 72: Generator Loss = 27.3905, Discriminator Loss = 4.9616\n",
      "Epoch 73: Generator Loss = 27.6652, Discriminator Loss = 4.6553\n",
      "Epoch 74: Generator Loss = 32.4322, Discriminator Loss = 3.6535\n",
      "Epoch 75: Generator Loss = 32.7488, Discriminator Loss = 5.1302\n",
      "Epoch 76: Generator Loss = 32.4674, Discriminator Loss = 5.6472\n",
      "Epoch 77: Generator Loss = 28.7521, Discriminator Loss = 5.1432\n",
      "Epoch 78: Generator Loss = 29.1873, Discriminator Loss = 3.8945\n",
      "Epoch 79: Generator Loss = 30.2595, Discriminator Loss = 4.9868\n",
      "Epoch 80: Generator Loss = 36.1372, Discriminator Loss = 3.8407\n",
      "Epoch 81: Generator Loss = 32.9218, Discriminator Loss = 4.4051\n",
      "Epoch 82: Generator Loss = 36.5432, Discriminator Loss = 5.9359\n",
      "Epoch 83: Generator Loss = 35.8872, Discriminator Loss = 4.2004\n",
      "Epoch 84: Generator Loss = 37.8095, Discriminator Loss = 3.3580\n",
      "Epoch 85: Generator Loss = 35.0965, Discriminator Loss = 4.1616\n",
      "Epoch 86: Generator Loss = 35.6297, Discriminator Loss = 6.0030\n",
      "Epoch 87: Generator Loss = 32.9931, Discriminator Loss = 5.0699\n",
      "Epoch 88: Generator Loss = 36.7699, Discriminator Loss = 4.8719\n",
      "Epoch 89: Generator Loss = 38.0583, Discriminator Loss = 4.2570\n",
      "Epoch 90: Generator Loss = 48.0069, Discriminator Loss = 3.3524\n",
      "Epoch 91: Generator Loss = 42.8238, Discriminator Loss = 3.0443\n",
      "Epoch 92: Generator Loss = 29.2024, Discriminator Loss = 7.1283\n",
      "Epoch 93: Generator Loss = 38.5334, Discriminator Loss = 4.3085\n",
      "Epoch 94: Generator Loss = 42.2749, Discriminator Loss = 3.5006\n",
      "Epoch 95: Generator Loss = 50.5624, Discriminator Loss = 3.2437\n",
      "Epoch 96: Generator Loss = 12.3780, Discriminator Loss = 11.7616\n",
      "Epoch 97: Generator Loss = 11.8583, Discriminator Loss = 10.0420\n",
      "Epoch 98: Generator Loss = 14.3234, Discriminator Loss = 8.0034\n",
      "Epoch 99: Generator Loss = 22.8891, Discriminator Loss = 6.6091\n",
      "Epoch 100: Generator Loss = 28.3331, Discriminator Loss = 5.1046\n",
      "Epoch 101: Generator Loss = 35.4729, Discriminator Loss = 6.4352\n",
      "Epoch 102: Generator Loss = 40.3243, Discriminator Loss = 3.5146\n",
      "Epoch 103: Generator Loss = 45.1866, Discriminator Loss = 3.6458\n",
      "Epoch 104: Generator Loss = 40.0654, Discriminator Loss = 2.5781\n",
      "Epoch 105: Generator Loss = 57.2883, Discriminator Loss = 2.3476\n",
      "Epoch 106: Generator Loss = 46.1058, Discriminator Loss = 4.4255\n",
      "Epoch 107: Generator Loss = 44.1551, Discriminator Loss = 3.2458\n",
      "Epoch 108: Generator Loss = 32.7664, Discriminator Loss = 3.6062\n",
      "Epoch 109: Generator Loss = 52.4113, Discriminator Loss = 4.3091\n",
      "Epoch 110: Generator Loss = 46.7222, Discriminator Loss = 2.2292\n",
      "Epoch 111: Generator Loss = 51.9352, Discriminator Loss = 2.5948\n",
      "Epoch 112: Generator Loss = 40.3609, Discriminator Loss = 4.3825\n",
      "Epoch 113: Generator Loss = 50.9004, Discriminator Loss = 3.1361\n",
      "Epoch 114: Generator Loss = 52.6861, Discriminator Loss = 2.4273\n",
      "Epoch 115: Generator Loss = 49.8529, Discriminator Loss = 1.9588\n",
      "Epoch 116: Generator Loss = 51.5143, Discriminator Loss = 1.4007\n",
      "Epoch 117: Generator Loss = 51.8693, Discriminator Loss = 1.5176\n",
      "Epoch 118: Generator Loss = 49.8596, Discriminator Loss = 2.1688\n",
      "Epoch 119: Generator Loss = 59.0004, Discriminator Loss = 1.4691\n",
      "Epoch 120: Generator Loss = 48.6906, Discriminator Loss = 3.9582\n",
      "Epoch 121: Generator Loss = 60.1579, Discriminator Loss = 1.3322\n",
      "Epoch 122: Generator Loss = 54.9905, Discriminator Loss = 1.6592\n",
      "Epoch 123: Generator Loss = 47.1226, Discriminator Loss = 3.3697\n",
      "Epoch 124: Generator Loss = 48.9613, Discriminator Loss = 4.9381\n",
      "Epoch 125: Generator Loss = 48.7819, Discriminator Loss = 3.6559\n",
      "Epoch 126: Generator Loss = 53.7714, Discriminator Loss = 2.7771\n",
      "Epoch 127: Generator Loss = 41.6835, Discriminator Loss = 2.3014\n",
      "Epoch 128: Generator Loss = 61.6533, Discriminator Loss = 2.8660\n",
      "Epoch 129: Generator Loss = 51.8698, Discriminator Loss = 1.9763\n",
      "Epoch 130: Generator Loss = 51.9292, Discriminator Loss = 1.6386\n",
      "Epoch 131: Generator Loss = 61.4722, Discriminator Loss = 1.8228\n",
      "Epoch 132: Generator Loss = 41.4877, Discriminator Loss = 6.9487\n",
      "Epoch 133: Generator Loss = 52.2326, Discriminator Loss = 3.0655\n",
      "Epoch 134: Generator Loss = 40.1168, Discriminator Loss = 1.9162\n",
      "Epoch 135: Generator Loss = 62.1432, Discriminator Loss = 1.1868\n",
      "Epoch 136: Generator Loss = 58.7771, Discriminator Loss = 1.6487\n",
      "Epoch 137: Generator Loss = 48.9236, Discriminator Loss = 1.4749\n",
      "Epoch 138: Generator Loss = 54.1173, Discriminator Loss = 1.2662\n",
      "Epoch 139: Generator Loss = 61.9448, Discriminator Loss = 1.8370\n",
      "Epoch 140: Generator Loss = 60.2009, Discriminator Loss = 1.0826\n",
      "Epoch 141: Generator Loss = 60.4966, Discriminator Loss = 1.2344\n",
      "Epoch 142: Generator Loss = 75.5053, Discriminator Loss = 0.8517\n",
      "Epoch 143: Generator Loss = 65.6845, Discriminator Loss = 1.5758\n",
      "Epoch 144: Generator Loss = 63.4822, Discriminator Loss = 1.6132\n",
      "Epoch 145: Generator Loss = 58.4563, Discriminator Loss = 2.1266\n",
      "Epoch 146: Generator Loss = 76.4977, Discriminator Loss = 1.6940\n",
      "Epoch 147: Generator Loss = 53.4557, Discriminator Loss = 2.4648\n",
      "Epoch 148: Generator Loss = 65.8842, Discriminator Loss = 2.0699\n",
      "Epoch 149: Generator Loss = 54.3973, Discriminator Loss = 2.2569\n",
      "Epoch 150: Generator Loss = 63.0147, Discriminator Loss = 7.9915\n",
      "Epoch 151: Generator Loss = 24.7438, Discriminator Loss = 8.6090\n",
      "Epoch 152: Generator Loss = 35.2005, Discriminator Loss = 5.2262\n",
      "Epoch 153: Generator Loss = 48.3543, Discriminator Loss = 3.1968\n",
      "Epoch 154: Generator Loss = 55.6339, Discriminator Loss = 2.8744\n",
      "Epoch 155: Generator Loss = 53.2493, Discriminator Loss = 4.3478\n",
      "Epoch 156: Generator Loss = 44.7230, Discriminator Loss = 3.6824\n",
      "Epoch 157: Generator Loss = 67.8067, Discriminator Loss = 1.4879\n",
      "Epoch 158: Generator Loss = 54.3502, Discriminator Loss = 1.6667\n",
      "Epoch 159: Generator Loss = 64.6365, Discriminator Loss = 1.6168\n",
      "Epoch 160: Generator Loss = 52.0302, Discriminator Loss = 4.1322\n",
      "Epoch 161: Generator Loss = 78.1704, Discriminator Loss = 1.8719\n",
      "Epoch 162: Generator Loss = 56.1404, Discriminator Loss = 0.7598\n",
      "Epoch 163: Generator Loss = 69.4329, Discriminator Loss = 1.4692\n",
      "Epoch 164: Generator Loss = 65.7096, Discriminator Loss = 1.2861\n",
      "Epoch 165: Generator Loss = 76.9651, Discriminator Loss = 0.5982\n",
      "Epoch 166: Generator Loss = 81.3091, Discriminator Loss = 0.7986\n",
      "Epoch 167: Generator Loss = 70.3623, Discriminator Loss = 0.8454\n",
      "Epoch 168: Generator Loss = 64.7378, Discriminator Loss = 0.7972\n",
      "Epoch 169: Generator Loss = 66.5526, Discriminator Loss = 0.8080\n",
      "Epoch 170: Generator Loss = 83.2098, Discriminator Loss = 0.8359\n",
      "Epoch 171: Generator Loss = 70.9247, Discriminator Loss = 0.8089\n",
      "Epoch 172: Generator Loss = 73.4227, Discriminator Loss = 0.5180\n",
      "Epoch 173: Generator Loss = 114.5216, Discriminator Loss = 0.2605\n",
      "Epoch 174: Generator Loss = 104.5914, Discriminator Loss = 0.3099\n",
      "Epoch 175: Generator Loss = 106.4037, Discriminator Loss = 0.5151\n",
      "Epoch 176: Generator Loss = 96.0649, Discriminator Loss = 0.4154\n",
      "Epoch 177: Generator Loss = 71.2942, Discriminator Loss = 1.5424\n",
      "Epoch 178: Generator Loss = 78.1360, Discriminator Loss = 3.4897\n",
      "Epoch 179: Generator Loss = 73.1644, Discriminator Loss = 1.0580\n",
      "Epoch 180: Generator Loss = 68.9123, Discriminator Loss = 1.3120\n",
      "Epoch 181: Generator Loss = 79.7170, Discriminator Loss = 1.8683\n",
      "Epoch 182: Generator Loss = 91.3320, Discriminator Loss = 1.6892\n",
      "Epoch 183: Generator Loss = 66.4409, Discriminator Loss = 2.4433\n",
      "Epoch 184: Generator Loss = 80.4201, Discriminator Loss = 0.7560\n",
      "Epoch 185: Generator Loss = 83.5219, Discriminator Loss = 0.9098\n",
      "Epoch 186: Generator Loss = 71.1780, Discriminator Loss = 1.0553\n",
      "Epoch 187: Generator Loss = 81.3404, Discriminator Loss = 0.7634\n",
      "Epoch 188: Generator Loss = 79.2754, Discriminator Loss = 0.7241\n",
      "Epoch 189: Generator Loss = 78.8427, Discriminator Loss = 0.4973\n",
      "Epoch 190: Generator Loss = 70.6753, Discriminator Loss = 0.6366\n",
      "Epoch 191: Generator Loss = 71.8071, Discriminator Loss = 0.9083\n",
      "Epoch 192: Generator Loss = 68.5795, Discriminator Loss = 0.5123\n",
      "Epoch 193: Generator Loss = 78.3105, Discriminator Loss = 0.5428\n",
      "Epoch 194: Generator Loss = 75.0759, Discriminator Loss = 1.1801\n",
      "Epoch 195: Generator Loss = 78.3794, Discriminator Loss = 0.9140\n",
      "Epoch 196: Generator Loss = 86.5919, Discriminator Loss = 0.5304\n",
      "Epoch 197: Generator Loss = 74.6024, Discriminator Loss = 0.5505\n",
      "Epoch 198: Generator Loss = 93.4105, Discriminator Loss = 1.3759\n",
      "Epoch 199: Generator Loss = 66.2688, Discriminator Loss = 0.9437\n",
      "Epoch 200: Generator Loss = 77.3356, Discriminator Loss = 3.6818\n",
      "Epoch 201: Generator Loss = 70.6971, Discriminator Loss = 1.3541\n",
      "Epoch 202: Generator Loss = 74.7146, Discriminator Loss = 0.7123\n",
      "Epoch 203: Generator Loss = 78.2113, Discriminator Loss = 1.5233\n",
      "Epoch 204: Generator Loss = 65.1930, Discriminator Loss = 1.2352\n",
      "Epoch 205: Generator Loss = 68.4175, Discriminator Loss = 1.4660\n",
      "Epoch 206: Generator Loss = 88.3568, Discriminator Loss = 1.4479\n",
      "Epoch 207: Generator Loss = 85.7978, Discriminator Loss = 1.6402\n",
      "Epoch 208: Generator Loss = 83.2316, Discriminator Loss = 0.6183\n",
      "Epoch 209: Generator Loss = 90.7727, Discriminator Loss = 0.8830\n",
      "Epoch 210: Generator Loss = 88.1043, Discriminator Loss = 1.1542\n",
      "Epoch 211: Generator Loss = 91.1302, Discriminator Loss = 1.0328\n",
      "Epoch 212: Generator Loss = 76.5977, Discriminator Loss = 0.7506\n",
      "Epoch 213: Generator Loss = 83.8899, Discriminator Loss = 0.6322\n",
      "Epoch 214: Generator Loss = 83.7273, Discriminator Loss = 1.2854\n",
      "Epoch 215: Generator Loss = 71.6670, Discriminator Loss = 1.3671\n",
      "Epoch 216: Generator Loss = 82.1451, Discriminator Loss = 1.7297\n",
      "Epoch 217: Generator Loss = 86.6646, Discriminator Loss = 2.1248\n",
      "Epoch 218: Generator Loss = 83.9280, Discriminator Loss = 2.5697\n",
      "Epoch 219: Generator Loss = 69.5305, Discriminator Loss = 2.6021\n",
      "Epoch 220: Generator Loss = 86.5464, Discriminator Loss = 1.3548\n",
      "Epoch 221: Generator Loss = 72.2822, Discriminator Loss = 1.1538\n",
      "Epoch 222: Generator Loss = 85.4985, Discriminator Loss = 1.5485\n",
      "Epoch 223: Generator Loss = 80.9024, Discriminator Loss = 2.0488\n",
      "Epoch 224: Generator Loss = 94.7851, Discriminator Loss = 1.8967\n",
      "Epoch 225: Generator Loss = 74.2951, Discriminator Loss = 8.2955\n",
      "Epoch 226: Generator Loss = 29.3526, Discriminator Loss = 11.7863\n",
      "Epoch 227: Generator Loss = 63.4707, Discriminator Loss = 5.8986\n",
      "Epoch 228: Generator Loss = 69.7708, Discriminator Loss = 3.2241\n",
      "Epoch 229: Generator Loss = 66.5578, Discriminator Loss = 1.6726\n",
      "Epoch 230: Generator Loss = 61.9708, Discriminator Loss = 1.0125\n",
      "Epoch 231: Generator Loss = 65.7118, Discriminator Loss = 1.9548\n",
      "Epoch 232: Generator Loss = 74.2610, Discriminator Loss = 1.2243\n",
      "Epoch 233: Generator Loss = 56.1195, Discriminator Loss = 1.1964\n",
      "Epoch 234: Generator Loss = 79.8309, Discriminator Loss = 1.4553\n",
      "Epoch 235: Generator Loss = 73.0790, Discriminator Loss = 1.7836\n",
      "Epoch 236: Generator Loss = 72.3398, Discriminator Loss = 1.0321\n",
      "Epoch 237: Generator Loss = 84.3829, Discriminator Loss = 1.5753\n",
      "Epoch 238: Generator Loss = 76.9978, Discriminator Loss = 2.5971\n",
      "Epoch 239: Generator Loss = 69.9053, Discriminator Loss = 2.9269\n",
      "Epoch 240: Generator Loss = 69.2443, Discriminator Loss = 1.7183\n",
      "Epoch 241: Generator Loss = 66.8700, Discriminator Loss = 1.7170\n",
      "Epoch 242: Generator Loss = 64.8145, Discriminator Loss = 1.5079\n",
      "Epoch 243: Generator Loss = 66.7059, Discriminator Loss = 1.8336\n",
      "Epoch 244: Generator Loss = 64.6585, Discriminator Loss = 1.4483\n",
      "Epoch 245: Generator Loss = 64.2867, Discriminator Loss = 1.9974\n",
      "Epoch 246: Generator Loss = 76.8031, Discriminator Loss = 1.1350\n",
      "Epoch 247: Generator Loss = 84.3942, Discriminator Loss = 1.0150\n",
      "Epoch 248: Generator Loss = 76.6873, Discriminator Loss = 0.5110\n",
      "Epoch 249: Generator Loss = 93.3489, Discriminator Loss = 0.7930\n",
      "Epoch 250: Generator Loss = 74.8434, Discriminator Loss = 1.2886\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.4851 Acc: 0.7650\n",
      "Epoch 2/15 | Train Loss: 0.4148 Acc: 0.7859\n",
      "Epoch 3/15 | Train Loss: 0.3763 Acc: 0.7859\n",
      "Epoch 4/15 | Train Loss: 0.3335 Acc: 0.8251\n",
      "Epoch 5/15 | Train Loss: 0.3206 Acc: 0.8329\n",
      "Epoch 6/15 | Train Loss: 0.2955 Acc: 0.8616\n",
      "Epoch 7/15 | Train Loss: 0.3812 Acc: 0.8329\n",
      "Epoch 8/15 | Train Loss: 0.2793 Acc: 0.8538\n",
      "Epoch 9/15 | Train Loss: 0.2930 Acc: 0.8486\n",
      "Epoch 10/15 | Train Loss: 0.2928 Acc: 0.8642\n",
      "Epoch 11/15 | Train Loss: 0.2657 Acc: 0.8616\n",
      "Epoch 12/15 | Train Loss: 0.2468 Acc: 0.8877\n",
      "Epoch 13/15 | Train Loss: 0.2640 Acc: 0.8877\n",
      "Epoch 14/15 | Train Loss: 0.2644 Acc: 0.8721\n",
      "Epoch 15/15 | Train Loss: 0.2656 Acc: 0.8642\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.5919 Acc: 0.7598\n",
      "Epoch 2/15 | Train Loss: 0.4412 Acc: 0.7520\n",
      "Epoch 3/15 | Train Loss: 0.3866 Acc: 0.8172\n",
      "Epoch 4/15 | Train Loss: 0.3979 Acc: 0.7807\n",
      "Epoch 5/15 | Train Loss: 0.3915 Acc: 0.8042\n",
      "Epoch 6/15 | Train Loss: 0.3171 Acc: 0.8433\n",
      "Epoch 7/15 | Train Loss: 0.3354 Acc: 0.8225\n",
      "Epoch 8/15 | Train Loss: 0.3301 Acc: 0.8381\n",
      "Epoch 9/15 | Train Loss: 0.3104 Acc: 0.8433\n",
      "Epoch 10/15 | Train Loss: 0.3138 Acc: 0.8460\n",
      "Epoch 11/15 | Train Loss: 0.2960 Acc: 0.8329\n",
      "Epoch 12/15 | Train Loss: 0.3088 Acc: 0.8355\n",
      "Epoch 13/15 | Train Loss: 0.2934 Acc: 0.8590\n",
      "Epoch 14/15 | Train Loss: 0.2992 Acc: 0.8538\n",
      "Epoch 15/15 | Train Loss: 0.2646 Acc: 0.8747\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5347 Acc: 0.7292\n",
      "Epoch 2/15 | Train Loss: 0.3859 Acc: 0.7917\n",
      "Epoch 3/15 | Train Loss: 0.3994 Acc: 0.7630\n",
      "Epoch 4/15 | Train Loss: 0.3940 Acc: 0.7656\n",
      "Epoch 5/15 | Train Loss: 0.3349 Acc: 0.8229\n",
      "Epoch 6/15 | Train Loss: 0.3720 Acc: 0.7995\n",
      "Epoch 7/15 | Train Loss: 0.3274 Acc: 0.8073\n",
      "Epoch 8/15 | Train Loss: 0.3205 Acc: 0.8151\n",
      "Epoch 9/15 | Train Loss: 0.3296 Acc: 0.8411\n",
      "Epoch 10/15 | Train Loss: 0.2868 Acc: 0.8646\n",
      "Epoch 11/15 | Train Loss: 0.3109 Acc: 0.8411\n",
      "Epoch 12/15 | Train Loss: 0.3170 Acc: 0.8542\n",
      "Epoch 13/15 | Train Loss: 0.2915 Acc: 0.8646\n",
      "Epoch 14/15 | Train Loss: 0.2945 Acc: 0.8516\n",
      "Epoch 15/15 | Train Loss: 0.2824 Acc: 0.8620\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5520 Acc: 0.8089\n",
      "Epoch 2/15 | Train Loss: 0.4351 Acc: 0.7994\n",
      "Epoch 3/15 | Train Loss: 0.4757 Acc: 0.8089\n",
      "Epoch 4/15 | Train Loss: 0.4212 Acc: 0.8185\n",
      "Epoch 5/15 | Train Loss: 0.3869 Acc: 0.8217\n",
      "Epoch 6/15 | Train Loss: 0.3234 Acc: 0.8599\n",
      "Epoch 7/15 | Train Loss: 0.3316 Acc: 0.8599\n",
      "Epoch 8/15 | Train Loss: 0.3152 Acc: 0.8439\n",
      "Epoch 9/15 | Train Loss: 0.3015 Acc: 0.8662\n",
      "Epoch 10/15 | Train Loss: 0.3125 Acc: 0.8662\n",
      "Epoch 11/15 | Train Loss: 0.3342 Acc: 0.8535\n",
      "Epoch 12/15 | Train Loss: 0.2810 Acc: 0.8790\n",
      "Epoch 13/15 | Train Loss: 0.2788 Acc: 0.8949\n",
      "Epoch 14/15 | Train Loss: 0.2824 Acc: 0.8726\n",
      "Epoch 15/15 | Train Loss: 0.3101 Acc: 0.8790\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4316 Acc: 0.8429\n",
      "Epoch 2/15 | Train Loss: 0.3907 Acc: 0.8393\n",
      "Epoch 3/15 | Train Loss: 0.3962 Acc: 0.8536\n",
      "Epoch 4/15 | Train Loss: 0.3306 Acc: 0.9000\n",
      "Epoch 5/15 | Train Loss: 0.3207 Acc: 0.8714\n",
      "Epoch 6/15 | Train Loss: 0.3150 Acc: 0.8786\n",
      "Epoch 7/15 | Train Loss: 0.3159 Acc: 0.8821\n",
      "Epoch 8/15 | Train Loss: 0.2695 Acc: 0.8964\n",
      "Epoch 9/15 | Train Loss: 0.2753 Acc: 0.8750\n",
      "Epoch 10/15 | Train Loss: 0.2573 Acc: 0.9071\n",
      "Epoch 11/15 | Train Loss: 0.2593 Acc: 0.9143\n",
      "Epoch 12/15 | Train Loss: 0.2311 Acc: 0.9071\n",
      "Epoch 13/15 | Train Loss: 0.2469 Acc: 0.8964\n",
      "Epoch 14/15 | Train Loss: 0.2215 Acc: 0.8821\n",
      "Epoch 15/15 | Train Loss: 0.1933 Acc: 0.9107\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6571 Acc: 0.6151\n",
      "Epoch 2/15 | Train Loss: 0.6205 Acc: 0.6667\n",
      "Epoch 3/15 | Train Loss: 0.5510 Acc: 0.7222\n",
      "Epoch 4/15 | Train Loss: 0.5119 Acc: 0.7579\n",
      "Epoch 5/15 | Train Loss: 0.5176 Acc: 0.7341\n",
      "Epoch 6/15 | Train Loss: 0.4511 Acc: 0.7738\n",
      "Epoch 7/15 | Train Loss: 0.4164 Acc: 0.8016\n",
      "Epoch 8/15 | Train Loss: 0.4093 Acc: 0.7857\n",
      "Epoch 9/15 | Train Loss: 0.3935 Acc: 0.8016\n",
      "Epoch 10/15 | Train Loss: 0.3899 Acc: 0.8056\n",
      "Epoch 11/15 | Train Loss: 0.3620 Acc: 0.8214\n",
      "Epoch 12/15 | Train Loss: 0.3701 Acc: 0.8373\n",
      "Epoch 13/15 | Train Loss: 0.3656 Acc: 0.8254\n",
      "Epoch 14/15 | Train Loss: 0.3612 Acc: 0.8413\n",
      "Epoch 15/15 | Train Loss: 0.3804 Acc: 0.8135\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6636 Acc: 0.6429\n",
      "Epoch 2/15 | Train Loss: 0.5983 Acc: 0.6825\n",
      "Epoch 3/15 | Train Loss: 0.5917 Acc: 0.6468\n",
      "Epoch 4/15 | Train Loss: 0.5779 Acc: 0.6587\n",
      "Epoch 5/15 | Train Loss: 0.5198 Acc: 0.7143\n",
      "Epoch 6/15 | Train Loss: 0.5283 Acc: 0.7262\n",
      "Epoch 7/15 | Train Loss: 0.5107 Acc: 0.7103\n",
      "Epoch 8/15 | Train Loss: 0.4590 Acc: 0.7659\n",
      "Epoch 9/15 | Train Loss: 0.4271 Acc: 0.7897\n",
      "Epoch 10/15 | Train Loss: 0.4437 Acc: 0.7817\n",
      "Epoch 11/15 | Train Loss: 0.4238 Acc: 0.7976\n",
      "Epoch 12/15 | Train Loss: 0.4261 Acc: 0.8056\n",
      "Epoch 13/15 | Train Loss: 0.4156 Acc: 0.8056\n",
      "Epoch 14/15 | Train Loss: 0.3854 Acc: 0.8333\n",
      "Epoch 15/15 | Train Loss: 0.4011 Acc: 0.7937\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7081 Acc: 0.6364\n",
      "Epoch 2/15 | Train Loss: 0.5930 Acc: 0.6957\n",
      "Epoch 3/15 | Train Loss: 0.5308 Acc: 0.7273\n",
      "Epoch 4/15 | Train Loss: 0.5646 Acc: 0.6917\n",
      "Epoch 5/15 | Train Loss: 0.5964 Acc: 0.6719\n",
      "Epoch 6/15 | Train Loss: 0.4820 Acc: 0.7747\n",
      "Epoch 7/15 | Train Loss: 0.4887 Acc: 0.7431\n",
      "Epoch 8/15 | Train Loss: 0.4261 Acc: 0.7787\n",
      "Epoch 9/15 | Train Loss: 0.5119 Acc: 0.7628\n",
      "Epoch 10/15 | Train Loss: 0.4026 Acc: 0.7905\n",
      "Epoch 11/15 | Train Loss: 0.4162 Acc: 0.7984\n",
      "Epoch 12/15 | Train Loss: 0.4184 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.4279 Acc: 0.7866\n",
      "Epoch 14/15 | Train Loss: 0.4370 Acc: 0.7866\n",
      "Epoch 15/15 | Train Loss: 0.4976 Acc: 0.7628\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6591 Acc: 0.6992\n",
      "Epoch 2/15 | Train Loss: 0.5670 Acc: 0.7754\n",
      "Epoch 3/15 | Train Loss: 0.4700 Acc: 0.8093\n",
      "Epoch 4/15 | Train Loss: 0.5137 Acc: 0.7754\n",
      "Epoch 5/15 | Train Loss: 0.5187 Acc: 0.7627\n",
      "Epoch 6/15 | Train Loss: 0.4493 Acc: 0.7881\n",
      "Epoch 7/15 | Train Loss: 0.4360 Acc: 0.7966\n",
      "Epoch 8/15 | Train Loss: 0.3952 Acc: 0.8475\n",
      "Epoch 9/15 | Train Loss: 0.4303 Acc: 0.8220\n",
      "Epoch 10/15 | Train Loss: 0.4279 Acc: 0.8008\n",
      "Epoch 11/15 | Train Loss: 0.4154 Acc: 0.8051\n",
      "Epoch 12/15 | Train Loss: 0.3927 Acc: 0.8475\n",
      "Epoch 13/15 | Train Loss: 0.4024 Acc: 0.8051\n",
      "Epoch 14/15 | Train Loss: 0.4069 Acc: 0.8220\n",
      "Epoch 15/15 | Train Loss: 0.4319 Acc: 0.8051\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.8588 Acc: 0.5595\n",
      "Epoch 2/15 | Train Loss: 0.5691 Acc: 0.7753\n",
      "Epoch 3/15 | Train Loss: 0.4434 Acc: 0.8150\n",
      "Epoch 4/15 | Train Loss: 0.3616 Acc: 0.8634\n",
      "Epoch 5/15 | Train Loss: 0.3595 Acc: 0.8458\n",
      "Epoch 6/15 | Train Loss: 0.3934 Acc: 0.8458\n",
      "Epoch 7/15 | Train Loss: 0.3633 Acc: 0.8590\n",
      "Epoch 8/15 | Train Loss: 0.3343 Acc: 0.8722\n",
      "Epoch 9/15 | Train Loss: 0.3213 Acc: 0.8722\n",
      "Epoch 10/15 | Train Loss: 0.3100 Acc: 0.8767\n",
      "Epoch 11/15 | Train Loss: 0.2973 Acc: 0.8811\n",
      "Epoch 12/15 | Train Loss: 0.3093 Acc: 0.8855\n",
      "Epoch 13/15 | Train Loss: 0.2898 Acc: 0.8899\n",
      "Epoch 14/15 | Train Loss: 0.2915 Acc: 0.8811\n",
      "Epoch 15/15 | Train Loss: 0.2856 Acc: 0.8943\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6251 Acc: 0.6723\n",
      "Epoch 2/15 | Train Loss: 0.4784 Acc: 0.7297\n",
      "Epoch 3/15 | Train Loss: 0.4646 Acc: 0.7635\n",
      "Epoch 4/15 | Train Loss: 0.4204 Acc: 0.7973\n",
      "Epoch 5/15 | Train Loss: 0.4336 Acc: 0.7905\n",
      "Epoch 6/15 | Train Loss: 0.4176 Acc: 0.7905\n",
      "Epoch 7/15 | Train Loss: 0.3669 Acc: 0.8311\n",
      "Epoch 8/15 | Train Loss: 0.3480 Acc: 0.8209\n",
      "Epoch 9/15 | Train Loss: 0.2908 Acc: 0.8919\n",
      "Epoch 10/15 | Train Loss: 0.3310 Acc: 0.8480\n",
      "Epoch 11/15 | Train Loss: 0.3221 Acc: 0.8446\n",
      "Epoch 12/15 | Train Loss: 0.3607 Acc: 0.8345\n",
      "Epoch 13/15 | Train Loss: 0.3013 Acc: 0.8547\n",
      "Epoch 14/15 | Train Loss: 0.2774 Acc: 0.8682\n",
      "Epoch 15/15 | Train Loss: 0.3084 Acc: 0.8615\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6142 Acc: 0.6824\n",
      "Epoch 2/15 | Train Loss: 0.5735 Acc: 0.7162\n",
      "Epoch 3/15 | Train Loss: 0.5237 Acc: 0.7365\n",
      "Epoch 4/15 | Train Loss: 0.4395 Acc: 0.7703\n",
      "Epoch 5/15 | Train Loss: 0.4147 Acc: 0.7905\n",
      "Epoch 6/15 | Train Loss: 0.4785 Acc: 0.7399\n",
      "Epoch 7/15 | Train Loss: 0.4261 Acc: 0.7703\n",
      "Epoch 8/15 | Train Loss: 0.3897 Acc: 0.7872\n",
      "Epoch 9/15 | Train Loss: 0.3814 Acc: 0.8007\n",
      "Epoch 10/15 | Train Loss: 0.3844 Acc: 0.7973\n",
      "Epoch 11/15 | Train Loss: 0.4127 Acc: 0.7838\n",
      "Epoch 12/15 | Train Loss: 0.3223 Acc: 0.8750\n",
      "Epoch 13/15 | Train Loss: 0.3748 Acc: 0.8108\n",
      "Epoch 14/15 | Train Loss: 0.3400 Acc: 0.8345\n",
      "Epoch 15/15 | Train Loss: 0.3672 Acc: 0.8311\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6431 Acc: 0.6633\n",
      "Epoch 2/15 | Train Loss: 0.5243 Acc: 0.7104\n",
      "Epoch 3/15 | Train Loss: 0.4615 Acc: 0.7778\n",
      "Epoch 4/15 | Train Loss: 0.4619 Acc: 0.7576\n",
      "Epoch 5/15 | Train Loss: 0.4571 Acc: 0.7609\n",
      "Epoch 6/15 | Train Loss: 0.4155 Acc: 0.8047\n",
      "Epoch 7/15 | Train Loss: 0.4769 Acc: 0.7542\n",
      "Epoch 8/15 | Train Loss: 0.3866 Acc: 0.7946\n",
      "Epoch 9/15 | Train Loss: 0.3757 Acc: 0.7845\n",
      "Epoch 10/15 | Train Loss: 0.3424 Acc: 0.8485\n",
      "Epoch 11/15 | Train Loss: 0.3775 Acc: 0.8081\n",
      "Epoch 12/15 | Train Loss: 0.3444 Acc: 0.8249\n",
      "Epoch 13/15 | Train Loss: 0.3376 Acc: 0.8451\n",
      "Epoch 14/15 | Train Loss: 0.3857 Acc: 0.8182\n",
      "Epoch 15/15 | Train Loss: 0.3745 Acc: 0.8081\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5769 Acc: 0.7328\n",
      "Epoch 2/15 | Train Loss: 0.4602 Acc: 0.7977\n",
      "Epoch 3/15 | Train Loss: 0.4846 Acc: 0.7366\n",
      "Epoch 4/15 | Train Loss: 0.4861 Acc: 0.7939\n",
      "Epoch 5/15 | Train Loss: 0.4859 Acc: 0.7977\n",
      "Epoch 6/15 | Train Loss: 0.4462 Acc: 0.7824\n",
      "Epoch 7/15 | Train Loss: 0.4554 Acc: 0.7977\n",
      "Epoch 8/15 | Train Loss: 0.4162 Acc: 0.8015\n",
      "Epoch 9/15 | Train Loss: 0.3238 Acc: 0.8550\n",
      "Epoch 10/15 | Train Loss: 0.3520 Acc: 0.8397\n",
      "Epoch 11/15 | Train Loss: 0.3788 Acc: 0.8282\n",
      "Epoch 12/15 | Train Loss: 0.3519 Acc: 0.8740\n",
      "Epoch 13/15 | Train Loss: 0.3174 Acc: 0.8664\n",
      "Epoch 14/15 | Train Loss: 0.3404 Acc: 0.8435\n",
      "Epoch 15/15 | Train Loss: 0.3186 Acc: 0.8626\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5391 Acc: 0.7224\n",
      "Epoch 2/15 | Train Loss: 0.4771 Acc: 0.8163\n",
      "Epoch 3/15 | Train Loss: 0.3831 Acc: 0.8367\n",
      "Epoch 4/15 | Train Loss: 0.3175 Acc: 0.8571\n",
      "Epoch 5/15 | Train Loss: 0.3602 Acc: 0.8612\n",
      "Epoch 6/15 | Train Loss: 0.3311 Acc: 0.8490\n",
      "Epoch 7/15 | Train Loss: 0.3326 Acc: 0.8694\n",
      "Epoch 8/15 | Train Loss: 0.3282 Acc: 0.8653\n",
      "Epoch 9/15 | Train Loss: 0.2863 Acc: 0.8857\n",
      "Epoch 10/15 | Train Loss: 0.2644 Acc: 0.8857\n",
      "Epoch 11/15 | Train Loss: 0.2899 Acc: 0.8898\n",
      "Epoch 12/15 | Train Loss: 0.2657 Acc: 0.8898\n",
      "Epoch 13/15 | Train Loss: 0.3079 Acc: 0.8857\n",
      "Epoch 14/15 | Train Loss: 0.2389 Acc: 0.9184\n",
      "Epoch 15/15 | Train Loss: 0.2711 Acc: 0.8980\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6902 Acc: 0.6548\n",
      "Epoch 2/15 | Train Loss: 0.5828 Acc: 0.6786\n",
      "Epoch 3/15 | Train Loss: 0.4813 Acc: 0.7421\n",
      "Epoch 4/15 | Train Loss: 0.5259 Acc: 0.7302\n",
      "Epoch 5/15 | Train Loss: 0.4581 Acc: 0.7421\n",
      "Epoch 6/15 | Train Loss: 0.5064 Acc: 0.7817\n",
      "Epoch 7/15 | Train Loss: 0.4634 Acc: 0.7659\n",
      "Epoch 8/15 | Train Loss: 0.3985 Acc: 0.8056\n",
      "Epoch 9/15 | Train Loss: 0.4304 Acc: 0.7897\n",
      "Epoch 10/15 | Train Loss: 0.4111 Acc: 0.8135\n",
      "Epoch 11/15 | Train Loss: 0.3483 Acc: 0.8333\n",
      "Epoch 12/15 | Train Loss: 0.3630 Acc: 0.8492\n",
      "Epoch 13/15 | Train Loss: 0.3401 Acc: 0.8333\n",
      "Epoch 14/15 | Train Loss: 0.3618 Acc: 0.8452\n",
      "Epoch 15/15 | Train Loss: 0.3373 Acc: 0.8294\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7099 Acc: 0.6389\n",
      "Epoch 2/15 | Train Loss: 0.6311 Acc: 0.6865\n",
      "Epoch 3/15 | Train Loss: 0.5690 Acc: 0.7063\n",
      "Epoch 4/15 | Train Loss: 0.5626 Acc: 0.7222\n",
      "Epoch 5/15 | Train Loss: 0.5616 Acc: 0.7222\n",
      "Epoch 6/15 | Train Loss: 0.4796 Acc: 0.7381\n",
      "Epoch 7/15 | Train Loss: 0.4580 Acc: 0.7659\n",
      "Epoch 8/15 | Train Loss: 0.4104 Acc: 0.7976\n",
      "Epoch 9/15 | Train Loss: 0.4365 Acc: 0.7698\n",
      "Epoch 10/15 | Train Loss: 0.4261 Acc: 0.7976\n",
      "Epoch 11/15 | Train Loss: 0.4556 Acc: 0.7698\n",
      "Epoch 12/15 | Train Loss: 0.3930 Acc: 0.8135\n",
      "Epoch 13/15 | Train Loss: 0.4029 Acc: 0.8135\n",
      "Epoch 14/15 | Train Loss: 0.4209 Acc: 0.7698\n",
      "Epoch 15/15 | Train Loss: 0.3863 Acc: 0.8214\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7209 Acc: 0.6047\n",
      "Epoch 2/15 | Train Loss: 0.6351 Acc: 0.6403\n",
      "Epoch 3/15 | Train Loss: 0.5445 Acc: 0.6798\n",
      "Epoch 4/15 | Train Loss: 0.4822 Acc: 0.7589\n",
      "Epoch 5/15 | Train Loss: 0.5238 Acc: 0.7312\n",
      "Epoch 6/15 | Train Loss: 0.5118 Acc: 0.7352\n",
      "Epoch 7/15 | Train Loss: 0.4985 Acc: 0.7708\n",
      "Epoch 8/15 | Train Loss: 0.4479 Acc: 0.8142\n",
      "Epoch 9/15 | Train Loss: 0.4668 Acc: 0.7668\n",
      "Epoch 10/15 | Train Loss: 0.4814 Acc: 0.7391\n",
      "Epoch 11/15 | Train Loss: 0.3810 Acc: 0.8182\n",
      "Epoch 12/15 | Train Loss: 0.3789 Acc: 0.8103\n",
      "Epoch 13/15 | Train Loss: 0.4007 Acc: 0.8024\n",
      "Epoch 14/15 | Train Loss: 0.4826 Acc: 0.7826\n",
      "Epoch 15/15 | Train Loss: 0.4923 Acc: 0.7589\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7285 Acc: 0.6017\n",
      "Epoch 2/15 | Train Loss: 0.5721 Acc: 0.7585\n",
      "Epoch 3/15 | Train Loss: 0.5020 Acc: 0.7585\n",
      "Epoch 4/15 | Train Loss: 0.5001 Acc: 0.7924\n",
      "Epoch 5/15 | Train Loss: 0.4979 Acc: 0.7669\n",
      "Epoch 6/15 | Train Loss: 0.4331 Acc: 0.8093\n",
      "Epoch 7/15 | Train Loss: 0.4711 Acc: 0.7881\n",
      "Epoch 8/15 | Train Loss: 0.4632 Acc: 0.7712\n",
      "Epoch 9/15 | Train Loss: 0.4192 Acc: 0.8136\n",
      "Epoch 10/15 | Train Loss: 0.4325 Acc: 0.7924\n",
      "Epoch 11/15 | Train Loss: 0.4013 Acc: 0.8305\n",
      "Epoch 12/15 | Train Loss: 0.4248 Acc: 0.8093\n",
      "Epoch 13/15 | Train Loss: 0.4373 Acc: 0.8136\n",
      "Epoch 14/15 | Train Loss: 0.4276 Acc: 0.8093\n",
      "Epoch 15/15 | Train Loss: 0.3873 Acc: 0.8093\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5878 Acc: 0.6344\n",
      "Epoch 2/15 | Train Loss: 0.4901 Acc: 0.8238\n",
      "Epoch 3/15 | Train Loss: 0.3623 Acc: 0.8502\n",
      "Epoch 4/15 | Train Loss: 0.4418 Acc: 0.8370\n",
      "Epoch 5/15 | Train Loss: 0.4110 Acc: 0.8546\n",
      "Epoch 6/15 | Train Loss: 0.3370 Acc: 0.8502\n",
      "Epoch 7/15 | Train Loss: 0.3506 Acc: 0.8899\n",
      "Epoch 8/15 | Train Loss: 0.3305 Acc: 0.8767\n",
      "Epoch 9/15 | Train Loss: 0.2903 Acc: 0.8767\n",
      "Epoch 10/15 | Train Loss: 0.3275 Acc: 0.8678\n",
      "Epoch 11/15 | Train Loss: 0.2947 Acc: 0.8987\n",
      "Epoch 12/15 | Train Loss: 0.3050 Acc: 0.8943\n",
      "Epoch 13/15 | Train Loss: 0.3396 Acc: 0.8634\n",
      "Epoch 14/15 | Train Loss: 0.3080 Acc: 0.8987\n",
      "Epoch 15/15 | Train Loss: 0.2676 Acc: 0.8943\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5306 Acc: 0.7546\n",
      "Epoch 2/15 | Train Loss: 0.4153 Acc: 0.7702\n",
      "Epoch 3/15 | Train Loss: 0.3562 Acc: 0.8094\n",
      "Epoch 4/15 | Train Loss: 0.3402 Acc: 0.8407\n",
      "Epoch 5/15 | Train Loss: 0.3398 Acc: 0.8225\n",
      "Epoch 6/15 | Train Loss: 0.3342 Acc: 0.8381\n",
      "Epoch 7/15 | Train Loss: 0.3131 Acc: 0.8460\n",
      "Epoch 8/15 | Train Loss: 0.2954 Acc: 0.8668\n",
      "Epoch 9/15 | Train Loss: 0.2486 Acc: 0.8903\n",
      "Epoch 10/15 | Train Loss: 0.2541 Acc: 0.8930\n",
      "Epoch 11/15 | Train Loss: 0.2611 Acc: 0.8668\n",
      "Epoch 12/15 | Train Loss: 0.2496 Acc: 0.8721\n",
      "Epoch 13/15 | Train Loss: 0.2596 Acc: 0.8877\n",
      "Epoch 14/15 | Train Loss: 0.2341 Acc: 0.8956\n",
      "Epoch 15/15 | Train Loss: 0.2524 Acc: 0.8956\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4610 Acc: 0.7520\n",
      "Epoch 2/15 | Train Loss: 0.4271 Acc: 0.7702\n",
      "Epoch 3/15 | Train Loss: 0.4465 Acc: 0.7598\n",
      "Epoch 4/15 | Train Loss: 0.3812 Acc: 0.7728\n",
      "Epoch 5/15 | Train Loss: 0.3776 Acc: 0.7859\n",
      "Epoch 6/15 | Train Loss: 0.3200 Acc: 0.8433\n",
      "Epoch 7/15 | Train Loss: 0.3285 Acc: 0.8251\n",
      "Epoch 8/15 | Train Loss: 0.2884 Acc: 0.8642\n",
      "Epoch 9/15 | Train Loss: 0.2993 Acc: 0.8381\n",
      "Epoch 10/15 | Train Loss: 0.3049 Acc: 0.8512\n",
      "Epoch 11/15 | Train Loss: 0.3100 Acc: 0.8460\n",
      "Epoch 12/15 | Train Loss: 0.3141 Acc: 0.8303\n",
      "Epoch 13/15 | Train Loss: 0.2840 Acc: 0.8721\n",
      "Epoch 14/15 | Train Loss: 0.2831 Acc: 0.8721\n",
      "Epoch 15/15 | Train Loss: 0.2847 Acc: 0.8355\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6195 Acc: 0.7292\n",
      "Epoch 2/15 | Train Loss: 0.4650 Acc: 0.7682\n",
      "Epoch 3/15 | Train Loss: 0.4362 Acc: 0.7604\n",
      "Epoch 4/15 | Train Loss: 0.3874 Acc: 0.7995\n",
      "Epoch 5/15 | Train Loss: 0.3773 Acc: 0.8125\n",
      "Epoch 6/15 | Train Loss: 0.3459 Acc: 0.8203\n",
      "Epoch 7/15 | Train Loss: 0.3353 Acc: 0.8307\n",
      "Epoch 8/15 | Train Loss: 0.3020 Acc: 0.8568\n",
      "Epoch 9/15 | Train Loss: 0.2760 Acc: 0.8802\n",
      "Epoch 10/15 | Train Loss: 0.2978 Acc: 0.8438\n",
      "Epoch 11/15 | Train Loss: 0.3064 Acc: 0.8594\n",
      "Epoch 12/15 | Train Loss: 0.3014 Acc: 0.8672\n",
      "Epoch 13/15 | Train Loss: 0.2958 Acc: 0.8490\n",
      "Epoch 14/15 | Train Loss: 0.2755 Acc: 0.8672\n",
      "Epoch 15/15 | Train Loss: 0.2881 Acc: 0.8568\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5077 Acc: 0.7866\n",
      "Epoch 2/15 | Train Loss: 0.3650 Acc: 0.8344\n",
      "Epoch 3/15 | Train Loss: 0.3567 Acc: 0.8280\n",
      "Epoch 4/15 | Train Loss: 0.3818 Acc: 0.8280\n",
      "Epoch 5/15 | Train Loss: 0.3264 Acc: 0.8662\n",
      "Epoch 6/15 | Train Loss: 0.3742 Acc: 0.8153\n",
      "Epoch 7/15 | Train Loss: 0.3338 Acc: 0.8535\n",
      "Epoch 8/15 | Train Loss: 0.3521 Acc: 0.8344\n",
      "Epoch 9/15 | Train Loss: 0.3156 Acc: 0.8503\n",
      "Epoch 10/15 | Train Loss: 0.3179 Acc: 0.8758\n",
      "Epoch 11/15 | Train Loss: 0.2814 Acc: 0.8726\n",
      "Epoch 12/15 | Train Loss: 0.3586 Acc: 0.8408\n",
      "Epoch 13/15 | Train Loss: 0.2879 Acc: 0.8790\n",
      "Epoch 14/15 | Train Loss: 0.3116 Acc: 0.8726\n",
      "Epoch 15/15 | Train Loss: 0.3098 Acc: 0.8439\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5032 Acc: 0.7929\n",
      "Epoch 2/15 | Train Loss: 0.3572 Acc: 0.8607\n",
      "Epoch 3/15 | Train Loss: 0.3668 Acc: 0.8571\n",
      "Epoch 4/15 | Train Loss: 0.2955 Acc: 0.8821\n",
      "Epoch 5/15 | Train Loss: 0.3392 Acc: 0.8750\n",
      "Epoch 6/15 | Train Loss: 0.2859 Acc: 0.8893\n",
      "Epoch 7/15 | Train Loss: 0.3123 Acc: 0.8821\n",
      "Epoch 8/15 | Train Loss: 0.2812 Acc: 0.8714\n",
      "Epoch 9/15 | Train Loss: 0.2489 Acc: 0.8750\n",
      "Epoch 10/15 | Train Loss: 0.2379 Acc: 0.9036\n",
      "Epoch 11/15 | Train Loss: 0.2613 Acc: 0.9107\n",
      "Epoch 12/15 | Train Loss: 0.2430 Acc: 0.9036\n",
      "Epoch 13/15 | Train Loss: 0.2287 Acc: 0.8929\n",
      "Epoch 14/15 | Train Loss: 0.1999 Acc: 0.9286\n",
      "Epoch 15/15 | Train Loss: 0.2075 Acc: 0.9214\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6643 Acc: 0.6667\n",
      "Epoch 2/15 | Train Loss: 0.5556 Acc: 0.7143\n",
      "Epoch 3/15 | Train Loss: 0.5298 Acc: 0.6508\n",
      "Epoch 4/15 | Train Loss: 0.4787 Acc: 0.7579\n",
      "Epoch 5/15 | Train Loss: 0.4877 Acc: 0.7579\n",
      "Epoch 6/15 | Train Loss: 0.5150 Acc: 0.7500\n",
      "Epoch 7/15 | Train Loss: 0.3779 Acc: 0.8175\n",
      "Epoch 8/15 | Train Loss: 0.4388 Acc: 0.7778\n",
      "Epoch 9/15 | Train Loss: 0.4340 Acc: 0.8095\n",
      "Epoch 10/15 | Train Loss: 0.3803 Acc: 0.8214\n",
      "Epoch 11/15 | Train Loss: 0.3573 Acc: 0.8333\n",
      "Epoch 12/15 | Train Loss: 0.3358 Acc: 0.8452\n",
      "Epoch 13/15 | Train Loss: 0.3719 Acc: 0.8452\n",
      "Epoch 14/15 | Train Loss: 0.3970 Acc: 0.8016\n",
      "Epoch 15/15 | Train Loss: 0.3987 Acc: 0.8175\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7279 Acc: 0.6230\n",
      "Epoch 2/15 | Train Loss: 0.6598 Acc: 0.6627\n",
      "Epoch 3/15 | Train Loss: 0.5754 Acc: 0.6548\n",
      "Epoch 4/15 | Train Loss: 0.4917 Acc: 0.7579\n",
      "Epoch 5/15 | Train Loss: 0.4907 Acc: 0.7341\n",
      "Epoch 6/15 | Train Loss: 0.5091 Acc: 0.7183\n",
      "Epoch 7/15 | Train Loss: 0.4704 Acc: 0.7619\n",
      "Epoch 8/15 | Train Loss: 0.4277 Acc: 0.7817\n",
      "Epoch 9/15 | Train Loss: 0.4643 Acc: 0.7579\n",
      "Epoch 10/15 | Train Loss: 0.4408 Acc: 0.7421\n",
      "Epoch 11/15 | Train Loss: 0.4156 Acc: 0.7778\n",
      "Epoch 12/15 | Train Loss: 0.3926 Acc: 0.8016\n",
      "Epoch 13/15 | Train Loss: 0.4353 Acc: 0.8095\n",
      "Epoch 14/15 | Train Loss: 0.4130 Acc: 0.7937\n",
      "Epoch 15/15 | Train Loss: 0.4322 Acc: 0.8095\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.8115 Acc: 0.5810\n",
      "Epoch 2/15 | Train Loss: 0.6149 Acc: 0.6561\n",
      "Epoch 3/15 | Train Loss: 0.5650 Acc: 0.6877\n",
      "Epoch 4/15 | Train Loss: 0.5062 Acc: 0.6996\n",
      "Epoch 5/15 | Train Loss: 0.4884 Acc: 0.7549\n",
      "Epoch 6/15 | Train Loss: 0.4948 Acc: 0.7470\n",
      "Epoch 7/15 | Train Loss: 0.5042 Acc: 0.7470\n",
      "Epoch 8/15 | Train Loss: 0.4379 Acc: 0.7826\n",
      "Epoch 9/15 | Train Loss: 0.5236 Acc: 0.7312\n",
      "Epoch 10/15 | Train Loss: 0.5106 Acc: 0.7470\n",
      "Epoch 11/15 | Train Loss: 0.4554 Acc: 0.7391\n",
      "Epoch 12/15 | Train Loss: 0.4475 Acc: 0.8063\n",
      "Epoch 13/15 | Train Loss: 0.4300 Acc: 0.7708\n",
      "Epoch 14/15 | Train Loss: 0.4418 Acc: 0.7787\n",
      "Epoch 15/15 | Train Loss: 0.4210 Acc: 0.8261\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6990 Acc: 0.6483\n",
      "Epoch 2/15 | Train Loss: 0.5677 Acc: 0.7585\n",
      "Epoch 3/15 | Train Loss: 0.5265 Acc: 0.7669\n",
      "Epoch 4/15 | Train Loss: 0.4963 Acc: 0.7966\n",
      "Epoch 5/15 | Train Loss: 0.4484 Acc: 0.7966\n",
      "Epoch 6/15 | Train Loss: 0.4815 Acc: 0.7924\n",
      "Epoch 7/15 | Train Loss: 0.4233 Acc: 0.8051\n",
      "Epoch 8/15 | Train Loss: 0.4557 Acc: 0.8008\n",
      "Epoch 9/15 | Train Loss: 0.4317 Acc: 0.7839\n",
      "Epoch 10/15 | Train Loss: 0.3974 Acc: 0.8644\n",
      "Epoch 11/15 | Train Loss: 0.4232 Acc: 0.8136\n",
      "Epoch 12/15 | Train Loss: 0.3741 Acc: 0.8263\n",
      "Epoch 13/15 | Train Loss: 0.4197 Acc: 0.8263\n",
      "Epoch 14/15 | Train Loss: 0.3908 Acc: 0.8305\n",
      "Epoch 15/15 | Train Loss: 0.3597 Acc: 0.8305\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.4760 Acc: 0.7665\n",
      "Epoch 2/15 | Train Loss: 0.4449 Acc: 0.8150\n",
      "Epoch 3/15 | Train Loss: 0.4302 Acc: 0.8018\n",
      "Epoch 4/15 | Train Loss: 0.3770 Acc: 0.8414\n",
      "Epoch 5/15 | Train Loss: 0.4552 Acc: 0.8282\n",
      "Epoch 6/15 | Train Loss: 0.3455 Acc: 0.8546\n",
      "Epoch 7/15 | Train Loss: 0.3440 Acc: 0.8678\n",
      "Epoch 8/15 | Train Loss: 0.4283 Acc: 0.8062\n",
      "Epoch 9/15 | Train Loss: 0.3043 Acc: 0.8811\n",
      "Epoch 10/15 | Train Loss: 0.2873 Acc: 0.8767\n",
      "Epoch 11/15 | Train Loss: 0.3614 Acc: 0.8502\n",
      "Epoch 12/15 | Train Loss: 0.3026 Acc: 0.8855\n",
      "Epoch 13/15 | Train Loss: 0.3104 Acc: 0.8678\n",
      "Epoch 14/15 | Train Loss: 0.2681 Acc: 0.8899\n",
      "Epoch 15/15 | Train Loss: 0.3204 Acc: 0.8899\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6133 Acc: 0.7615\n",
      "Epoch 2/15 | Train Loss: 0.4704 Acc: 0.7959\n",
      "Epoch 3/15 | Train Loss: 0.4243 Acc: 0.7959\n",
      "Epoch 4/15 | Train Loss: 0.3599 Acc: 0.8188\n",
      "Epoch 5/15 | Train Loss: 0.3640 Acc: 0.8372\n",
      "Epoch 6/15 | Train Loss: 0.3441 Acc: 0.8463\n",
      "Epoch 7/15 | Train Loss: 0.3155 Acc: 0.8532\n",
      "Epoch 8/15 | Train Loss: 0.3092 Acc: 0.8555\n",
      "Epoch 9/15 | Train Loss: 0.2666 Acc: 0.8853\n",
      "Epoch 10/15 | Train Loss: 0.2695 Acc: 0.8899\n",
      "Epoch 11/15 | Train Loss: 0.2829 Acc: 0.8601\n",
      "Epoch 12/15 | Train Loss: 0.2942 Acc: 0.8532\n",
      "Epoch 13/15 | Train Loss: 0.2625 Acc: 0.8853\n",
      "Epoch 14/15 | Train Loss: 0.2687 Acc: 0.8922\n",
      "Epoch 15/15 | Train Loss: 0.3009 Acc: 0.8440\n",
      "Fold 1 Test Accuracy: 0.6818\n",
      "===== Fold 2 =====\n",
      "Epoch 1: Generator Loss = 9.8105, Discriminator Loss = 8.4732\n",
      "Epoch 2: Generator Loss = 9.6912, Discriminator Loss = 9.0555\n",
      "Epoch 3: Generator Loss = 10.9229, Discriminator Loss = 8.3284\n",
      "Epoch 4: Generator Loss = 17.1684, Discriminator Loss = 6.2105\n",
      "Epoch 5: Generator Loss = 25.3723, Discriminator Loss = 4.0716\n",
      "Epoch 6: Generator Loss = 29.3015, Discriminator Loss = 5.3107\n",
      "Epoch 7: Generator Loss = 27.7601, Discriminator Loss = 5.2750\n",
      "Epoch 8: Generator Loss = 31.9351, Discriminator Loss = 5.0160\n",
      "Epoch 9: Generator Loss = 26.6615, Discriminator Loss = 4.1994\n",
      "Epoch 10: Generator Loss = 26.7629, Discriminator Loss = 4.9672\n",
      "Epoch 11: Generator Loss = 27.2182, Discriminator Loss = 7.1660\n",
      "Epoch 12: Generator Loss = 23.9134, Discriminator Loss = 8.7410\n",
      "Epoch 13: Generator Loss = 22.6089, Discriminator Loss = 8.0364\n",
      "Epoch 14: Generator Loss = 19.0480, Discriminator Loss = 7.8345\n",
      "Epoch 15: Generator Loss = 20.9120, Discriminator Loss = 6.8547\n",
      "Epoch 16: Generator Loss = 18.3556, Discriminator Loss = 6.8300\n",
      "Epoch 17: Generator Loss = 23.6068, Discriminator Loss = 6.7289\n",
      "Epoch 18: Generator Loss = 15.8216, Discriminator Loss = 9.1742\n",
      "Epoch 19: Generator Loss = 23.7470, Discriminator Loss = 9.6529\n",
      "Epoch 20: Generator Loss = 20.1906, Discriminator Loss = 7.5683\n",
      "Epoch 21: Generator Loss = 21.0114, Discriminator Loss = 7.4788\n",
      "Epoch 22: Generator Loss = 18.1949, Discriminator Loss = 7.2499\n",
      "Epoch 23: Generator Loss = 17.9490, Discriminator Loss = 7.8338\n",
      "Epoch 24: Generator Loss = 22.9891, Discriminator Loss = 7.6119\n",
      "Epoch 25: Generator Loss = 21.2771, Discriminator Loss = 7.1750\n",
      "Epoch 26: Generator Loss = 20.8129, Discriminator Loss = 7.1766\n",
      "Epoch 27: Generator Loss = 19.9622, Discriminator Loss = 7.7123\n",
      "Epoch 28: Generator Loss = 19.9248, Discriminator Loss = 6.7222\n",
      "Epoch 29: Generator Loss = 22.4197, Discriminator Loss = 6.3165\n",
      "Epoch 30: Generator Loss = 21.3418, Discriminator Loss = 7.8581\n",
      "Epoch 31: Generator Loss = 21.9782, Discriminator Loss = 9.1651\n",
      "Epoch 32: Generator Loss = 22.7599, Discriminator Loss = 7.1641\n",
      "Epoch 33: Generator Loss = 18.6386, Discriminator Loss = 7.9598\n",
      "Epoch 34: Generator Loss = 21.6717, Discriminator Loss = 7.5718\n",
      "Epoch 35: Generator Loss = 22.4485, Discriminator Loss = 8.0733\n",
      "Epoch 36: Generator Loss = 23.1816, Discriminator Loss = 7.4819\n",
      "Epoch 37: Generator Loss = 18.4047, Discriminator Loss = 8.3132\n",
      "Epoch 38: Generator Loss = 19.3180, Discriminator Loss = 6.8735\n",
      "Epoch 39: Generator Loss = 21.0440, Discriminator Loss = 7.6227\n",
      "Epoch 40: Generator Loss = 18.4083, Discriminator Loss = 6.5658\n",
      "Epoch 41: Generator Loss = 23.4855, Discriminator Loss = 7.3174\n",
      "Epoch 42: Generator Loss = 18.9811, Discriminator Loss = 6.2876\n",
      "Epoch 43: Generator Loss = 29.3223, Discriminator Loss = 5.7714\n",
      "Epoch 44: Generator Loss = 17.1686, Discriminator Loss = 6.7573\n",
      "Epoch 45: Generator Loss = 23.4464, Discriminator Loss = 6.1763\n",
      "Epoch 46: Generator Loss = 22.5630, Discriminator Loss = 10.1140\n",
      "Epoch 47: Generator Loss = 17.5614, Discriminator Loss = 7.9075\n",
      "Epoch 48: Generator Loss = 21.1766, Discriminator Loss = 6.8143\n",
      "Epoch 49: Generator Loss = 19.2541, Discriminator Loss = 7.3255\n",
      "Epoch 50: Generator Loss = 21.3255, Discriminator Loss = 8.2024\n",
      "Epoch 51: Generator Loss = 21.1562, Discriminator Loss = 6.5252\n",
      "Epoch 52: Generator Loss = 26.4582, Discriminator Loss = 5.1201\n",
      "Epoch 53: Generator Loss = 28.9264, Discriminator Loss = 5.7548\n",
      "Epoch 54: Generator Loss = 28.1335, Discriminator Loss = 5.0921\n",
      "Epoch 55: Generator Loss = 31.3168, Discriminator Loss = 4.2999\n",
      "Epoch 56: Generator Loss = 34.0489, Discriminator Loss = 4.6280\n",
      "Epoch 57: Generator Loss = 29.0226, Discriminator Loss = 5.0169\n",
      "Epoch 58: Generator Loss = 39.0013, Discriminator Loss = 3.2155\n",
      "Epoch 59: Generator Loss = 31.9892, Discriminator Loss = 2.8598\n",
      "Epoch 60: Generator Loss = 32.4623, Discriminator Loss = 5.0190\n",
      "Epoch 61: Generator Loss = 43.5113, Discriminator Loss = 3.3369\n",
      "Epoch 62: Generator Loss = 28.3055, Discriminator Loss = 7.7047\n",
      "Epoch 63: Generator Loss = 38.7193, Discriminator Loss = 3.3408\n",
      "Epoch 64: Generator Loss = 41.1165, Discriminator Loss = 2.1470\n",
      "Epoch 65: Generator Loss = 50.4190, Discriminator Loss = 1.9357\n",
      "Epoch 66: Generator Loss = 38.2889, Discriminator Loss = 4.1853\n",
      "Epoch 67: Generator Loss = 42.8010, Discriminator Loss = 1.6643\n",
      "Epoch 68: Generator Loss = 55.0414, Discriminator Loss = 2.5697\n",
      "Epoch 69: Generator Loss = 58.1303, Discriminator Loss = 1.8017\n",
      "Epoch 70: Generator Loss = 51.0943, Discriminator Loss = 1.6414\n",
      "Epoch 71: Generator Loss = 50.8764, Discriminator Loss = 1.6304\n",
      "Epoch 72: Generator Loss = 53.4787, Discriminator Loss = 1.6339\n",
      "Epoch 73: Generator Loss = 59.9888, Discriminator Loss = 1.2066\n",
      "Epoch 74: Generator Loss = 59.1602, Discriminator Loss = 1.4366\n",
      "Epoch 75: Generator Loss = 64.5698, Discriminator Loss = 1.3996\n",
      "Epoch 76: Generator Loss = 64.6976, Discriminator Loss = 2.2138\n",
      "Epoch 77: Generator Loss = 60.8292, Discriminator Loss = 1.0215\n",
      "Epoch 78: Generator Loss = 56.1149, Discriminator Loss = 0.8878\n",
      "Epoch 79: Generator Loss = 60.5609, Discriminator Loss = 1.3156\n",
      "Epoch 80: Generator Loss = 60.8436, Discriminator Loss = 0.9959\n",
      "Epoch 81: Generator Loss = 63.3251, Discriminator Loss = 1.2403\n",
      "Epoch 82: Generator Loss = 60.8576, Discriminator Loss = 1.1464\n",
      "Epoch 83: Generator Loss = 62.3069, Discriminator Loss = 0.5354\n",
      "Epoch 84: Generator Loss = 61.9011, Discriminator Loss = 1.0670\n",
      "Epoch 85: Generator Loss = 66.4728, Discriminator Loss = 0.5506\n",
      "Epoch 86: Generator Loss = 65.5595, Discriminator Loss = 1.3642\n",
      "Epoch 87: Generator Loss = 57.2501, Discriminator Loss = 4.3373\n",
      "Epoch 88: Generator Loss = 56.5158, Discriminator Loss = 0.9695\n",
      "Epoch 89: Generator Loss = 65.7615, Discriminator Loss = 1.6123\n",
      "Epoch 90: Generator Loss = 68.9529, Discriminator Loss = 0.7240\n",
      "Epoch 91: Generator Loss = 67.4100, Discriminator Loss = 0.5860\n",
      "Epoch 92: Generator Loss = 78.0783, Discriminator Loss = 0.9128\n",
      "Epoch 93: Generator Loss = 58.9160, Discriminator Loss = 0.7015\n",
      "Epoch 94: Generator Loss = 67.0849, Discriminator Loss = 1.0568\n",
      "Epoch 95: Generator Loss = 66.2449, Discriminator Loss = 2.1201\n",
      "Epoch 96: Generator Loss = 71.8882, Discriminator Loss = 0.9033\n",
      "Epoch 97: Generator Loss = 71.7210, Discriminator Loss = 1.9324\n",
      "Epoch 98: Generator Loss = 55.9999, Discriminator Loss = 0.8389\n",
      "Epoch 99: Generator Loss = 66.7376, Discriminator Loss = 1.2355\n",
      "Epoch 100: Generator Loss = 67.5477, Discriminator Loss = 0.6474\n",
      "Epoch 101: Generator Loss = 68.9080, Discriminator Loss = 0.7012\n",
      "Epoch 102: Generator Loss = 78.0627, Discriminator Loss = 0.7798\n",
      "Epoch 103: Generator Loss = 67.0787, Discriminator Loss = 0.5442\n",
      "Epoch 104: Generator Loss = 79.3442, Discriminator Loss = 0.5527\n",
      "Epoch 105: Generator Loss = 87.7069, Discriminator Loss = 0.4499\n",
      "Epoch 106: Generator Loss = 86.0198, Discriminator Loss = 0.2727\n",
      "Epoch 107: Generator Loss = 84.0439, Discriminator Loss = 0.7475\n",
      "Epoch 108: Generator Loss = 82.0418, Discriminator Loss = 0.9368\n",
      "Epoch 109: Generator Loss = 72.7638, Discriminator Loss = 0.7741\n",
      "Epoch 110: Generator Loss = 77.2745, Discriminator Loss = 0.7314\n",
      "Epoch 111: Generator Loss = 80.8309, Discriminator Loss = 1.4945\n",
      "Epoch 112: Generator Loss = 85.4842, Discriminator Loss = 1.6016\n",
      "Epoch 113: Generator Loss = 73.7666, Discriminator Loss = 0.6152\n",
      "Epoch 114: Generator Loss = 68.7014, Discriminator Loss = 0.7855\n",
      "Epoch 115: Generator Loss = 78.1717, Discriminator Loss = 1.4888\n",
      "Epoch 116: Generator Loss = 67.1743, Discriminator Loss = 0.6837\n",
      "Epoch 117: Generator Loss = 78.8429, Discriminator Loss = 0.4538\n",
      "Epoch 118: Generator Loss = 88.3927, Discriminator Loss = 6.1778\n",
      "Epoch 119: Generator Loss = 51.1829, Discriminator Loss = 7.4044\n",
      "Epoch 120: Generator Loss = 66.6366, Discriminator Loss = 2.2025\n",
      "Epoch 121: Generator Loss = 56.0055, Discriminator Loss = 1.4542\n",
      "Epoch 122: Generator Loss = 64.1339, Discriminator Loss = 3.0654\n",
      "Epoch 123: Generator Loss = 52.5331, Discriminator Loss = 1.3012\n",
      "Epoch 124: Generator Loss = 64.9747, Discriminator Loss = 1.4453\n",
      "Epoch 125: Generator Loss = 67.2204, Discriminator Loss = 0.9397\n",
      "Epoch 126: Generator Loss = 66.8052, Discriminator Loss = 0.5721\n",
      "Epoch 127: Generator Loss = 78.3189, Discriminator Loss = 0.9489\n",
      "Epoch 128: Generator Loss = 82.0074, Discriminator Loss = 0.5858\n",
      "Epoch 129: Generator Loss = 68.0473, Discriminator Loss = 0.6944\n",
      "Epoch 130: Generator Loss = 72.1568, Discriminator Loss = 0.9964\n",
      "Epoch 131: Generator Loss = 78.3840, Discriminator Loss = 1.9026\n",
      "Epoch 132: Generator Loss = 61.5116, Discriminator Loss = 2.6807\n",
      "Epoch 133: Generator Loss = 62.4616, Discriminator Loss = 1.8242\n",
      "Epoch 134: Generator Loss = 84.2608, Discriminator Loss = 0.8694\n",
      "Epoch 135: Generator Loss = 73.7815, Discriminator Loss = 0.9069\n",
      "Epoch 136: Generator Loss = 62.1439, Discriminator Loss = 0.6623\n",
      "Epoch 137: Generator Loss = 80.8169, Discriminator Loss = 1.4542\n",
      "Epoch 138: Generator Loss = 75.7926, Discriminator Loss = 1.2165\n",
      "Epoch 139: Generator Loss = 80.3861, Discriminator Loss = 3.5116\n",
      "Epoch 140: Generator Loss = 67.7136, Discriminator Loss = 3.0432\n",
      "Epoch 141: Generator Loss = 71.6377, Discriminator Loss = 1.5584\n",
      "Epoch 142: Generator Loss = 76.4056, Discriminator Loss = 0.8549\n",
      "Epoch 143: Generator Loss = 73.6205, Discriminator Loss = 0.7114\n",
      "Epoch 144: Generator Loss = 78.9930, Discriminator Loss = 0.9773\n",
      "Epoch 145: Generator Loss = 75.5615, Discriminator Loss = 0.7487\n",
      "Epoch 146: Generator Loss = 83.4200, Discriminator Loss = 0.6173\n",
      "Epoch 147: Generator Loss = 84.7271, Discriminator Loss = 0.7931\n",
      "Epoch 148: Generator Loss = 90.5392, Discriminator Loss = 0.6592\n",
      "Epoch 149: Generator Loss = 79.2249, Discriminator Loss = 1.3453\n",
      "Epoch 150: Generator Loss = 66.1935, Discriminator Loss = 0.4862\n",
      "Epoch 151: Generator Loss = 81.8194, Discriminator Loss = 0.7454\n",
      "Epoch 152: Generator Loss = 84.7440, Discriminator Loss = 0.5482\n",
      "Epoch 153: Generator Loss = 73.2692, Discriminator Loss = 1.0076\n",
      "Epoch 154: Generator Loss = 78.7597, Discriminator Loss = 0.6743\n",
      "Epoch 155: Generator Loss = 72.7253, Discriminator Loss = 0.6579\n",
      "Epoch 156: Generator Loss = 67.6597, Discriminator Loss = 1.0643\n",
      "Epoch 157: Generator Loss = 88.6165, Discriminator Loss = 1.6322\n",
      "Epoch 158: Generator Loss = 89.8947, Discriminator Loss = 2.8627\n",
      "Epoch 159: Generator Loss = 40.8833, Discriminator Loss = 16.1287\n",
      "Epoch 160: Generator Loss = 29.6041, Discriminator Loss = 4.7234\n",
      "Epoch 161: Generator Loss = 42.4300, Discriminator Loss = 7.3973\n",
      "Epoch 162: Generator Loss = 46.3254, Discriminator Loss = 2.6051\n",
      "Epoch 163: Generator Loss = 52.2277, Discriminator Loss = 3.3494\n",
      "Epoch 164: Generator Loss = 46.9576, Discriminator Loss = 3.3582\n",
      "Epoch 165: Generator Loss = 56.5459, Discriminator Loss = 3.0937\n",
      "Epoch 166: Generator Loss = 56.6669, Discriminator Loss = 2.7437\n",
      "Epoch 167: Generator Loss = 56.3558, Discriminator Loss = 3.6521\n",
      "Epoch 168: Generator Loss = 57.6810, Discriminator Loss = 4.5902\n",
      "Epoch 169: Generator Loss = 68.6735, Discriminator Loss = 2.3864\n",
      "Epoch 170: Generator Loss = 56.8790, Discriminator Loss = 0.7080\n",
      "Epoch 171: Generator Loss = 69.5625, Discriminator Loss = 0.8604\n",
      "Epoch 172: Generator Loss = 64.6002, Discriminator Loss = 0.9706\n",
      "Epoch 173: Generator Loss = 67.6438, Discriminator Loss = 0.8356\n",
      "Epoch 174: Generator Loss = 86.8193, Discriminator Loss = 0.4550\n",
      "Epoch 175: Generator Loss = 72.8337, Discriminator Loss = 1.6177\n",
      "Epoch 176: Generator Loss = 49.3274, Discriminator Loss = 3.4310\n",
      "Epoch 177: Generator Loss = 52.8264, Discriminator Loss = 2.0211\n",
      "Epoch 178: Generator Loss = 74.8549, Discriminator Loss = 1.6840\n",
      "Epoch 179: Generator Loss = 59.9036, Discriminator Loss = 2.0885\n",
      "Epoch 180: Generator Loss = 72.5064, Discriminator Loss = 0.9871\n",
      "Epoch 181: Generator Loss = 80.8657, Discriminator Loss = 1.0521\n",
      "Epoch 182: Generator Loss = 79.8229, Discriminator Loss = 1.0042\n",
      "Epoch 183: Generator Loss = 71.2243, Discriminator Loss = 1.0501\n",
      "Epoch 184: Generator Loss = 69.2492, Discriminator Loss = 1.0543\n",
      "Epoch 185: Generator Loss = 78.1794, Discriminator Loss = 0.6414\n",
      "Epoch 186: Generator Loss = 78.1512, Discriminator Loss = 0.6960\n",
      "Epoch 187: Generator Loss = 67.0354, Discriminator Loss = 1.6635\n",
      "Epoch 188: Generator Loss = 74.2623, Discriminator Loss = 0.6443\n",
      "Epoch 189: Generator Loss = 84.2383, Discriminator Loss = 0.9901\n",
      "Epoch 190: Generator Loss = 93.1844, Discriminator Loss = 1.0987\n",
      "Epoch 191: Generator Loss = 80.6709, Discriminator Loss = 0.8011\n",
      "Epoch 192: Generator Loss = 84.2236, Discriminator Loss = 1.0636\n",
      "Epoch 193: Generator Loss = 96.5012, Discriminator Loss = 1.6544\n",
      "Epoch 194: Generator Loss = 74.5938, Discriminator Loss = 0.7066\n",
      "Epoch 195: Generator Loss = 254.3617, Discriminator Loss = 45.0753\n",
      "Epoch 196: Generator Loss = 169.2209, Discriminator Loss = 8.0503\n",
      "Epoch 197: Generator Loss = 83.6930, Discriminator Loss = 1.9933\n",
      "Epoch 198: Generator Loss = 84.7736, Discriminator Loss = 4.2834\n",
      "Epoch 199: Generator Loss = 69.4291, Discriminator Loss = 2.9519\n",
      "Epoch 200: Generator Loss = 70.0063, Discriminator Loss = 2.1829\n",
      "Epoch 201: Generator Loss = 66.2773, Discriminator Loss = 1.4689\n",
      "Epoch 202: Generator Loss = 74.5968, Discriminator Loss = 6.7824\n",
      "Epoch 203: Generator Loss = 65.6351, Discriminator Loss = 5.8438\n",
      "Epoch 204: Generator Loss = 61.5874, Discriminator Loss = 2.3866\n",
      "Epoch 205: Generator Loss = 60.6313, Discriminator Loss = 1.0570\n",
      "Epoch 206: Generator Loss = 82.4213, Discriminator Loss = 1.9782\n",
      "Epoch 207: Generator Loss = 65.7421, Discriminator Loss = 1.6584\n",
      "Epoch 208: Generator Loss = 63.8515, Discriminator Loss = 1.1925\n",
      "Epoch 209: Generator Loss = 66.8820, Discriminator Loss = 2.4587\n",
      "Epoch 210: Generator Loss = 70.8103, Discriminator Loss = 1.5794\n",
      "Epoch 211: Generator Loss = 68.4365, Discriminator Loss = 1.1184\n",
      "Epoch 212: Generator Loss = 76.2351, Discriminator Loss = 1.2513\n",
      "Epoch 213: Generator Loss = 58.8566, Discriminator Loss = 0.9906\n",
      "Epoch 214: Generator Loss = 77.0290, Discriminator Loss = 0.6918\n",
      "Epoch 215: Generator Loss = 67.1990, Discriminator Loss = 0.5392\n",
      "Epoch 216: Generator Loss = 55.8568, Discriminator Loss = 1.3763\n",
      "Epoch 217: Generator Loss = 73.2528, Discriminator Loss = 1.0852\n",
      "Epoch 218: Generator Loss = 76.6688, Discriminator Loss = 1.0265\n",
      "Epoch 219: Generator Loss = 75.9890, Discriminator Loss = 2.9219\n",
      "Epoch 220: Generator Loss = 67.3856, Discriminator Loss = 1.2644\n",
      "Epoch 221: Generator Loss = 64.7783, Discriminator Loss = 0.8273\n",
      "Epoch 222: Generator Loss = 69.5330, Discriminator Loss = 0.6236\n",
      "Epoch 223: Generator Loss = 87.8085, Discriminator Loss = 1.0080\n",
      "Epoch 224: Generator Loss = 78.4398, Discriminator Loss = 0.8279\n",
      "Epoch 225: Generator Loss = 74.0463, Discriminator Loss = 1.3833\n",
      "Epoch 226: Generator Loss = 89.2217, Discriminator Loss = 1.2724\n",
      "Epoch 227: Generator Loss = 68.7251, Discriminator Loss = 1.3612\n",
      "Epoch 228: Generator Loss = 82.6396, Discriminator Loss = 1.6988\n",
      "Epoch 229: Generator Loss = 63.7610, Discriminator Loss = 1.6754\n",
      "Epoch 230: Generator Loss = 82.3189, Discriminator Loss = 1.0336\n",
      "Epoch 231: Generator Loss = 89.4825, Discriminator Loss = 0.6861\n",
      "Epoch 232: Generator Loss = 65.0388, Discriminator Loss = 3.5648\n",
      "Epoch 233: Generator Loss = 69.3255, Discriminator Loss = 7.1025\n",
      "Epoch 234: Generator Loss = 65.0194, Discriminator Loss = 1.8007\n",
      "Epoch 235: Generator Loss = 56.4693, Discriminator Loss = 1.3148\n",
      "Epoch 236: Generator Loss = 66.6258, Discriminator Loss = 0.7698\n",
      "Epoch 237: Generator Loss = 72.9032, Discriminator Loss = 1.1642\n",
      "Epoch 238: Generator Loss = 74.7129, Discriminator Loss = 0.9491\n",
      "Epoch 239: Generator Loss = 83.4608, Discriminator Loss = 0.4174\n",
      "Epoch 240: Generator Loss = 71.6330, Discriminator Loss = 0.9331\n",
      "Epoch 241: Generator Loss = 74.6635, Discriminator Loss = 0.5075\n",
      "Epoch 242: Generator Loss = 87.1461, Discriminator Loss = 0.7929\n",
      "Epoch 243: Generator Loss = 78.8326, Discriminator Loss = 2.1521\n",
      "Epoch 244: Generator Loss = 71.3619, Discriminator Loss = 0.9437\n",
      "Epoch 245: Generator Loss = 64.3724, Discriminator Loss = 1.4973\n",
      "Epoch 246: Generator Loss = 82.7372, Discriminator Loss = 1.4416\n",
      "Epoch 247: Generator Loss = 62.7270, Discriminator Loss = 1.5803\n",
      "Epoch 248: Generator Loss = 106.1518, Discriminator Loss = 0.7130\n",
      "Epoch 249: Generator Loss = 82.3367, Discriminator Loss = 2.7804\n",
      "Epoch 250: Generator Loss = 98.7480, Discriminator Loss = 0.7332\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.5060 Acc: 0.7650\n",
      "Epoch 2/15 | Train Loss: 0.4034 Acc: 0.7781\n",
      "Epoch 3/15 | Train Loss: 0.3450 Acc: 0.8251\n",
      "Epoch 4/15 | Train Loss: 0.3997 Acc: 0.8120\n",
      "Epoch 5/15 | Train Loss: 0.3325 Acc: 0.8355\n",
      "Epoch 6/15 | Train Loss: 0.3193 Acc: 0.8538\n",
      "Epoch 7/15 | Train Loss: 0.2893 Acc: 0.8668\n",
      "Epoch 8/15 | Train Loss: 0.2946 Acc: 0.8538\n",
      "Epoch 9/15 | Train Loss: 0.2704 Acc: 0.8695\n",
      "Epoch 10/15 | Train Loss: 0.2512 Acc: 0.8903\n",
      "Epoch 11/15 | Train Loss: 0.2354 Acc: 0.8903\n",
      "Epoch 12/15 | Train Loss: 0.2415 Acc: 0.8799\n",
      "Epoch 13/15 | Train Loss: 0.2484 Acc: 0.8877\n",
      "Epoch 14/15 | Train Loss: 0.2253 Acc: 0.9008\n",
      "Epoch 15/15 | Train Loss: 0.2342 Acc: 0.9034\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5017 Acc: 0.7598\n",
      "Epoch 2/15 | Train Loss: 0.3844 Acc: 0.7937\n",
      "Epoch 3/15 | Train Loss: 0.3633 Acc: 0.7833\n",
      "Epoch 4/15 | Train Loss: 0.3771 Acc: 0.7911\n",
      "Epoch 5/15 | Train Loss: 0.3443 Acc: 0.8277\n",
      "Epoch 6/15 | Train Loss: 0.3156 Acc: 0.8251\n",
      "Epoch 7/15 | Train Loss: 0.3021 Acc: 0.8512\n",
      "Epoch 8/15 | Train Loss: 0.2941 Acc: 0.8538\n",
      "Epoch 9/15 | Train Loss: 0.2959 Acc: 0.8486\n",
      "Epoch 10/15 | Train Loss: 0.2688 Acc: 0.8642\n",
      "Epoch 11/15 | Train Loss: 0.2714 Acc: 0.8642\n",
      "Epoch 12/15 | Train Loss: 0.2615 Acc: 0.8695\n",
      "Epoch 13/15 | Train Loss: 0.2593 Acc: 0.8695\n",
      "Epoch 14/15 | Train Loss: 0.2667 Acc: 0.8668\n",
      "Epoch 15/15 | Train Loss: 0.2512 Acc: 0.8721\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4928 Acc: 0.7786\n",
      "Epoch 2/15 | Train Loss: 0.4029 Acc: 0.7969\n",
      "Epoch 3/15 | Train Loss: 0.3776 Acc: 0.7943\n",
      "Epoch 4/15 | Train Loss: 0.3657 Acc: 0.8177\n",
      "Epoch 5/15 | Train Loss: 0.3156 Acc: 0.8490\n",
      "Epoch 6/15 | Train Loss: 0.3614 Acc: 0.8047\n",
      "Epoch 7/15 | Train Loss: 0.3034 Acc: 0.8333\n",
      "Epoch 8/15 | Train Loss: 0.3294 Acc: 0.8594\n",
      "Epoch 9/15 | Train Loss: 0.2685 Acc: 0.8906\n",
      "Epoch 10/15 | Train Loss: 0.2541 Acc: 0.8828\n",
      "Epoch 11/15 | Train Loss: 0.2509 Acc: 0.8672\n",
      "Epoch 12/15 | Train Loss: 0.2686 Acc: 0.8568\n",
      "Epoch 13/15 | Train Loss: 0.2895 Acc: 0.8516\n",
      "Epoch 14/15 | Train Loss: 0.2455 Acc: 0.8854\n",
      "Epoch 15/15 | Train Loss: 0.2706 Acc: 0.8516\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5123 Acc: 0.7866\n",
      "Epoch 2/15 | Train Loss: 0.4148 Acc: 0.8121\n",
      "Epoch 3/15 | Train Loss: 0.4045 Acc: 0.8248\n",
      "Epoch 4/15 | Train Loss: 0.3928 Acc: 0.8439\n",
      "Epoch 5/15 | Train Loss: 0.3490 Acc: 0.8503\n",
      "Epoch 6/15 | Train Loss: 0.3497 Acc: 0.8535\n",
      "Epoch 7/15 | Train Loss: 0.3159 Acc: 0.8758\n",
      "Epoch 8/15 | Train Loss: 0.3158 Acc: 0.8662\n",
      "Epoch 9/15 | Train Loss: 0.3023 Acc: 0.8567\n",
      "Epoch 10/15 | Train Loss: 0.3077 Acc: 0.8790\n",
      "Epoch 11/15 | Train Loss: 0.2909 Acc: 0.8822\n",
      "Epoch 12/15 | Train Loss: 0.2565 Acc: 0.8981\n",
      "Epoch 13/15 | Train Loss: 0.2857 Acc: 0.8726\n",
      "Epoch 14/15 | Train Loss: 0.2805 Acc: 0.8917\n",
      "Epoch 15/15 | Train Loss: 0.2549 Acc: 0.8917\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5212 Acc: 0.8214\n",
      "Epoch 2/15 | Train Loss: 0.4131 Acc: 0.8429\n",
      "Epoch 3/15 | Train Loss: 0.3288 Acc: 0.8821\n",
      "Epoch 4/15 | Train Loss: 0.3038 Acc: 0.8857\n",
      "Epoch 5/15 | Train Loss: 0.3258 Acc: 0.8786\n",
      "Epoch 6/15 | Train Loss: 0.2934 Acc: 0.8964\n",
      "Epoch 7/15 | Train Loss: 0.2540 Acc: 0.8536\n",
      "Epoch 8/15 | Train Loss: 0.3195 Acc: 0.8964\n",
      "Epoch 9/15 | Train Loss: 0.2715 Acc: 0.9036\n",
      "Epoch 10/15 | Train Loss: 0.2221 Acc: 0.9107\n",
      "Epoch 11/15 | Train Loss: 0.2653 Acc: 0.9036\n",
      "Epoch 12/15 | Train Loss: 0.2370 Acc: 0.9036\n",
      "Epoch 13/15 | Train Loss: 0.2158 Acc: 0.9179\n",
      "Epoch 14/15 | Train Loss: 0.2077 Acc: 0.9321\n",
      "Epoch 15/15 | Train Loss: 0.2342 Acc: 0.9143\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7001 Acc: 0.6468\n",
      "Epoch 2/15 | Train Loss: 0.5520 Acc: 0.6865\n",
      "Epoch 3/15 | Train Loss: 0.5333 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.4990 Acc: 0.7460\n",
      "Epoch 5/15 | Train Loss: 0.4836 Acc: 0.7579\n",
      "Epoch 6/15 | Train Loss: 0.4382 Acc: 0.7857\n",
      "Epoch 7/15 | Train Loss: 0.4320 Acc: 0.7579\n",
      "Epoch 8/15 | Train Loss: 0.4668 Acc: 0.8095\n",
      "Epoch 9/15 | Train Loss: 0.4033 Acc: 0.8095\n",
      "Epoch 10/15 | Train Loss: 0.3779 Acc: 0.8175\n",
      "Epoch 11/15 | Train Loss: 0.3789 Acc: 0.8175\n",
      "Epoch 12/15 | Train Loss: 0.3856 Acc: 0.8373\n",
      "Epoch 13/15 | Train Loss: 0.4267 Acc: 0.7857\n",
      "Epoch 14/15 | Train Loss: 0.4088 Acc: 0.7937\n",
      "Epoch 15/15 | Train Loss: 0.3340 Acc: 0.8413\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.8083 Acc: 0.6151\n",
      "Epoch 2/15 | Train Loss: 0.6174 Acc: 0.6865\n",
      "Epoch 3/15 | Train Loss: 0.5879 Acc: 0.6746\n",
      "Epoch 4/15 | Train Loss: 0.5589 Acc: 0.7024\n",
      "Epoch 5/15 | Train Loss: 0.4849 Acc: 0.7500\n",
      "Epoch 6/15 | Train Loss: 0.5067 Acc: 0.7381\n",
      "Epoch 7/15 | Train Loss: 0.4539 Acc: 0.7817\n",
      "Epoch 8/15 | Train Loss: 0.4628 Acc: 0.7659\n",
      "Epoch 9/15 | Train Loss: 0.4696 Acc: 0.7619\n",
      "Epoch 10/15 | Train Loss: 0.4309 Acc: 0.8056\n",
      "Epoch 11/15 | Train Loss: 0.4022 Acc: 0.7778\n",
      "Epoch 12/15 | Train Loss: 0.4907 Acc: 0.7579\n",
      "Epoch 13/15 | Train Loss: 0.4048 Acc: 0.8135\n",
      "Epoch 14/15 | Train Loss: 0.4109 Acc: 0.8095\n",
      "Epoch 15/15 | Train Loss: 0.4479 Acc: 0.7738\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6906 Acc: 0.6126\n",
      "Epoch 2/15 | Train Loss: 0.6226 Acc: 0.6482\n",
      "Epoch 3/15 | Train Loss: 0.6013 Acc: 0.6601\n",
      "Epoch 4/15 | Train Loss: 0.4931 Acc: 0.7194\n",
      "Epoch 5/15 | Train Loss: 0.5382 Acc: 0.7391\n",
      "Epoch 6/15 | Train Loss: 0.5180 Acc: 0.7273\n",
      "Epoch 7/15 | Train Loss: 0.4362 Acc: 0.7747\n",
      "Epoch 8/15 | Train Loss: 0.5022 Acc: 0.7589\n",
      "Epoch 9/15 | Train Loss: 0.4183 Acc: 0.8024\n",
      "Epoch 10/15 | Train Loss: 0.3942 Acc: 0.8340\n",
      "Epoch 11/15 | Train Loss: 0.4505 Acc: 0.7747\n",
      "Epoch 12/15 | Train Loss: 0.4022 Acc: 0.8261\n",
      "Epoch 13/15 | Train Loss: 0.3995 Acc: 0.8142\n",
      "Epoch 14/15 | Train Loss: 0.3748 Acc: 0.8182\n",
      "Epoch 15/15 | Train Loss: 0.4112 Acc: 0.8024\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6738 Acc: 0.6568\n",
      "Epoch 2/15 | Train Loss: 0.4927 Acc: 0.7924\n",
      "Epoch 3/15 | Train Loss: 0.4920 Acc: 0.7797\n",
      "Epoch 4/15 | Train Loss: 0.4682 Acc: 0.7839\n",
      "Epoch 5/15 | Train Loss: 0.5033 Acc: 0.7712\n",
      "Epoch 6/15 | Train Loss: 0.4304 Acc: 0.7966\n",
      "Epoch 7/15 | Train Loss: 0.4830 Acc: 0.8051\n",
      "Epoch 8/15 | Train Loss: 0.3895 Acc: 0.8220\n",
      "Epoch 9/15 | Train Loss: 0.3377 Acc: 0.8602\n",
      "Epoch 10/15 | Train Loss: 0.3802 Acc: 0.8220\n",
      "Epoch 11/15 | Train Loss: 0.3666 Acc: 0.8517\n",
      "Epoch 12/15 | Train Loss: 0.4135 Acc: 0.8178\n",
      "Epoch 13/15 | Train Loss: 0.3594 Acc: 0.8517\n",
      "Epoch 14/15 | Train Loss: 0.3681 Acc: 0.8347\n",
      "Epoch 15/15 | Train Loss: 0.3362 Acc: 0.8559\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5624 Acc: 0.7445\n",
      "Epoch 2/15 | Train Loss: 0.5242 Acc: 0.7930\n",
      "Epoch 3/15 | Train Loss: 0.4563 Acc: 0.8194\n",
      "Epoch 4/15 | Train Loss: 0.3666 Acc: 0.8546\n",
      "Epoch 5/15 | Train Loss: 0.3533 Acc: 0.8502\n",
      "Epoch 6/15 | Train Loss: 0.3132 Acc: 0.8590\n",
      "Epoch 7/15 | Train Loss: 0.3121 Acc: 0.8502\n",
      "Epoch 8/15 | Train Loss: 0.3175 Acc: 0.8722\n",
      "Epoch 9/15 | Train Loss: 0.3140 Acc: 0.8767\n",
      "Epoch 10/15 | Train Loss: 0.2485 Acc: 0.8943\n",
      "Epoch 11/15 | Train Loss: 0.2913 Acc: 0.8811\n",
      "Epoch 12/15 | Train Loss: 0.2776 Acc: 0.8855\n",
      "Epoch 13/15 | Train Loss: 0.2593 Acc: 0.8943\n",
      "Epoch 14/15 | Train Loss: 0.2617 Acc: 0.8899\n",
      "Epoch 15/15 | Train Loss: 0.2400 Acc: 0.8987\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5962 Acc: 0.7162\n",
      "Epoch 2/15 | Train Loss: 0.5073 Acc: 0.7128\n",
      "Epoch 3/15 | Train Loss: 0.4681 Acc: 0.7466\n",
      "Epoch 4/15 | Train Loss: 0.5177 Acc: 0.7568\n",
      "Epoch 5/15 | Train Loss: 0.4277 Acc: 0.7770\n",
      "Epoch 6/15 | Train Loss: 0.3895 Acc: 0.8176\n",
      "Epoch 7/15 | Train Loss: 0.4369 Acc: 0.7804\n",
      "Epoch 8/15 | Train Loss: 0.3646 Acc: 0.8277\n",
      "Epoch 9/15 | Train Loss: 0.3326 Acc: 0.8446\n",
      "Epoch 10/15 | Train Loss: 0.3113 Acc: 0.8581\n",
      "Epoch 11/15 | Train Loss: 0.3188 Acc: 0.8514\n",
      "Epoch 12/15 | Train Loss: 0.3353 Acc: 0.8176\n",
      "Epoch 13/15 | Train Loss: 0.3315 Acc: 0.8480\n",
      "Epoch 14/15 | Train Loss: 0.3384 Acc: 0.8514\n",
      "Epoch 15/15 | Train Loss: 0.2966 Acc: 0.8682\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6611 Acc: 0.7061\n",
      "Epoch 2/15 | Train Loss: 0.5256 Acc: 0.7331\n",
      "Epoch 3/15 | Train Loss: 0.4439 Acc: 0.7534\n",
      "Epoch 4/15 | Train Loss: 0.4712 Acc: 0.7568\n",
      "Epoch 5/15 | Train Loss: 0.4586 Acc: 0.7365\n",
      "Epoch 6/15 | Train Loss: 0.4462 Acc: 0.7736\n",
      "Epoch 7/15 | Train Loss: 0.4456 Acc: 0.7872\n",
      "Epoch 8/15 | Train Loss: 0.3816 Acc: 0.8007\n",
      "Epoch 9/15 | Train Loss: 0.4241 Acc: 0.7804\n",
      "Epoch 10/15 | Train Loss: 0.3564 Acc: 0.8480\n",
      "Epoch 11/15 | Train Loss: 0.3535 Acc: 0.8311\n",
      "Epoch 12/15 | Train Loss: 0.3424 Acc: 0.8209\n",
      "Epoch 13/15 | Train Loss: 0.3677 Acc: 0.8176\n",
      "Epoch 14/15 | Train Loss: 0.3593 Acc: 0.8311\n",
      "Epoch 15/15 | Train Loss: 0.3539 Acc: 0.8378\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6368 Acc: 0.6465\n",
      "Epoch 2/15 | Train Loss: 0.5132 Acc: 0.7306\n",
      "Epoch 3/15 | Train Loss: 0.5191 Acc: 0.7037\n",
      "Epoch 4/15 | Train Loss: 0.4302 Acc: 0.8081\n",
      "Epoch 5/15 | Train Loss: 0.4585 Acc: 0.7205\n",
      "Epoch 6/15 | Train Loss: 0.3539 Acc: 0.8451\n",
      "Epoch 7/15 | Train Loss: 0.4027 Acc: 0.7980\n",
      "Epoch 8/15 | Train Loss: 0.3795 Acc: 0.8047\n",
      "Epoch 9/15 | Train Loss: 0.3393 Acc: 0.8384\n",
      "Epoch 10/15 | Train Loss: 0.3785 Acc: 0.7946\n",
      "Epoch 11/15 | Train Loss: 0.3124 Acc: 0.8552\n",
      "Epoch 12/15 | Train Loss: 0.3191 Acc: 0.8687\n",
      "Epoch 13/15 | Train Loss: 0.3280 Acc: 0.8418\n",
      "Epoch 14/15 | Train Loss: 0.3619 Acc: 0.7946\n",
      "Epoch 15/15 | Train Loss: 0.2960 Acc: 0.8687\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6183 Acc: 0.7252\n",
      "Epoch 2/15 | Train Loss: 0.5100 Acc: 0.7939\n",
      "Epoch 3/15 | Train Loss: 0.4846 Acc: 0.7748\n",
      "Epoch 4/15 | Train Loss: 0.4250 Acc: 0.8321\n",
      "Epoch 5/15 | Train Loss: 0.4159 Acc: 0.7901\n",
      "Epoch 6/15 | Train Loss: 0.4006 Acc: 0.8206\n",
      "Epoch 7/15 | Train Loss: 0.4052 Acc: 0.8168\n",
      "Epoch 8/15 | Train Loss: 0.4183 Acc: 0.8321\n",
      "Epoch 9/15 | Train Loss: 0.3880 Acc: 0.8244\n",
      "Epoch 10/15 | Train Loss: 0.3572 Acc: 0.8588\n",
      "Epoch 11/15 | Train Loss: 0.3318 Acc: 0.8626\n",
      "Epoch 12/15 | Train Loss: 0.3559 Acc: 0.8435\n",
      "Epoch 13/15 | Train Loss: 0.3457 Acc: 0.8397\n",
      "Epoch 14/15 | Train Loss: 0.3329 Acc: 0.8473\n",
      "Epoch 15/15 | Train Loss: 0.3452 Acc: 0.8664\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5212 Acc: 0.7878\n",
      "Epoch 2/15 | Train Loss: 0.4734 Acc: 0.8163\n",
      "Epoch 3/15 | Train Loss: 0.3662 Acc: 0.8571\n",
      "Epoch 4/15 | Train Loss: 0.3481 Acc: 0.8571\n",
      "Epoch 5/15 | Train Loss: 0.3249 Acc: 0.8653\n",
      "Epoch 6/15 | Train Loss: 0.3367 Acc: 0.8735\n",
      "Epoch 7/15 | Train Loss: 0.2773 Acc: 0.8735\n",
      "Epoch 8/15 | Train Loss: 0.3138 Acc: 0.8776\n",
      "Epoch 9/15 | Train Loss: 0.3056 Acc: 0.8694\n",
      "Epoch 10/15 | Train Loss: 0.2501 Acc: 0.9265\n",
      "Epoch 11/15 | Train Loss: 0.2921 Acc: 0.8816\n",
      "Epoch 12/15 | Train Loss: 0.2444 Acc: 0.9102\n",
      "Epoch 13/15 | Train Loss: 0.3028 Acc: 0.8776\n",
      "Epoch 14/15 | Train Loss: 0.2294 Acc: 0.9102\n",
      "Epoch 15/15 | Train Loss: 0.2465 Acc: 0.9061\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6741 Acc: 0.6587\n",
      "Epoch 2/15 | Train Loss: 0.5596 Acc: 0.6944\n",
      "Epoch 3/15 | Train Loss: 0.5316 Acc: 0.7302\n",
      "Epoch 4/15 | Train Loss: 0.5104 Acc: 0.7421\n",
      "Epoch 5/15 | Train Loss: 0.4923 Acc: 0.7381\n",
      "Epoch 6/15 | Train Loss: 0.4970 Acc: 0.7698\n",
      "Epoch 7/15 | Train Loss: 0.4534 Acc: 0.8095\n",
      "Epoch 8/15 | Train Loss: 0.3938 Acc: 0.8294\n",
      "Epoch 9/15 | Train Loss: 0.3948 Acc: 0.8294\n",
      "Epoch 10/15 | Train Loss: 0.3981 Acc: 0.8175\n",
      "Epoch 11/15 | Train Loss: 0.3193 Acc: 0.8571\n",
      "Epoch 12/15 | Train Loss: 0.3675 Acc: 0.8373\n",
      "Epoch 13/15 | Train Loss: 0.3134 Acc: 0.8571\n",
      "Epoch 14/15 | Train Loss: 0.3083 Acc: 0.8690\n",
      "Epoch 15/15 | Train Loss: 0.3231 Acc: 0.8611\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6959 Acc: 0.6587\n",
      "Epoch 2/15 | Train Loss: 0.6082 Acc: 0.6508\n",
      "Epoch 3/15 | Train Loss: 0.5154 Acc: 0.7222\n",
      "Epoch 4/15 | Train Loss: 0.5016 Acc: 0.7183\n",
      "Epoch 5/15 | Train Loss: 0.5455 Acc: 0.7540\n",
      "Epoch 6/15 | Train Loss: 0.5431 Acc: 0.7262\n",
      "Epoch 7/15 | Train Loss: 0.5375 Acc: 0.7302\n",
      "Epoch 8/15 | Train Loss: 0.4420 Acc: 0.7579\n",
      "Epoch 9/15 | Train Loss: 0.4994 Acc: 0.7460\n",
      "Epoch 10/15 | Train Loss: 0.4327 Acc: 0.7976\n",
      "Epoch 11/15 | Train Loss: 0.4314 Acc: 0.7937\n",
      "Epoch 12/15 | Train Loss: 0.4220 Acc: 0.7976\n",
      "Epoch 13/15 | Train Loss: 0.4416 Acc: 0.7659\n",
      "Epoch 14/15 | Train Loss: 0.4220 Acc: 0.7897\n",
      "Epoch 15/15 | Train Loss: 0.4436 Acc: 0.7857\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7742 Acc: 0.6245\n",
      "Epoch 2/15 | Train Loss: 0.6134 Acc: 0.6482\n",
      "Epoch 3/15 | Train Loss: 0.5696 Acc: 0.7470\n",
      "Epoch 4/15 | Train Loss: 0.5473 Acc: 0.6957\n",
      "Epoch 5/15 | Train Loss: 0.4921 Acc: 0.7589\n",
      "Epoch 6/15 | Train Loss: 0.4878 Acc: 0.7233\n",
      "Epoch 7/15 | Train Loss: 0.4978 Acc: 0.7115\n",
      "Epoch 8/15 | Train Loss: 0.4298 Acc: 0.8182\n",
      "Epoch 9/15 | Train Loss: 0.4175 Acc: 0.7826\n",
      "Epoch 10/15 | Train Loss: 0.4450 Acc: 0.7747\n",
      "Epoch 11/15 | Train Loss: 0.4621 Acc: 0.7470\n",
      "Epoch 12/15 | Train Loss: 0.4197 Acc: 0.8103\n",
      "Epoch 13/15 | Train Loss: 0.3893 Acc: 0.8182\n",
      "Epoch 14/15 | Train Loss: 0.3787 Acc: 0.8419\n",
      "Epoch 15/15 | Train Loss: 0.3730 Acc: 0.8221\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7673 Acc: 0.5847\n",
      "Epoch 2/15 | Train Loss: 0.5222 Acc: 0.7669\n",
      "Epoch 3/15 | Train Loss: 0.5572 Acc: 0.7839\n",
      "Epoch 4/15 | Train Loss: 0.4804 Acc: 0.7839\n",
      "Epoch 5/15 | Train Loss: 0.4235 Acc: 0.8263\n",
      "Epoch 6/15 | Train Loss: 0.5137 Acc: 0.7712\n",
      "Epoch 7/15 | Train Loss: 0.4589 Acc: 0.8008\n",
      "Epoch 8/15 | Train Loss: 0.5021 Acc: 0.7797\n",
      "Epoch 9/15 | Train Loss: 0.3834 Acc: 0.8390\n",
      "Epoch 10/15 | Train Loss: 0.3804 Acc: 0.8305\n",
      "Epoch 11/15 | Train Loss: 0.4219 Acc: 0.8178\n",
      "Epoch 12/15 | Train Loss: 0.3697 Acc: 0.8475\n",
      "Epoch 13/15 | Train Loss: 0.3729 Acc: 0.8263\n",
      "Epoch 14/15 | Train Loss: 0.3534 Acc: 0.8305\n",
      "Epoch 15/15 | Train Loss: 0.4381 Acc: 0.8051\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7240 Acc: 0.6035\n",
      "Epoch 2/15 | Train Loss: 0.5281 Acc: 0.7930\n",
      "Epoch 3/15 | Train Loss: 0.3945 Acc: 0.8502\n",
      "Epoch 4/15 | Train Loss: 0.3648 Acc: 0.8370\n",
      "Epoch 5/15 | Train Loss: 0.3554 Acc: 0.8634\n",
      "Epoch 6/15 | Train Loss: 0.3576 Acc: 0.8678\n",
      "Epoch 7/15 | Train Loss: 0.3472 Acc: 0.8590\n",
      "Epoch 8/15 | Train Loss: 0.2779 Acc: 0.8943\n",
      "Epoch 9/15 | Train Loss: 0.2988 Acc: 0.8811\n",
      "Epoch 10/15 | Train Loss: 0.2557 Acc: 0.9031\n",
      "Epoch 11/15 | Train Loss: 0.2546 Acc: 0.9031\n",
      "Epoch 12/15 | Train Loss: 0.3142 Acc: 0.8811\n",
      "Epoch 13/15 | Train Loss: 0.2688 Acc: 0.8899\n",
      "Epoch 14/15 | Train Loss: 0.2837 Acc: 0.9031\n",
      "Epoch 15/15 | Train Loss: 0.2414 Acc: 0.9031\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5450 Acc: 0.7232\n",
      "Epoch 2/15 | Train Loss: 0.3893 Acc: 0.8042\n",
      "Epoch 3/15 | Train Loss: 0.3643 Acc: 0.7990\n",
      "Epoch 4/15 | Train Loss: 0.3330 Acc: 0.8564\n",
      "Epoch 5/15 | Train Loss: 0.3609 Acc: 0.8251\n",
      "Epoch 6/15 | Train Loss: 0.3244 Acc: 0.8381\n",
      "Epoch 7/15 | Train Loss: 0.3136 Acc: 0.8538\n",
      "Epoch 8/15 | Train Loss: 0.2796 Acc: 0.8564\n",
      "Epoch 9/15 | Train Loss: 0.2541 Acc: 0.8825\n",
      "Epoch 10/15 | Train Loss: 0.2679 Acc: 0.8721\n",
      "Epoch 11/15 | Train Loss: 0.2649 Acc: 0.8747\n",
      "Epoch 12/15 | Train Loss: 0.2434 Acc: 0.8799\n",
      "Epoch 13/15 | Train Loss: 0.2307 Acc: 0.8930\n",
      "Epoch 14/15 | Train Loss: 0.2017 Acc: 0.9269\n",
      "Epoch 15/15 | Train Loss: 0.2465 Acc: 0.8799\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5421 Acc: 0.7572\n",
      "Epoch 2/15 | Train Loss: 0.3753 Acc: 0.7807\n",
      "Epoch 3/15 | Train Loss: 0.3855 Acc: 0.7885\n",
      "Epoch 4/15 | Train Loss: 0.3307 Acc: 0.8094\n",
      "Epoch 5/15 | Train Loss: 0.3520 Acc: 0.8198\n",
      "Epoch 6/15 | Train Loss: 0.3546 Acc: 0.7833\n",
      "Epoch 7/15 | Train Loss: 0.3230 Acc: 0.8381\n",
      "Epoch 8/15 | Train Loss: 0.2791 Acc: 0.8564\n",
      "Epoch 9/15 | Train Loss: 0.2871 Acc: 0.8590\n",
      "Epoch 10/15 | Train Loss: 0.2544 Acc: 0.8747\n",
      "Epoch 11/15 | Train Loss: 0.2753 Acc: 0.8590\n",
      "Epoch 12/15 | Train Loss: 0.2639 Acc: 0.8668\n",
      "Epoch 13/15 | Train Loss: 0.2549 Acc: 0.8721\n",
      "Epoch 14/15 | Train Loss: 0.2721 Acc: 0.8590\n",
      "Epoch 15/15 | Train Loss: 0.2494 Acc: 0.8825\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5505 Acc: 0.7109\n",
      "Epoch 2/15 | Train Loss: 0.4163 Acc: 0.7839\n",
      "Epoch 3/15 | Train Loss: 0.3559 Acc: 0.8177\n",
      "Epoch 4/15 | Train Loss: 0.3549 Acc: 0.7943\n",
      "Epoch 5/15 | Train Loss: 0.3343 Acc: 0.8333\n",
      "Epoch 6/15 | Train Loss: 0.3206 Acc: 0.8490\n",
      "Epoch 7/15 | Train Loss: 0.2912 Acc: 0.8542\n",
      "Epoch 8/15 | Train Loss: 0.2757 Acc: 0.8646\n",
      "Epoch 9/15 | Train Loss: 0.2896 Acc: 0.8438\n",
      "Epoch 10/15 | Train Loss: 0.2491 Acc: 0.8750\n",
      "Epoch 11/15 | Train Loss: 0.2692 Acc: 0.8646\n",
      "Epoch 12/15 | Train Loss: 0.2649 Acc: 0.8698\n",
      "Epoch 13/15 | Train Loss: 0.2507 Acc: 0.8776\n",
      "Epoch 14/15 | Train Loss: 0.2794 Acc: 0.8698\n",
      "Epoch 15/15 | Train Loss: 0.2711 Acc: 0.8594\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4948 Acc: 0.7548\n",
      "Epoch 2/15 | Train Loss: 0.4692 Acc: 0.8025\n",
      "Epoch 3/15 | Train Loss: 0.3957 Acc: 0.8057\n",
      "Epoch 4/15 | Train Loss: 0.3769 Acc: 0.8503\n",
      "Epoch 5/15 | Train Loss: 0.3670 Acc: 0.8248\n",
      "Epoch 6/15 | Train Loss: 0.3560 Acc: 0.8535\n",
      "Epoch 7/15 | Train Loss: 0.3385 Acc: 0.8535\n",
      "Epoch 8/15 | Train Loss: 0.2906 Acc: 0.8822\n",
      "Epoch 9/15 | Train Loss: 0.3286 Acc: 0.8599\n",
      "Epoch 10/15 | Train Loss: 0.3078 Acc: 0.8694\n",
      "Epoch 11/15 | Train Loss: 0.2951 Acc: 0.8854\n",
      "Epoch 12/15 | Train Loss: 0.2586 Acc: 0.8758\n",
      "Epoch 13/15 | Train Loss: 0.2970 Acc: 0.8758\n",
      "Epoch 14/15 | Train Loss: 0.2854 Acc: 0.8885\n",
      "Epoch 15/15 | Train Loss: 0.2675 Acc: 0.8885\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4642 Acc: 0.8071\n",
      "Epoch 2/15 | Train Loss: 0.4203 Acc: 0.8679\n",
      "Epoch 3/15 | Train Loss: 0.3687 Acc: 0.8536\n",
      "Epoch 4/15 | Train Loss: 0.3550 Acc: 0.8607\n",
      "Epoch 5/15 | Train Loss: 0.2991 Acc: 0.8929\n",
      "Epoch 6/15 | Train Loss: 0.2735 Acc: 0.9000\n",
      "Epoch 7/15 | Train Loss: 0.2743 Acc: 0.8857\n",
      "Epoch 8/15 | Train Loss: 0.2549 Acc: 0.8964\n",
      "Epoch 9/15 | Train Loss: 0.2650 Acc: 0.8750\n",
      "Epoch 10/15 | Train Loss: 0.2233 Acc: 0.9000\n",
      "Epoch 11/15 | Train Loss: 0.2413 Acc: 0.9179\n",
      "Epoch 12/15 | Train Loss: 0.2290 Acc: 0.8964\n",
      "Epoch 13/15 | Train Loss: 0.1956 Acc: 0.9321\n",
      "Epoch 14/15 | Train Loss: 0.2094 Acc: 0.9143\n",
      "Epoch 15/15 | Train Loss: 0.1968 Acc: 0.9286\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6942 Acc: 0.6111\n",
      "Epoch 2/15 | Train Loss: 0.5613 Acc: 0.6825\n",
      "Epoch 3/15 | Train Loss: 0.5223 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.4814 Acc: 0.7579\n",
      "Epoch 5/15 | Train Loss: 0.5476 Acc: 0.7341\n",
      "Epoch 6/15 | Train Loss: 0.4920 Acc: 0.7698\n",
      "Epoch 7/15 | Train Loss: 0.4138 Acc: 0.7778\n",
      "Epoch 8/15 | Train Loss: 0.4050 Acc: 0.8016\n",
      "Epoch 9/15 | Train Loss: 0.3383 Acc: 0.8452\n",
      "Epoch 10/15 | Train Loss: 0.3898 Acc: 0.8214\n",
      "Epoch 11/15 | Train Loss: 0.3327 Acc: 0.8373\n",
      "Epoch 12/15 | Train Loss: 0.3529 Acc: 0.8532\n",
      "Epoch 13/15 | Train Loss: 0.3514 Acc: 0.8294\n",
      "Epoch 14/15 | Train Loss: 0.3619 Acc: 0.8135\n",
      "Epoch 15/15 | Train Loss: 0.3453 Acc: 0.8532\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7764 Acc: 0.6190\n",
      "Epoch 2/15 | Train Loss: 0.5974 Acc: 0.6190\n",
      "Epoch 3/15 | Train Loss: 0.5952 Acc: 0.6667\n",
      "Epoch 4/15 | Train Loss: 0.5667 Acc: 0.7302\n",
      "Epoch 5/15 | Train Loss: 0.5174 Acc: 0.7262\n",
      "Epoch 6/15 | Train Loss: 0.5164 Acc: 0.7262\n",
      "Epoch 7/15 | Train Loss: 0.4764 Acc: 0.7341\n",
      "Epoch 8/15 | Train Loss: 0.4318 Acc: 0.7817\n",
      "Epoch 9/15 | Train Loss: 0.4515 Acc: 0.7817\n",
      "Epoch 10/15 | Train Loss: 0.4305 Acc: 0.7778\n",
      "Epoch 11/15 | Train Loss: 0.4193 Acc: 0.8373\n",
      "Epoch 12/15 | Train Loss: 0.4097 Acc: 0.8254\n",
      "Epoch 13/15 | Train Loss: 0.4922 Acc: 0.7659\n",
      "Epoch 14/15 | Train Loss: 0.4323 Acc: 0.7698\n",
      "Epoch 15/15 | Train Loss: 0.3754 Acc: 0.8294\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6730 Acc: 0.6482\n",
      "Epoch 2/15 | Train Loss: 0.5421 Acc: 0.7115\n",
      "Epoch 3/15 | Train Loss: 0.5024 Acc: 0.7273\n",
      "Epoch 4/15 | Train Loss: 0.5413 Acc: 0.7352\n",
      "Epoch 5/15 | Train Loss: 0.4681 Acc: 0.7747\n",
      "Epoch 6/15 | Train Loss: 0.4600 Acc: 0.7510\n",
      "Epoch 7/15 | Train Loss: 0.4467 Acc: 0.7668\n",
      "Epoch 8/15 | Train Loss: 0.4189 Acc: 0.8024\n",
      "Epoch 9/15 | Train Loss: 0.3905 Acc: 0.8261\n",
      "Epoch 10/15 | Train Loss: 0.4174 Acc: 0.7708\n",
      "Epoch 11/15 | Train Loss: 0.4920 Acc: 0.7787\n",
      "Epoch 12/15 | Train Loss: 0.3790 Acc: 0.8182\n",
      "Epoch 13/15 | Train Loss: 0.3978 Acc: 0.8221\n",
      "Epoch 14/15 | Train Loss: 0.3724 Acc: 0.8498\n",
      "Epoch 15/15 | Train Loss: 0.3650 Acc: 0.8261\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6704 Acc: 0.6610\n",
      "Epoch 2/15 | Train Loss: 0.5898 Acc: 0.7203\n",
      "Epoch 3/15 | Train Loss: 0.5123 Acc: 0.7966\n",
      "Epoch 4/15 | Train Loss: 0.4613 Acc: 0.7797\n",
      "Epoch 5/15 | Train Loss: 0.4827 Acc: 0.7839\n",
      "Epoch 6/15 | Train Loss: 0.4204 Acc: 0.8390\n",
      "Epoch 7/15 | Train Loss: 0.3883 Acc: 0.8305\n",
      "Epoch 8/15 | Train Loss: 0.5116 Acc: 0.7754\n",
      "Epoch 9/15 | Train Loss: 0.4386 Acc: 0.8051\n",
      "Epoch 10/15 | Train Loss: 0.4202 Acc: 0.8390\n",
      "Epoch 11/15 | Train Loss: 0.4160 Acc: 0.8263\n",
      "Epoch 12/15 | Train Loss: 0.3743 Acc: 0.8136\n",
      "Epoch 13/15 | Train Loss: 0.4091 Acc: 0.8178\n",
      "Epoch 14/15 | Train Loss: 0.3762 Acc: 0.8432\n",
      "Epoch 15/15 | Train Loss: 0.3611 Acc: 0.8390\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5131 Acc: 0.7709\n",
      "Epoch 2/15 | Train Loss: 0.4623 Acc: 0.8238\n",
      "Epoch 3/15 | Train Loss: 0.3986 Acc: 0.8502\n",
      "Epoch 4/15 | Train Loss: 0.3993 Acc: 0.8458\n",
      "Epoch 5/15 | Train Loss: 0.3805 Acc: 0.8150\n",
      "Epoch 6/15 | Train Loss: 0.4741 Acc: 0.8282\n",
      "Epoch 7/15 | Train Loss: 0.3494 Acc: 0.8634\n",
      "Epoch 8/15 | Train Loss: 0.3804 Acc: 0.8502\n",
      "Epoch 9/15 | Train Loss: 0.3196 Acc: 0.8502\n",
      "Epoch 10/15 | Train Loss: 0.3149 Acc: 0.8767\n",
      "Epoch 11/15 | Train Loss: 0.2887 Acc: 0.9075\n",
      "Epoch 12/15 | Train Loss: 0.2470 Acc: 0.9207\n",
      "Epoch 13/15 | Train Loss: 0.2738 Acc: 0.8943\n",
      "Epoch 14/15 | Train Loss: 0.2528 Acc: 0.9075\n",
      "Epoch 15/15 | Train Loss: 0.2641 Acc: 0.8987\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.4892 Acc: 0.7683\n",
      "Epoch 2/15 | Train Loss: 0.4378 Acc: 0.7936\n",
      "Epoch 3/15 | Train Loss: 0.3903 Acc: 0.8073\n",
      "Epoch 4/15 | Train Loss: 0.3482 Acc: 0.8326\n",
      "Epoch 5/15 | Train Loss: 0.3529 Acc: 0.8096\n",
      "Epoch 6/15 | Train Loss: 0.3239 Acc: 0.8417\n",
      "Epoch 7/15 | Train Loss: 0.3434 Acc: 0.8417\n",
      "Epoch 8/15 | Train Loss: 0.3036 Acc: 0.8463\n",
      "Epoch 9/15 | Train Loss: 0.2930 Acc: 0.8670\n",
      "Epoch 10/15 | Train Loss: 0.2755 Acc: 0.8899\n",
      "Epoch 11/15 | Train Loss: 0.2665 Acc: 0.8830\n",
      "Epoch 12/15 | Train Loss: 0.2693 Acc: 0.8945\n",
      "Epoch 13/15 | Train Loss: 0.2708 Acc: 0.8784\n",
      "Epoch 14/15 | Train Loss: 0.2908 Acc: 0.8670\n",
      "Epoch 15/15 | Train Loss: 0.2725 Acc: 0.8761\n",
      "Fold 2 Test Accuracy: 0.6364\n",
      "===== Fold 3 =====\n",
      "Epoch 1: Generator Loss = 9.5919, Discriminator Loss = 9.0786\n",
      "Epoch 2: Generator Loss = 10.4514, Discriminator Loss = 9.4216\n",
      "Epoch 3: Generator Loss = 11.5757, Discriminator Loss = 8.3664\n",
      "Epoch 4: Generator Loss = 16.0023, Discriminator Loss = 6.8176\n",
      "Epoch 5: Generator Loss = 23.0950, Discriminator Loss = 4.5143\n",
      "Epoch 6: Generator Loss = 30.1140, Discriminator Loss = 3.5131\n",
      "Epoch 7: Generator Loss = 39.3201, Discriminator Loss = 3.0782\n",
      "Epoch 8: Generator Loss = 37.9350, Discriminator Loss = 3.0398\n",
      "Epoch 9: Generator Loss = 39.8761, Discriminator Loss = 4.2941\n",
      "Epoch 10: Generator Loss = 33.2986, Discriminator Loss = 5.0198\n",
      "Epoch 11: Generator Loss = 38.3143, Discriminator Loss = 4.9038\n",
      "Epoch 12: Generator Loss = 31.0985, Discriminator Loss = 6.2334\n",
      "Epoch 13: Generator Loss = 32.0069, Discriminator Loss = 9.5975\n",
      "Epoch 14: Generator Loss = 24.0276, Discriminator Loss = 9.2594\n",
      "Epoch 15: Generator Loss = 32.1760, Discriminator Loss = 6.3697\n",
      "Epoch 16: Generator Loss = 27.4603, Discriminator Loss = 6.1280\n",
      "Epoch 17: Generator Loss = 21.5853, Discriminator Loss = 8.8972\n",
      "Epoch 18: Generator Loss = 25.1433, Discriminator Loss = 7.6800\n",
      "Epoch 19: Generator Loss = 19.1995, Discriminator Loss = 8.5404\n",
      "Epoch 20: Generator Loss = 17.4831, Discriminator Loss = 8.7318\n",
      "Epoch 21: Generator Loss = 13.3378, Discriminator Loss = 8.7120\n",
      "Epoch 22: Generator Loss = 13.9403, Discriminator Loss = 8.4586\n",
      "Epoch 23: Generator Loss = 19.4676, Discriminator Loss = 7.2215\n",
      "Epoch 24: Generator Loss = 18.6564, Discriminator Loss = 9.7013\n",
      "Epoch 25: Generator Loss = 21.5724, Discriminator Loss = 7.3735\n",
      "Epoch 26: Generator Loss = 19.4945, Discriminator Loss = 7.7521\n",
      "Epoch 27: Generator Loss = 20.6364, Discriminator Loss = 6.6382\n",
      "Epoch 28: Generator Loss = 20.6965, Discriminator Loss = 6.3553\n",
      "Epoch 29: Generator Loss = 24.6732, Discriminator Loss = 7.0119\n",
      "Epoch 30: Generator Loss = 21.5337, Discriminator Loss = 8.5073\n",
      "Epoch 31: Generator Loss = 27.4009, Discriminator Loss = 7.5253\n",
      "Epoch 32: Generator Loss = 20.6810, Discriminator Loss = 8.3203\n",
      "Epoch 33: Generator Loss = 26.4000, Discriminator Loss = 9.2221\n",
      "Epoch 34: Generator Loss = 23.9047, Discriminator Loss = 7.8162\n",
      "Epoch 35: Generator Loss = 17.3209, Discriminator Loss = 8.3866\n",
      "Epoch 36: Generator Loss = 17.4576, Discriminator Loss = 7.1437\n",
      "Epoch 37: Generator Loss = 20.8909, Discriminator Loss = 8.9285\n",
      "Epoch 38: Generator Loss = 23.2249, Discriminator Loss = 8.9374\n",
      "Epoch 39: Generator Loss = 16.6584, Discriminator Loss = 7.7963\n",
      "Epoch 40: Generator Loss = 18.4988, Discriminator Loss = 8.0422\n",
      "Epoch 41: Generator Loss = 18.7519, Discriminator Loss = 10.8433\n",
      "Epoch 42: Generator Loss = 16.4599, Discriminator Loss = 8.5137\n",
      "Epoch 43: Generator Loss = 14.9249, Discriminator Loss = 7.0642\n",
      "Epoch 44: Generator Loss = 19.2289, Discriminator Loss = 6.8480\n",
      "Epoch 45: Generator Loss = 17.4868, Discriminator Loss = 8.7093\n",
      "Epoch 46: Generator Loss = 15.9574, Discriminator Loss = 7.9089\n",
      "Epoch 47: Generator Loss = 19.7865, Discriminator Loss = 6.9007\n",
      "Epoch 48: Generator Loss = 20.7191, Discriminator Loss = 5.7145\n",
      "Epoch 49: Generator Loss = 27.0722, Discriminator Loss = 6.4737\n",
      "Epoch 50: Generator Loss = 19.1077, Discriminator Loss = 7.2906\n",
      "Epoch 51: Generator Loss = 17.6480, Discriminator Loss = 8.7319\n",
      "Epoch 52: Generator Loss = 14.5550, Discriminator Loss = 7.6644\n",
      "Epoch 53: Generator Loss = 15.4982, Discriminator Loss = 7.7657\n",
      "Epoch 54: Generator Loss = 27.3102, Discriminator Loss = 9.5827\n",
      "Epoch 55: Generator Loss = 16.6774, Discriminator Loss = 9.6493\n",
      "Epoch 56: Generator Loss = 15.1422, Discriminator Loss = 6.9926\n",
      "Epoch 57: Generator Loss = 20.1479, Discriminator Loss = 7.4134\n",
      "Epoch 58: Generator Loss = 23.8256, Discriminator Loss = 7.2162\n",
      "Epoch 59: Generator Loss = 20.5858, Discriminator Loss = 7.4476\n",
      "Epoch 60: Generator Loss = 15.1193, Discriminator Loss = 7.9386\n",
      "Epoch 61: Generator Loss = 18.1537, Discriminator Loss = 6.3079\n",
      "Epoch 62: Generator Loss = 23.4841, Discriminator Loss = 5.8418\n",
      "Epoch 63: Generator Loss = 24.5447, Discriminator Loss = 4.9568\n",
      "Epoch 64: Generator Loss = 23.8910, Discriminator Loss = 5.8568\n",
      "Epoch 65: Generator Loss = 27.5545, Discriminator Loss = 5.0880\n",
      "Epoch 66: Generator Loss = 26.0890, Discriminator Loss = 4.8397\n",
      "Epoch 67: Generator Loss = 29.1094, Discriminator Loss = 5.3768\n",
      "Epoch 68: Generator Loss = 26.0588, Discriminator Loss = 4.6629\n",
      "Epoch 69: Generator Loss = 34.1852, Discriminator Loss = 3.4779\n",
      "Epoch 70: Generator Loss = 23.1363, Discriminator Loss = 6.4855\n",
      "Epoch 71: Generator Loss = 30.6407, Discriminator Loss = 4.1370\n",
      "Epoch 72: Generator Loss = 33.4034, Discriminator Loss = 4.5983\n",
      "Epoch 73: Generator Loss = 35.4557, Discriminator Loss = 4.2582\n",
      "Epoch 74: Generator Loss = 40.1573, Discriminator Loss = 3.9229\n",
      "Epoch 75: Generator Loss = 34.5036, Discriminator Loss = 3.5619\n",
      "Epoch 76: Generator Loss = 34.7794, Discriminator Loss = 5.3444\n",
      "Epoch 77: Generator Loss = 41.3701, Discriminator Loss = 4.0906\n",
      "Epoch 78: Generator Loss = 33.2020, Discriminator Loss = 3.0794\n",
      "Epoch 79: Generator Loss = 47.9039, Discriminator Loss = 3.6623\n",
      "Epoch 80: Generator Loss = 37.1232, Discriminator Loss = 2.4937\n",
      "Epoch 81: Generator Loss = 50.7471, Discriminator Loss = 2.5598\n",
      "Epoch 82: Generator Loss = 51.6646, Discriminator Loss = 4.8376\n",
      "Epoch 83: Generator Loss = 47.1836, Discriminator Loss = 1.4843\n",
      "Epoch 84: Generator Loss = 51.4324, Discriminator Loss = 1.7769\n",
      "Epoch 85: Generator Loss = 44.0153, Discriminator Loss = 2.0089\n",
      "Epoch 86: Generator Loss = 58.7915, Discriminator Loss = 2.4896\n",
      "Epoch 87: Generator Loss = 48.0745, Discriminator Loss = 1.6362\n",
      "Epoch 88: Generator Loss = 58.2100, Discriminator Loss = 1.8140\n",
      "Epoch 89: Generator Loss = 51.0602, Discriminator Loss = 1.4057\n",
      "Epoch 90: Generator Loss = 53.0960, Discriminator Loss = 1.3113\n",
      "Epoch 91: Generator Loss = 53.4387, Discriminator Loss = 6.0274\n",
      "Epoch 92: Generator Loss = 54.7227, Discriminator Loss = 3.0171\n",
      "Epoch 93: Generator Loss = 48.8814, Discriminator Loss = 1.2294\n",
      "Epoch 94: Generator Loss = 47.7249, Discriminator Loss = 1.4759\n",
      "Epoch 95: Generator Loss = 54.5398, Discriminator Loss = 0.9679\n",
      "Epoch 96: Generator Loss = 49.6005, Discriminator Loss = 1.6992\n",
      "Epoch 97: Generator Loss = 58.3066, Discriminator Loss = 1.2261\n",
      "Epoch 98: Generator Loss = 64.7632, Discriminator Loss = 2.3663\n",
      "Epoch 99: Generator Loss = 59.6013, Discriminator Loss = 1.1188\n",
      "Epoch 100: Generator Loss = 60.9046, Discriminator Loss = 4.4183\n",
      "Epoch 101: Generator Loss = 37.8066, Discriminator Loss = 4.4807\n",
      "Epoch 102: Generator Loss = 55.8758, Discriminator Loss = 1.5491\n",
      "Epoch 103: Generator Loss = 62.1301, Discriminator Loss = 1.4804\n",
      "Epoch 104: Generator Loss = 63.2792, Discriminator Loss = 0.8959\n",
      "Epoch 105: Generator Loss = 61.4807, Discriminator Loss = 0.5362\n",
      "Epoch 106: Generator Loss = 61.1007, Discriminator Loss = 1.5476\n",
      "Epoch 107: Generator Loss = 52.9245, Discriminator Loss = 0.9823\n",
      "Epoch 108: Generator Loss = 63.6566, Discriminator Loss = 1.4961\n",
      "Epoch 109: Generator Loss = 84.0130, Discriminator Loss = 1.9241\n",
      "Epoch 110: Generator Loss = 63.6066, Discriminator Loss = 1.2276\n",
      "Epoch 111: Generator Loss = 68.2059, Discriminator Loss = 1.0438\n",
      "Epoch 112: Generator Loss = 72.4547, Discriminator Loss = 0.9407\n",
      "Epoch 113: Generator Loss = 63.2429, Discriminator Loss = 0.9522\n",
      "Epoch 114: Generator Loss = 76.4608, Discriminator Loss = 1.0053\n",
      "Epoch 115: Generator Loss = 75.3315, Discriminator Loss = 1.2123\n",
      "Epoch 116: Generator Loss = 68.5702, Discriminator Loss = 0.6545\n",
      "Epoch 117: Generator Loss = 68.4098, Discriminator Loss = 0.5918\n",
      "Epoch 118: Generator Loss = 68.2821, Discriminator Loss = 1.0397\n",
      "Epoch 119: Generator Loss = 49.8951, Discriminator Loss = 4.8432\n",
      "Epoch 120: Generator Loss = 53.8035, Discriminator Loss = 9.3024\n",
      "Epoch 121: Generator Loss = 48.8073, Discriminator Loss = 3.0769\n",
      "Epoch 122: Generator Loss = 64.1805, Discriminator Loss = 1.3502\n",
      "Epoch 123: Generator Loss = 60.9916, Discriminator Loss = 1.4492\n",
      "Epoch 124: Generator Loss = 58.5388, Discriminator Loss = 1.9056\n",
      "Epoch 125: Generator Loss = 75.1037, Discriminator Loss = 1.0945\n",
      "Epoch 126: Generator Loss = 78.6962, Discriminator Loss = 0.7229\n",
      "Epoch 127: Generator Loss = 76.2053, Discriminator Loss = 0.4939\n",
      "Epoch 128: Generator Loss = 79.0400, Discriminator Loss = 0.8369\n",
      "Epoch 129: Generator Loss = 65.6537, Discriminator Loss = 0.8347\n",
      "Epoch 130: Generator Loss = 65.1109, Discriminator Loss = 1.2048\n",
      "Epoch 131: Generator Loss = 73.3471, Discriminator Loss = 1.3763\n",
      "Epoch 132: Generator Loss = 57.5046, Discriminator Loss = 0.6754\n",
      "Epoch 133: Generator Loss = 67.4379, Discriminator Loss = 1.1308\n",
      "Epoch 134: Generator Loss = 69.2941, Discriminator Loss = 0.5328\n",
      "Epoch 135: Generator Loss = 81.6894, Discriminator Loss = 1.0108\n",
      "Epoch 136: Generator Loss = 60.7396, Discriminator Loss = 0.8069\n",
      "Epoch 137: Generator Loss = 65.4990, Discriminator Loss = 2.6988\n",
      "Epoch 138: Generator Loss = 63.5756, Discriminator Loss = 1.7764\n",
      "Epoch 139: Generator Loss = 79.4645, Discriminator Loss = 1.1126\n",
      "Epoch 140: Generator Loss = 68.3803, Discriminator Loss = 0.4218\n",
      "Epoch 141: Generator Loss = 72.7216, Discriminator Loss = 2.6093\n",
      "Epoch 142: Generator Loss = 72.1768, Discriminator Loss = 1.7252\n",
      "Epoch 143: Generator Loss = 59.8791, Discriminator Loss = 0.6045\n",
      "Epoch 144: Generator Loss = 62.0365, Discriminator Loss = 1.3823\n",
      "Epoch 145: Generator Loss = 70.9453, Discriminator Loss = 1.1363\n",
      "Epoch 146: Generator Loss = 53.7819, Discriminator Loss = 0.6822\n",
      "Epoch 147: Generator Loss = 66.8258, Discriminator Loss = 0.8686\n",
      "Epoch 148: Generator Loss = 66.2881, Discriminator Loss = 1.1156\n",
      "Epoch 149: Generator Loss = 50.3793, Discriminator Loss = 1.1681\n",
      "Epoch 150: Generator Loss = 59.1104, Discriminator Loss = 0.7831\n",
      "Epoch 151: Generator Loss = 69.6550, Discriminator Loss = 0.5679\n",
      "Epoch 152: Generator Loss = 83.4919, Discriminator Loss = 1.8235\n",
      "Epoch 153: Generator Loss = 60.1917, Discriminator Loss = 12.7939\n",
      "Epoch 154: Generator Loss = 43.0842, Discriminator Loss = 8.9845\n",
      "Epoch 155: Generator Loss = 33.7809, Discriminator Loss = 3.6844\n",
      "Epoch 156: Generator Loss = 56.2786, Discriminator Loss = 3.5042\n",
      "Epoch 157: Generator Loss = 58.2352, Discriminator Loss = 2.5736\n",
      "Epoch 158: Generator Loss = 46.9987, Discriminator Loss = 3.4160\n",
      "Epoch 159: Generator Loss = 65.9958, Discriminator Loss = 1.8210\n",
      "Epoch 160: Generator Loss = 53.2399, Discriminator Loss = 2.0530\n",
      "Epoch 161: Generator Loss = 72.5438, Discriminator Loss = 1.3757\n",
      "Epoch 162: Generator Loss = 78.8249, Discriminator Loss = 1.3537\n",
      "Epoch 163: Generator Loss = 61.7786, Discriminator Loss = 1.5111\n",
      "Epoch 164: Generator Loss = 76.1335, Discriminator Loss = 2.6088\n",
      "Epoch 165: Generator Loss = 69.7474, Discriminator Loss = 1.0089\n",
      "Epoch 166: Generator Loss = 77.1347, Discriminator Loss = 1.5269\n",
      "Epoch 167: Generator Loss = 50.7135, Discriminator Loss = 2.6889\n",
      "Epoch 168: Generator Loss = 84.8812, Discriminator Loss = 1.7986\n",
      "Epoch 169: Generator Loss = 56.8829, Discriminator Loss = 1.6468\n",
      "Epoch 170: Generator Loss = 67.8155, Discriminator Loss = 2.5766\n",
      "Epoch 171: Generator Loss = 49.8154, Discriminator Loss = 12.3828\n",
      "Epoch 172: Generator Loss = 47.3937, Discriminator Loss = 3.9573\n",
      "Epoch 173: Generator Loss = 54.1700, Discriminator Loss = 2.1442\n",
      "Epoch 174: Generator Loss = 60.9159, Discriminator Loss = 1.5327\n",
      "Epoch 175: Generator Loss = 82.5497, Discriminator Loss = 1.3562\n",
      "Epoch 176: Generator Loss = 68.5242, Discriminator Loss = 1.2739\n",
      "Epoch 177: Generator Loss = 82.3890, Discriminator Loss = 1.7705\n",
      "Epoch 178: Generator Loss = 79.8533, Discriminator Loss = 1.4433\n",
      "Epoch 179: Generator Loss = 74.9536, Discriminator Loss = 1.5204\n",
      "Epoch 180: Generator Loss = 73.0562, Discriminator Loss = 0.7469\n",
      "Epoch 181: Generator Loss = 71.2691, Discriminator Loss = 0.8612\n",
      "Epoch 182: Generator Loss = 59.3229, Discriminator Loss = 0.6813\n",
      "Epoch 183: Generator Loss = 60.6525, Discriminator Loss = 1.9747\n",
      "Epoch 184: Generator Loss = 69.8548, Discriminator Loss = 1.5770\n",
      "Epoch 185: Generator Loss = 65.2560, Discriminator Loss = 0.5422\n",
      "Epoch 186: Generator Loss = 74.9159, Discriminator Loss = 1.0374\n",
      "Epoch 187: Generator Loss = 67.3923, Discriminator Loss = 2.9913\n",
      "Epoch 188: Generator Loss = 77.1381, Discriminator Loss = 1.0432\n",
      "Epoch 189: Generator Loss = 74.3499, Discriminator Loss = 1.6480\n",
      "Epoch 190: Generator Loss = 63.8783, Discriminator Loss = 0.5988\n",
      "Epoch 191: Generator Loss = 74.7729, Discriminator Loss = 0.9857\n",
      "Epoch 192: Generator Loss = 72.3163, Discriminator Loss = 1.1225\n",
      "Epoch 193: Generator Loss = 81.9595, Discriminator Loss = 1.0275\n",
      "Epoch 194: Generator Loss = 86.7144, Discriminator Loss = 0.5098\n",
      "Epoch 195: Generator Loss = 59.0346, Discriminator Loss = 1.0347\n",
      "Epoch 196: Generator Loss = 65.2364, Discriminator Loss = 1.9997\n",
      "Epoch 197: Generator Loss = 80.1752, Discriminator Loss = 1.3423\n",
      "Epoch 198: Generator Loss = 67.8941, Discriminator Loss = 1.1757\n",
      "Epoch 199: Generator Loss = 74.3308, Discriminator Loss = 1.1365\n",
      "Epoch 200: Generator Loss = 64.0860, Discriminator Loss = 1.1764\n",
      "Epoch 201: Generator Loss = 89.9256, Discriminator Loss = 0.7571\n",
      "Epoch 202: Generator Loss = 89.4096, Discriminator Loss = 1.4660\n",
      "Epoch 203: Generator Loss = 64.7376, Discriminator Loss = 1.6571\n",
      "Epoch 204: Generator Loss = 82.6102, Discriminator Loss = 0.7115\n",
      "Epoch 205: Generator Loss = 87.0201, Discriminator Loss = 2.2263\n",
      "Epoch 206: Generator Loss = 78.0769, Discriminator Loss = 7.1370\n",
      "Epoch 207: Generator Loss = 47.4412, Discriminator Loss = 3.2535\n",
      "Epoch 208: Generator Loss = 58.0556, Discriminator Loss = 1.6652\n",
      "Epoch 209: Generator Loss = 65.3888, Discriminator Loss = 1.2744\n",
      "Epoch 210: Generator Loss = 70.7559, Discriminator Loss = 2.2297\n",
      "Epoch 211: Generator Loss = 78.0792, Discriminator Loss = 4.5569\n",
      "Epoch 212: Generator Loss = 65.3727, Discriminator Loss = 1.0250\n",
      "Epoch 213: Generator Loss = 80.5379, Discriminator Loss = 1.5874\n",
      "Epoch 214: Generator Loss = 69.4013, Discriminator Loss = 2.3921\n",
      "Epoch 215: Generator Loss = 92.8220, Discriminator Loss = 1.9509\n",
      "Epoch 216: Generator Loss = 78.2338, Discriminator Loss = 2.0048\n",
      "Epoch 217: Generator Loss = 75.1462, Discriminator Loss = 1.5758\n",
      "Epoch 218: Generator Loss = 82.5741, Discriminator Loss = 0.9107\n",
      "Epoch 219: Generator Loss = 79.3404, Discriminator Loss = 0.7315\n",
      "Epoch 220: Generator Loss = 92.9751, Discriminator Loss = 0.9071\n",
      "Epoch 221: Generator Loss = 81.9565, Discriminator Loss = 0.7478\n",
      "Epoch 222: Generator Loss = 87.5515, Discriminator Loss = 0.7328\n",
      "Epoch 223: Generator Loss = 89.4275, Discriminator Loss = 1.2150\n",
      "Epoch 224: Generator Loss = 96.5707, Discriminator Loss = 3.6276\n",
      "Epoch 225: Generator Loss = 60.2622, Discriminator Loss = 1.2275\n",
      "Epoch 226: Generator Loss = 60.7016, Discriminator Loss = 1.9068\n",
      "Epoch 227: Generator Loss = 79.4430, Discriminator Loss = 1.5812\n",
      "Epoch 228: Generator Loss = 76.0459, Discriminator Loss = 1.3767\n",
      "Epoch 229: Generator Loss = 79.1796, Discriminator Loss = 1.1352\n",
      "Epoch 230: Generator Loss = 73.1508, Discriminator Loss = 1.9747\n",
      "Epoch 231: Generator Loss = 81.0089, Discriminator Loss = 0.5512\n",
      "Epoch 232: Generator Loss = 61.8397, Discriminator Loss = 1.0353\n",
      "Epoch 233: Generator Loss = 70.6173, Discriminator Loss = 1.6262\n",
      "Epoch 234: Generator Loss = 84.0031, Discriminator Loss = 1.3847\n",
      "Epoch 235: Generator Loss = 61.6397, Discriminator Loss = 4.8486\n",
      "Epoch 236: Generator Loss = 70.4031, Discriminator Loss = 2.9207\n",
      "Epoch 237: Generator Loss = 68.5774, Discriminator Loss = 1.5133\n",
      "Epoch 238: Generator Loss = 78.1486, Discriminator Loss = 0.9592\n",
      "Epoch 239: Generator Loss = 62.4498, Discriminator Loss = 0.6331\n",
      "Epoch 240: Generator Loss = 93.4758, Discriminator Loss = 0.9234\n",
      "Epoch 241: Generator Loss = 78.6345, Discriminator Loss = 0.9730\n",
      "Epoch 242: Generator Loss = 101.9948, Discriminator Loss = 0.7119\n",
      "Epoch 243: Generator Loss = 88.9774, Discriminator Loss = 0.6180\n",
      "Epoch 244: Generator Loss = 79.7655, Discriminator Loss = 0.7755\n",
      "Epoch 245: Generator Loss = 79.5689, Discriminator Loss = 1.0751\n",
      "Epoch 246: Generator Loss = 62.3615, Discriminator Loss = 3.7722\n",
      "Epoch 247: Generator Loss = 59.0739, Discriminator Loss = 2.0321\n",
      "Epoch 248: Generator Loss = 81.2334, Discriminator Loss = 3.8387\n",
      "Epoch 249: Generator Loss = 70.6332, Discriminator Loss = 8.8850\n",
      "Epoch 250: Generator Loss = 43.5981, Discriminator Loss = 3.9290\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.5313 Acc: 0.7258\n",
      "Epoch 2/15 | Train Loss: 0.4012 Acc: 0.7911\n",
      "Epoch 3/15 | Train Loss: 0.3977 Acc: 0.8198\n",
      "Epoch 4/15 | Train Loss: 0.3396 Acc: 0.8407\n",
      "Epoch 5/15 | Train Loss: 0.3206 Acc: 0.8355\n",
      "Epoch 6/15 | Train Loss: 0.3490 Acc: 0.8172\n",
      "Epoch 7/15 | Train Loss: 0.3161 Acc: 0.8512\n",
      "Epoch 8/15 | Train Loss: 0.2722 Acc: 0.8799\n",
      "Epoch 9/15 | Train Loss: 0.2768 Acc: 0.8721\n",
      "Epoch 10/15 | Train Loss: 0.2626 Acc: 0.8773\n",
      "Epoch 11/15 | Train Loss: 0.2635 Acc: 0.8799\n",
      "Epoch 12/15 | Train Loss: 0.2282 Acc: 0.8851\n",
      "Epoch 13/15 | Train Loss: 0.2424 Acc: 0.8799\n",
      "Epoch 14/15 | Train Loss: 0.2576 Acc: 0.8825\n",
      "Epoch 15/15 | Train Loss: 0.2407 Acc: 0.8747\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5720 Acc: 0.7363\n",
      "Epoch 2/15 | Train Loss: 0.4119 Acc: 0.7781\n",
      "Epoch 3/15 | Train Loss: 0.3953 Acc: 0.7833\n",
      "Epoch 4/15 | Train Loss: 0.3801 Acc: 0.7990\n",
      "Epoch 5/15 | Train Loss: 0.3589 Acc: 0.8146\n",
      "Epoch 6/15 | Train Loss: 0.3509 Acc: 0.8094\n",
      "Epoch 7/15 | Train Loss: 0.2969 Acc: 0.8564\n",
      "Epoch 8/15 | Train Loss: 0.2596 Acc: 0.8616\n",
      "Epoch 9/15 | Train Loss: 0.2864 Acc: 0.8825\n",
      "Epoch 10/15 | Train Loss: 0.2796 Acc: 0.8616\n",
      "Epoch 11/15 | Train Loss: 0.2873 Acc: 0.8564\n",
      "Epoch 12/15 | Train Loss: 0.2993 Acc: 0.8590\n",
      "Epoch 13/15 | Train Loss: 0.2890 Acc: 0.8538\n",
      "Epoch 14/15 | Train Loss: 0.2804 Acc: 0.8773\n",
      "Epoch 15/15 | Train Loss: 0.2912 Acc: 0.8433\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5001 Acc: 0.7760\n",
      "Epoch 2/15 | Train Loss: 0.4350 Acc: 0.7474\n",
      "Epoch 3/15 | Train Loss: 0.3773 Acc: 0.8021\n",
      "Epoch 4/15 | Train Loss: 0.3592 Acc: 0.8073\n",
      "Epoch 5/15 | Train Loss: 0.3434 Acc: 0.8229\n",
      "Epoch 6/15 | Train Loss: 0.3522 Acc: 0.8359\n",
      "Epoch 7/15 | Train Loss: 0.3207 Acc: 0.8255\n",
      "Epoch 8/15 | Train Loss: 0.3312 Acc: 0.8411\n",
      "Epoch 9/15 | Train Loss: 0.2831 Acc: 0.8568\n",
      "Epoch 10/15 | Train Loss: 0.3077 Acc: 0.8359\n",
      "Epoch 11/15 | Train Loss: 0.3127 Acc: 0.8411\n",
      "Epoch 12/15 | Train Loss: 0.2471 Acc: 0.8906\n",
      "Epoch 13/15 | Train Loss: 0.3105 Acc: 0.8411\n",
      "Epoch 14/15 | Train Loss: 0.2576 Acc: 0.8776\n",
      "Epoch 15/15 | Train Loss: 0.2697 Acc: 0.8620\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4934 Acc: 0.7866\n",
      "Epoch 2/15 | Train Loss: 0.4027 Acc: 0.8344\n",
      "Epoch 3/15 | Train Loss: 0.3580 Acc: 0.8248\n",
      "Epoch 4/15 | Train Loss: 0.3825 Acc: 0.8344\n",
      "Epoch 5/15 | Train Loss: 0.4018 Acc: 0.8312\n",
      "Epoch 6/15 | Train Loss: 0.3757 Acc: 0.8471\n",
      "Epoch 7/15 | Train Loss: 0.3871 Acc: 0.8408\n",
      "Epoch 8/15 | Train Loss: 0.3007 Acc: 0.8631\n",
      "Epoch 9/15 | Train Loss: 0.2813 Acc: 0.8790\n",
      "Epoch 10/15 | Train Loss: 0.2704 Acc: 0.8885\n",
      "Epoch 11/15 | Train Loss: 0.2939 Acc: 0.8726\n",
      "Epoch 12/15 | Train Loss: 0.3062 Acc: 0.8694\n",
      "Epoch 13/15 | Train Loss: 0.2786 Acc: 0.8790\n",
      "Epoch 14/15 | Train Loss: 0.3136 Acc: 0.8631\n",
      "Epoch 15/15 | Train Loss: 0.2814 Acc: 0.8822\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5514 Acc: 0.7929\n",
      "Epoch 2/15 | Train Loss: 0.3679 Acc: 0.8536\n",
      "Epoch 3/15 | Train Loss: 0.3279 Acc: 0.8750\n",
      "Epoch 4/15 | Train Loss: 0.3387 Acc: 0.8607\n",
      "Epoch 5/15 | Train Loss: 0.2875 Acc: 0.8857\n",
      "Epoch 6/15 | Train Loss: 0.3160 Acc: 0.8857\n",
      "Epoch 7/15 | Train Loss: 0.2555 Acc: 0.9036\n",
      "Epoch 8/15 | Train Loss: 0.2470 Acc: 0.9036\n",
      "Epoch 9/15 | Train Loss: 0.2393 Acc: 0.9036\n",
      "Epoch 10/15 | Train Loss: 0.2465 Acc: 0.9179\n",
      "Epoch 11/15 | Train Loss: 0.2286 Acc: 0.9071\n",
      "Epoch 12/15 | Train Loss: 0.2323 Acc: 0.9036\n",
      "Epoch 13/15 | Train Loss: 0.2286 Acc: 0.9071\n",
      "Epoch 14/15 | Train Loss: 0.1832 Acc: 0.9286\n",
      "Epoch 15/15 | Train Loss: 0.2375 Acc: 0.9036\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7252 Acc: 0.5992\n",
      "Epoch 2/15 | Train Loss: 0.6319 Acc: 0.6587\n",
      "Epoch 3/15 | Train Loss: 0.5800 Acc: 0.6865\n",
      "Epoch 4/15 | Train Loss: 0.5015 Acc: 0.7341\n",
      "Epoch 5/15 | Train Loss: 0.5085 Acc: 0.7381\n",
      "Epoch 6/15 | Train Loss: 0.4660 Acc: 0.7857\n",
      "Epoch 7/15 | Train Loss: 0.4135 Acc: 0.7937\n",
      "Epoch 8/15 | Train Loss: 0.4124 Acc: 0.7897\n",
      "Epoch 9/15 | Train Loss: 0.4141 Acc: 0.7897\n",
      "Epoch 10/15 | Train Loss: 0.4267 Acc: 0.7698\n",
      "Epoch 11/15 | Train Loss: 0.3885 Acc: 0.7857\n",
      "Epoch 12/15 | Train Loss: 0.3810 Acc: 0.8333\n",
      "Epoch 13/15 | Train Loss: 0.3679 Acc: 0.8214\n",
      "Epoch 14/15 | Train Loss: 0.4285 Acc: 0.8016\n",
      "Epoch 15/15 | Train Loss: 0.3821 Acc: 0.8095\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7381 Acc: 0.6508\n",
      "Epoch 2/15 | Train Loss: 0.5970 Acc: 0.6746\n",
      "Epoch 3/15 | Train Loss: 0.5695 Acc: 0.6905\n",
      "Epoch 4/15 | Train Loss: 0.5373 Acc: 0.7262\n",
      "Epoch 5/15 | Train Loss: 0.5228 Acc: 0.7381\n",
      "Epoch 6/15 | Train Loss: 0.4691 Acc: 0.7540\n",
      "Epoch 7/15 | Train Loss: 0.4444 Acc: 0.8095\n",
      "Epoch 8/15 | Train Loss: 0.4471 Acc: 0.8056\n",
      "Epoch 9/15 | Train Loss: 0.4867 Acc: 0.7778\n",
      "Epoch 10/15 | Train Loss: 0.4393 Acc: 0.7738\n",
      "Epoch 11/15 | Train Loss: 0.4281 Acc: 0.8135\n",
      "Epoch 12/15 | Train Loss: 0.3959 Acc: 0.8175\n",
      "Epoch 13/15 | Train Loss: 0.3736 Acc: 0.8214\n",
      "Epoch 14/15 | Train Loss: 0.3957 Acc: 0.8373\n",
      "Epoch 15/15 | Train Loss: 0.4053 Acc: 0.8175\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7056 Acc: 0.6008\n",
      "Epoch 2/15 | Train Loss: 0.5542 Acc: 0.7233\n",
      "Epoch 3/15 | Train Loss: 0.5732 Acc: 0.6996\n",
      "Epoch 4/15 | Train Loss: 0.5373 Acc: 0.7668\n",
      "Epoch 5/15 | Train Loss: 0.4894 Acc: 0.7787\n",
      "Epoch 6/15 | Train Loss: 0.5034 Acc: 0.7470\n",
      "Epoch 7/15 | Train Loss: 0.4818 Acc: 0.7747\n",
      "Epoch 8/15 | Train Loss: 0.4794 Acc: 0.7668\n",
      "Epoch 9/15 | Train Loss: 0.4109 Acc: 0.8300\n",
      "Epoch 10/15 | Train Loss: 0.4402 Acc: 0.7668\n",
      "Epoch 11/15 | Train Loss: 0.4173 Acc: 0.7826\n",
      "Epoch 12/15 | Train Loss: 0.4133 Acc: 0.8103\n",
      "Epoch 13/15 | Train Loss: 0.4121 Acc: 0.7984\n",
      "Epoch 14/15 | Train Loss: 0.4124 Acc: 0.8063\n",
      "Epoch 15/15 | Train Loss: 0.4060 Acc: 0.8182\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6171 Acc: 0.7161\n",
      "Epoch 2/15 | Train Loss: 0.6128 Acc: 0.7331\n",
      "Epoch 3/15 | Train Loss: 0.5226 Acc: 0.7669\n",
      "Epoch 4/15 | Train Loss: 0.4525 Acc: 0.7712\n",
      "Epoch 5/15 | Train Loss: 0.4646 Acc: 0.7966\n",
      "Epoch 6/15 | Train Loss: 0.4941 Acc: 0.7839\n",
      "Epoch 7/15 | Train Loss: 0.4352 Acc: 0.8051\n",
      "Epoch 8/15 | Train Loss: 0.4030 Acc: 0.8263\n",
      "Epoch 9/15 | Train Loss: 0.4083 Acc: 0.7966\n",
      "Epoch 10/15 | Train Loss: 0.3565 Acc: 0.8347\n",
      "Epoch 11/15 | Train Loss: 0.3763 Acc: 0.8178\n",
      "Epoch 12/15 | Train Loss: 0.3933 Acc: 0.8051\n",
      "Epoch 13/15 | Train Loss: 0.4130 Acc: 0.8136\n",
      "Epoch 14/15 | Train Loss: 0.3747 Acc: 0.8347\n",
      "Epoch 15/15 | Train Loss: 0.3909 Acc: 0.8517\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7193 Acc: 0.6079\n",
      "Epoch 2/15 | Train Loss: 0.4687 Acc: 0.8238\n",
      "Epoch 3/15 | Train Loss: 0.4317 Acc: 0.8282\n",
      "Epoch 4/15 | Train Loss: 0.4040 Acc: 0.8414\n",
      "Epoch 5/15 | Train Loss: 0.3638 Acc: 0.8502\n",
      "Epoch 6/15 | Train Loss: 0.3922 Acc: 0.8546\n",
      "Epoch 7/15 | Train Loss: 0.3307 Acc: 0.8678\n",
      "Epoch 8/15 | Train Loss: 0.3084 Acc: 0.8855\n",
      "Epoch 9/15 | Train Loss: 0.3141 Acc: 0.8590\n",
      "Epoch 10/15 | Train Loss: 0.3285 Acc: 0.8943\n",
      "Epoch 11/15 | Train Loss: 0.3180 Acc: 0.8634\n",
      "Epoch 12/15 | Train Loss: 0.2907 Acc: 0.8943\n",
      "Epoch 13/15 | Train Loss: 0.3202 Acc: 0.8899\n",
      "Epoch 14/15 | Train Loss: 0.2866 Acc: 0.8899\n",
      "Epoch 15/15 | Train Loss: 0.2693 Acc: 0.8943\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6343 Acc: 0.6655\n",
      "Epoch 2/15 | Train Loss: 0.5173 Acc: 0.7669\n",
      "Epoch 3/15 | Train Loss: 0.4768 Acc: 0.7568\n",
      "Epoch 4/15 | Train Loss: 0.4881 Acc: 0.7196\n",
      "Epoch 5/15 | Train Loss: 0.4160 Acc: 0.7872\n",
      "Epoch 6/15 | Train Loss: 0.4453 Acc: 0.7838\n",
      "Epoch 7/15 | Train Loss: 0.3761 Acc: 0.8176\n",
      "Epoch 8/15 | Train Loss: 0.3638 Acc: 0.8142\n",
      "Epoch 9/15 | Train Loss: 0.3486 Acc: 0.8412\n",
      "Epoch 10/15 | Train Loss: 0.3338 Acc: 0.8311\n",
      "Epoch 11/15 | Train Loss: 0.3432 Acc: 0.8041\n",
      "Epoch 12/15 | Train Loss: 0.3449 Acc: 0.8345\n",
      "Epoch 13/15 | Train Loss: 0.3226 Acc: 0.8514\n",
      "Epoch 14/15 | Train Loss: 0.3165 Acc: 0.8750\n",
      "Epoch 15/15 | Train Loss: 0.3237 Acc: 0.8277\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7267 Acc: 0.6824\n",
      "Epoch 2/15 | Train Loss: 0.5628 Acc: 0.7264\n",
      "Epoch 3/15 | Train Loss: 0.4906 Acc: 0.7331\n",
      "Epoch 4/15 | Train Loss: 0.5112 Acc: 0.7230\n",
      "Epoch 5/15 | Train Loss: 0.4165 Acc: 0.8007\n",
      "Epoch 6/15 | Train Loss: 0.4472 Acc: 0.7736\n",
      "Epoch 7/15 | Train Loss: 0.4660 Acc: 0.7264\n",
      "Epoch 8/15 | Train Loss: 0.3950 Acc: 0.8108\n",
      "Epoch 9/15 | Train Loss: 0.3923 Acc: 0.8108\n",
      "Epoch 10/15 | Train Loss: 0.4254 Acc: 0.7973\n",
      "Epoch 11/15 | Train Loss: 0.3891 Acc: 0.8074\n",
      "Epoch 12/15 | Train Loss: 0.3206 Acc: 0.8716\n",
      "Epoch 13/15 | Train Loss: 0.3250 Acc: 0.8649\n",
      "Epoch 14/15 | Train Loss: 0.3609 Acc: 0.8074\n",
      "Epoch 15/15 | Train Loss: 0.3594 Acc: 0.8243\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6300 Acc: 0.6667\n",
      "Epoch 2/15 | Train Loss: 0.5196 Acc: 0.7340\n",
      "Epoch 3/15 | Train Loss: 0.4650 Acc: 0.7475\n",
      "Epoch 4/15 | Train Loss: 0.4454 Acc: 0.7778\n",
      "Epoch 5/15 | Train Loss: 0.4190 Acc: 0.7778\n",
      "Epoch 6/15 | Train Loss: 0.4111 Acc: 0.7946\n",
      "Epoch 7/15 | Train Loss: 0.3804 Acc: 0.8114\n",
      "Epoch 8/15 | Train Loss: 0.3936 Acc: 0.8283\n",
      "Epoch 9/15 | Train Loss: 0.3854 Acc: 0.8182\n",
      "Epoch 10/15 | Train Loss: 0.3858 Acc: 0.8148\n",
      "Epoch 11/15 | Train Loss: 0.3563 Acc: 0.8148\n",
      "Epoch 12/15 | Train Loss: 0.3350 Acc: 0.8316\n",
      "Epoch 13/15 | Train Loss: 0.3521 Acc: 0.8249\n",
      "Epoch 14/15 | Train Loss: 0.3319 Acc: 0.8519\n",
      "Epoch 15/15 | Train Loss: 0.3221 Acc: 0.8384\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.6149 Acc: 0.7061\n",
      "Epoch 2/15 | Train Loss: 0.5179 Acc: 0.7748\n",
      "Epoch 3/15 | Train Loss: 0.5045 Acc: 0.8092\n",
      "Epoch 4/15 | Train Loss: 0.4469 Acc: 0.7863\n",
      "Epoch 5/15 | Train Loss: 0.4707 Acc: 0.7824\n",
      "Epoch 6/15 | Train Loss: 0.3753 Acc: 0.8282\n",
      "Epoch 7/15 | Train Loss: 0.3917 Acc: 0.8130\n",
      "Epoch 8/15 | Train Loss: 0.3768 Acc: 0.8511\n",
      "Epoch 9/15 | Train Loss: 0.4236 Acc: 0.8359\n",
      "Epoch 10/15 | Train Loss: 0.3546 Acc: 0.8435\n",
      "Epoch 11/15 | Train Loss: 0.3214 Acc: 0.8588\n",
      "Epoch 12/15 | Train Loss: 0.3246 Acc: 0.8588\n",
      "Epoch 13/15 | Train Loss: 0.3626 Acc: 0.8473\n",
      "Epoch 14/15 | Train Loss: 0.3476 Acc: 0.8664\n",
      "Epoch 15/15 | Train Loss: 0.3859 Acc: 0.8397\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6312 Acc: 0.6857\n",
      "Epoch 2/15 | Train Loss: 0.4321 Acc: 0.8367\n",
      "Epoch 3/15 | Train Loss: 0.4058 Acc: 0.8531\n",
      "Epoch 4/15 | Train Loss: 0.4037 Acc: 0.8286\n",
      "Epoch 5/15 | Train Loss: 0.3869 Acc: 0.8531\n",
      "Epoch 6/15 | Train Loss: 0.3874 Acc: 0.8776\n",
      "Epoch 7/15 | Train Loss: 0.3035 Acc: 0.8776\n",
      "Epoch 8/15 | Train Loss: 0.3252 Acc: 0.8449\n",
      "Epoch 9/15 | Train Loss: 0.2990 Acc: 0.8735\n",
      "Epoch 10/15 | Train Loss: 0.3205 Acc: 0.8694\n",
      "Epoch 11/15 | Train Loss: 0.3276 Acc: 0.8735\n",
      "Epoch 12/15 | Train Loss: 0.3226 Acc: 0.8571\n",
      "Epoch 13/15 | Train Loss: 0.2592 Acc: 0.8980\n",
      "Epoch 14/15 | Train Loss: 0.2548 Acc: 0.8980\n",
      "Epoch 15/15 | Train Loss: 0.2945 Acc: 0.8898\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7523 Acc: 0.6032\n",
      "Epoch 2/15 | Train Loss: 0.6366 Acc: 0.7024\n",
      "Epoch 3/15 | Train Loss: 0.5151 Acc: 0.7381\n",
      "Epoch 4/15 | Train Loss: 0.4703 Acc: 0.7698\n",
      "Epoch 5/15 | Train Loss: 0.4847 Acc: 0.7500\n",
      "Epoch 6/15 | Train Loss: 0.4621 Acc: 0.7897\n",
      "Epoch 7/15 | Train Loss: 0.5039 Acc: 0.7460\n",
      "Epoch 8/15 | Train Loss: 0.4314 Acc: 0.8095\n",
      "Epoch 9/15 | Train Loss: 0.3746 Acc: 0.8333\n",
      "Epoch 10/15 | Train Loss: 0.3639 Acc: 0.8413\n",
      "Epoch 11/15 | Train Loss: 0.3934 Acc: 0.8294\n",
      "Epoch 12/15 | Train Loss: 0.4188 Acc: 0.8254\n",
      "Epoch 13/15 | Train Loss: 0.3613 Acc: 0.8532\n",
      "Epoch 14/15 | Train Loss: 0.3783 Acc: 0.8294\n",
      "Epoch 15/15 | Train Loss: 0.3757 Acc: 0.8254\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6889 Acc: 0.6746\n",
      "Epoch 2/15 | Train Loss: 0.6420 Acc: 0.6706\n",
      "Epoch 3/15 | Train Loss: 0.5203 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.5251 Acc: 0.7262\n",
      "Epoch 5/15 | Train Loss: 0.5283 Acc: 0.7143\n",
      "Epoch 6/15 | Train Loss: 0.4733 Acc: 0.7698\n",
      "Epoch 7/15 | Train Loss: 0.4620 Acc: 0.7897\n",
      "Epoch 8/15 | Train Loss: 0.4545 Acc: 0.7857\n",
      "Epoch 9/15 | Train Loss: 0.4525 Acc: 0.7897\n",
      "Epoch 10/15 | Train Loss: 0.4299 Acc: 0.7897\n",
      "Epoch 11/15 | Train Loss: 0.4226 Acc: 0.7857\n",
      "Epoch 12/15 | Train Loss: 0.4058 Acc: 0.8016\n",
      "Epoch 13/15 | Train Loss: 0.3978 Acc: 0.8254\n",
      "Epoch 14/15 | Train Loss: 0.4014 Acc: 0.7976\n",
      "Epoch 15/15 | Train Loss: 0.4195 Acc: 0.7857\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7232 Acc: 0.6245\n",
      "Epoch 2/15 | Train Loss: 0.6080 Acc: 0.6996\n",
      "Epoch 3/15 | Train Loss: 0.5966 Acc: 0.7075\n",
      "Epoch 4/15 | Train Loss: 0.5544 Acc: 0.7075\n",
      "Epoch 5/15 | Train Loss: 0.4735 Acc: 0.7589\n",
      "Epoch 6/15 | Train Loss: 0.5980 Acc: 0.6877\n",
      "Epoch 7/15 | Train Loss: 0.4779 Acc: 0.7708\n",
      "Epoch 8/15 | Train Loss: 0.4404 Acc: 0.7787\n",
      "Epoch 9/15 | Train Loss: 0.4340 Acc: 0.7945\n",
      "Epoch 10/15 | Train Loss: 0.4294 Acc: 0.7668\n",
      "Epoch 11/15 | Train Loss: 0.4262 Acc: 0.8063\n",
      "Epoch 12/15 | Train Loss: 0.3945 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.3939 Acc: 0.7984\n",
      "Epoch 14/15 | Train Loss: 0.4018 Acc: 0.8024\n",
      "Epoch 15/15 | Train Loss: 0.4333 Acc: 0.7826\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6167 Acc: 0.6864\n",
      "Epoch 2/15 | Train Loss: 0.5518 Acc: 0.7712\n",
      "Epoch 3/15 | Train Loss: 0.5471 Acc: 0.7712\n",
      "Epoch 4/15 | Train Loss: 0.5156 Acc: 0.7458\n",
      "Epoch 5/15 | Train Loss: 0.4208 Acc: 0.8178\n",
      "Epoch 6/15 | Train Loss: 0.4457 Acc: 0.8008\n",
      "Epoch 7/15 | Train Loss: 0.4328 Acc: 0.8305\n",
      "Epoch 8/15 | Train Loss: 0.4488 Acc: 0.8093\n",
      "Epoch 9/15 | Train Loss: 0.4378 Acc: 0.8220\n",
      "Epoch 10/15 | Train Loss: 0.4166 Acc: 0.8178\n",
      "Epoch 11/15 | Train Loss: 0.3788 Acc: 0.8347\n",
      "Epoch 12/15 | Train Loss: 0.4121 Acc: 0.8263\n",
      "Epoch 13/15 | Train Loss: 0.3870 Acc: 0.8136\n",
      "Epoch 14/15 | Train Loss: 0.3833 Acc: 0.8136\n",
      "Epoch 15/15 | Train Loss: 0.3354 Acc: 0.8559\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5320 Acc: 0.7489\n",
      "Epoch 2/15 | Train Loss: 0.4307 Acc: 0.8238\n",
      "Epoch 3/15 | Train Loss: 0.4171 Acc: 0.8370\n",
      "Epoch 4/15 | Train Loss: 0.4341 Acc: 0.8282\n",
      "Epoch 5/15 | Train Loss: 0.3335 Acc: 0.8634\n",
      "Epoch 6/15 | Train Loss: 0.3552 Acc: 0.8326\n",
      "Epoch 7/15 | Train Loss: 0.3266 Acc: 0.8943\n",
      "Epoch 8/15 | Train Loss: 0.3630 Acc: 0.8722\n",
      "Epoch 9/15 | Train Loss: 0.3505 Acc: 0.8546\n",
      "Epoch 10/15 | Train Loss: 0.3500 Acc: 0.8634\n",
      "Epoch 11/15 | Train Loss: 0.2545 Acc: 0.9031\n",
      "Epoch 12/15 | Train Loss: 0.3092 Acc: 0.8767\n",
      "Epoch 13/15 | Train Loss: 0.2384 Acc: 0.9031\n",
      "Epoch 14/15 | Train Loss: 0.2699 Acc: 0.8943\n",
      "Epoch 15/15 | Train Loss: 0.2796 Acc: 0.8767\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.4719 Acc: 0.7546\n",
      "Epoch 2/15 | Train Loss: 0.3866 Acc: 0.7963\n",
      "Epoch 3/15 | Train Loss: 0.3886 Acc: 0.7885\n",
      "Epoch 4/15 | Train Loss: 0.3436 Acc: 0.8303\n",
      "Epoch 5/15 | Train Loss: 0.3215 Acc: 0.8564\n",
      "Epoch 6/15 | Train Loss: 0.3148 Acc: 0.8407\n",
      "Epoch 7/15 | Train Loss: 0.2752 Acc: 0.8486\n",
      "Epoch 8/15 | Train Loss: 0.2933 Acc: 0.8773\n",
      "Epoch 9/15 | Train Loss: 0.2658 Acc: 0.8668\n",
      "Epoch 10/15 | Train Loss: 0.2861 Acc: 0.8616\n",
      "Epoch 11/15 | Train Loss: 0.2347 Acc: 0.8825\n",
      "Epoch 12/15 | Train Loss: 0.2446 Acc: 0.9008\n",
      "Epoch 13/15 | Train Loss: 0.2447 Acc: 0.8825\n",
      "Epoch 14/15 | Train Loss: 0.2563 Acc: 0.8773\n",
      "Epoch 15/15 | Train Loss: 0.2160 Acc: 0.9008\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4877 Acc: 0.7520\n",
      "Epoch 2/15 | Train Loss: 0.3692 Acc: 0.8146\n",
      "Epoch 3/15 | Train Loss: 0.3724 Acc: 0.8016\n",
      "Epoch 4/15 | Train Loss: 0.3459 Acc: 0.8172\n",
      "Epoch 5/15 | Train Loss: 0.3246 Acc: 0.8303\n",
      "Epoch 6/15 | Train Loss: 0.3380 Acc: 0.8460\n",
      "Epoch 7/15 | Train Loss: 0.3251 Acc: 0.8198\n",
      "Epoch 8/15 | Train Loss: 0.2669 Acc: 0.8642\n",
      "Epoch 9/15 | Train Loss: 0.2806 Acc: 0.8695\n",
      "Epoch 10/15 | Train Loss: 0.2877 Acc: 0.8616\n",
      "Epoch 11/15 | Train Loss: 0.2767 Acc: 0.8773\n",
      "Epoch 12/15 | Train Loss: 0.2911 Acc: 0.8695\n",
      "Epoch 13/15 | Train Loss: 0.3096 Acc: 0.8407\n",
      "Epoch 14/15 | Train Loss: 0.2633 Acc: 0.8721\n",
      "Epoch 15/15 | Train Loss: 0.2727 Acc: 0.8747\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5754 Acc: 0.7578\n",
      "Epoch 2/15 | Train Loss: 0.4016 Acc: 0.8203\n",
      "Epoch 3/15 | Train Loss: 0.3755 Acc: 0.8151\n",
      "Epoch 4/15 | Train Loss: 0.3862 Acc: 0.8021\n",
      "Epoch 5/15 | Train Loss: 0.3871 Acc: 0.7917\n",
      "Epoch 6/15 | Train Loss: 0.3587 Acc: 0.8177\n",
      "Epoch 7/15 | Train Loss: 0.3395 Acc: 0.8307\n",
      "Epoch 8/15 | Train Loss: 0.2832 Acc: 0.8646\n",
      "Epoch 9/15 | Train Loss: 0.2680 Acc: 0.8776\n",
      "Epoch 10/15 | Train Loss: 0.2806 Acc: 0.8698\n",
      "Epoch 11/15 | Train Loss: 0.2657 Acc: 0.8698\n",
      "Epoch 12/15 | Train Loss: 0.2744 Acc: 0.8594\n",
      "Epoch 13/15 | Train Loss: 0.2697 Acc: 0.8646\n",
      "Epoch 14/15 | Train Loss: 0.2981 Acc: 0.8333\n",
      "Epoch 15/15 | Train Loss: 0.2815 Acc: 0.8620\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5880 Acc: 0.7803\n",
      "Epoch 2/15 | Train Loss: 0.4681 Acc: 0.7962\n",
      "Epoch 3/15 | Train Loss: 0.3830 Acc: 0.8217\n",
      "Epoch 4/15 | Train Loss: 0.4188 Acc: 0.8089\n",
      "Epoch 5/15 | Train Loss: 0.3766 Acc: 0.8280\n",
      "Epoch 6/15 | Train Loss: 0.3244 Acc: 0.8631\n",
      "Epoch 7/15 | Train Loss: 0.3783 Acc: 0.8471\n",
      "Epoch 8/15 | Train Loss: 0.3289 Acc: 0.8535\n",
      "Epoch 9/15 | Train Loss: 0.3035 Acc: 0.8631\n",
      "Epoch 10/15 | Train Loss: 0.3086 Acc: 0.8694\n",
      "Epoch 11/15 | Train Loss: 0.3001 Acc: 0.8694\n",
      "Epoch 12/15 | Train Loss: 0.2805 Acc: 0.8822\n",
      "Epoch 13/15 | Train Loss: 0.2774 Acc: 0.8790\n",
      "Epoch 14/15 | Train Loss: 0.2668 Acc: 0.8949\n",
      "Epoch 15/15 | Train Loss: 0.3040 Acc: 0.8662\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4581 Acc: 0.8357\n",
      "Epoch 2/15 | Train Loss: 0.4293 Acc: 0.8321\n",
      "Epoch 3/15 | Train Loss: 0.3583 Acc: 0.8714\n",
      "Epoch 4/15 | Train Loss: 0.3100 Acc: 0.8857\n",
      "Epoch 5/15 | Train Loss: 0.3278 Acc: 0.8679\n",
      "Epoch 6/15 | Train Loss: 0.3022 Acc: 0.8893\n",
      "Epoch 7/15 | Train Loss: 0.2804 Acc: 0.8964\n",
      "Epoch 8/15 | Train Loss: 0.2622 Acc: 0.9107\n",
      "Epoch 9/15 | Train Loss: 0.2423 Acc: 0.9143\n",
      "Epoch 10/15 | Train Loss: 0.2423 Acc: 0.9179\n",
      "Epoch 11/15 | Train Loss: 0.2256 Acc: 0.9143\n",
      "Epoch 12/15 | Train Loss: 0.2407 Acc: 0.9107\n",
      "Epoch 13/15 | Train Loss: 0.2359 Acc: 0.9036\n",
      "Epoch 14/15 | Train Loss: 0.2013 Acc: 0.9143\n",
      "Epoch 15/15 | Train Loss: 0.1948 Acc: 0.9357\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6959 Acc: 0.6429\n",
      "Epoch 2/15 | Train Loss: 0.5621 Acc: 0.7262\n",
      "Epoch 3/15 | Train Loss: 0.5689 Acc: 0.6825\n",
      "Epoch 4/15 | Train Loss: 0.4882 Acc: 0.7302\n",
      "Epoch 5/15 | Train Loss: 0.4756 Acc: 0.7659\n",
      "Epoch 6/15 | Train Loss: 0.5226 Acc: 0.7262\n",
      "Epoch 7/15 | Train Loss: 0.4384 Acc: 0.7817\n",
      "Epoch 8/15 | Train Loss: 0.3459 Acc: 0.8413\n",
      "Epoch 9/15 | Train Loss: 0.4548 Acc: 0.7738\n",
      "Epoch 10/15 | Train Loss: 0.4100 Acc: 0.8175\n",
      "Epoch 11/15 | Train Loss: 0.3700 Acc: 0.8254\n",
      "Epoch 12/15 | Train Loss: 0.3498 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3934 Acc: 0.8413\n",
      "Epoch 14/15 | Train Loss: 0.3219 Acc: 0.8571\n",
      "Epoch 15/15 | Train Loss: 0.3423 Acc: 0.8532\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7659 Acc: 0.5992\n",
      "Epoch 2/15 | Train Loss: 0.5877 Acc: 0.6984\n",
      "Epoch 3/15 | Train Loss: 0.5279 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.5179 Acc: 0.7222\n",
      "Epoch 5/15 | Train Loss: 0.5679 Acc: 0.7063\n",
      "Epoch 6/15 | Train Loss: 0.4902 Acc: 0.7381\n",
      "Epoch 7/15 | Train Loss: 0.4585 Acc: 0.7976\n",
      "Epoch 8/15 | Train Loss: 0.4750 Acc: 0.7421\n",
      "Epoch 9/15 | Train Loss: 0.4293 Acc: 0.7857\n",
      "Epoch 10/15 | Train Loss: 0.4188 Acc: 0.7897\n",
      "Epoch 11/15 | Train Loss: 0.4027 Acc: 0.8016\n",
      "Epoch 12/15 | Train Loss: 0.3701 Acc: 0.8413\n",
      "Epoch 13/15 | Train Loss: 0.4335 Acc: 0.7937\n",
      "Epoch 14/15 | Train Loss: 0.4130 Acc: 0.8056\n",
      "Epoch 15/15 | Train Loss: 0.3292 Acc: 0.8373\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6559 Acc: 0.6601\n",
      "Epoch 2/15 | Train Loss: 0.6179 Acc: 0.6759\n",
      "Epoch 3/15 | Train Loss: 0.6158 Acc: 0.6561\n",
      "Epoch 4/15 | Train Loss: 0.5004 Acc: 0.7628\n",
      "Epoch 5/15 | Train Loss: 0.4670 Acc: 0.7470\n",
      "Epoch 6/15 | Train Loss: 0.4970 Acc: 0.7747\n",
      "Epoch 7/15 | Train Loss: 0.4928 Acc: 0.7668\n",
      "Epoch 8/15 | Train Loss: 0.4345 Acc: 0.7826\n",
      "Epoch 9/15 | Train Loss: 0.4327 Acc: 0.8261\n",
      "Epoch 10/15 | Train Loss: 0.4101 Acc: 0.8182\n",
      "Epoch 11/15 | Train Loss: 0.4246 Acc: 0.7945\n",
      "Epoch 12/15 | Train Loss: 0.4003 Acc: 0.8103\n",
      "Epoch 13/15 | Train Loss: 0.3999 Acc: 0.8221\n",
      "Epoch 14/15 | Train Loss: 0.3759 Acc: 0.8340\n",
      "Epoch 15/15 | Train Loss: 0.3968 Acc: 0.7984\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6220 Acc: 0.6610\n",
      "Epoch 2/15 | Train Loss: 0.5905 Acc: 0.7627\n",
      "Epoch 3/15 | Train Loss: 0.5107 Acc: 0.7754\n",
      "Epoch 4/15 | Train Loss: 0.5252 Acc: 0.7585\n",
      "Epoch 5/15 | Train Loss: 0.4756 Acc: 0.7754\n",
      "Epoch 6/15 | Train Loss: 0.4842 Acc: 0.7924\n",
      "Epoch 7/15 | Train Loss: 0.4646 Acc: 0.8008\n",
      "Epoch 8/15 | Train Loss: 0.4098 Acc: 0.8347\n",
      "Epoch 9/15 | Train Loss: 0.3882 Acc: 0.8347\n",
      "Epoch 10/15 | Train Loss: 0.3916 Acc: 0.8178\n",
      "Epoch 11/15 | Train Loss: 0.3945 Acc: 0.8390\n",
      "Epoch 12/15 | Train Loss: 0.3627 Acc: 0.8432\n",
      "Epoch 13/15 | Train Loss: 0.4001 Acc: 0.8178\n",
      "Epoch 14/15 | Train Loss: 0.4156 Acc: 0.8136\n",
      "Epoch 15/15 | Train Loss: 0.3735 Acc: 0.8136\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5354 Acc: 0.7401\n",
      "Epoch 2/15 | Train Loss: 0.4558 Acc: 0.8150\n",
      "Epoch 3/15 | Train Loss: 0.3783 Acc: 0.8414\n",
      "Epoch 4/15 | Train Loss: 0.3520 Acc: 0.8458\n",
      "Epoch 5/15 | Train Loss: 0.3537 Acc: 0.9031\n",
      "Epoch 6/15 | Train Loss: 0.3376 Acc: 0.8326\n",
      "Epoch 7/15 | Train Loss: 0.3221 Acc: 0.8678\n",
      "Epoch 8/15 | Train Loss: 0.2350 Acc: 0.9119\n",
      "Epoch 9/15 | Train Loss: 0.3082 Acc: 0.8634\n",
      "Epoch 10/15 | Train Loss: 0.2799 Acc: 0.9163\n",
      "Epoch 11/15 | Train Loss: 0.2343 Acc: 0.8987\n",
      "Epoch 12/15 | Train Loss: 0.3069 Acc: 0.8634\n",
      "Epoch 13/15 | Train Loss: 0.3189 Acc: 0.8722\n",
      "Epoch 14/15 | Train Loss: 0.2480 Acc: 0.9119\n",
      "Epoch 15/15 | Train Loss: 0.2349 Acc: 0.9251\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5404 Acc: 0.7477\n",
      "Epoch 2/15 | Train Loss: 0.4174 Acc: 0.8028\n",
      "Epoch 3/15 | Train Loss: 0.4620 Acc: 0.7890\n",
      "Epoch 4/15 | Train Loss: 0.3818 Acc: 0.8257\n",
      "Epoch 5/15 | Train Loss: 0.3749 Acc: 0.8096\n",
      "Epoch 6/15 | Train Loss: 0.3158 Acc: 0.8440\n",
      "Epoch 7/15 | Train Loss: 0.3640 Acc: 0.8349\n",
      "Epoch 8/15 | Train Loss: 0.3194 Acc: 0.8647\n",
      "Epoch 9/15 | Train Loss: 0.3013 Acc: 0.8647\n",
      "Epoch 10/15 | Train Loss: 0.2777 Acc: 0.8761\n",
      "Epoch 11/15 | Train Loss: 0.2824 Acc: 0.8693\n",
      "Epoch 12/15 | Train Loss: 0.2657 Acc: 0.8876\n",
      "Epoch 13/15 | Train Loss: 0.2789 Acc: 0.8670\n",
      "Epoch 14/15 | Train Loss: 0.2768 Acc: 0.8807\n",
      "Epoch 15/15 | Train Loss: 0.2695 Acc: 0.8761\n",
      "Fold 3 Test Accuracy: 0.6250\n",
      "===== Fold 4 =====\n",
      "Epoch 1: Generator Loss = 10.5444, Discriminator Loss = 8.9029\n",
      "Epoch 2: Generator Loss = 13.3193, Discriminator Loss = 5.1469\n",
      "Epoch 3: Generator Loss = 18.2388, Discriminator Loss = 4.1868\n",
      "Epoch 4: Generator Loss = 29.9670, Discriminator Loss = 4.3363\n",
      "Epoch 5: Generator Loss = 33.1760, Discriminator Loss = 4.1176\n",
      "Epoch 6: Generator Loss = 36.9053, Discriminator Loss = 4.5071\n",
      "Epoch 7: Generator Loss = 33.9565, Discriminator Loss = 6.5691\n",
      "Epoch 8: Generator Loss = 30.4528, Discriminator Loss = 5.8847\n",
      "Epoch 9: Generator Loss = 28.4456, Discriminator Loss = 7.8393\n",
      "Epoch 10: Generator Loss = 25.6580, Discriminator Loss = 7.3826\n",
      "Epoch 11: Generator Loss = 24.7477, Discriminator Loss = 6.3122\n",
      "Epoch 12: Generator Loss = 25.1536, Discriminator Loss = 5.9197\n",
      "Epoch 13: Generator Loss = 19.5263, Discriminator Loss = 7.4983\n",
      "Epoch 14: Generator Loss = 26.4323, Discriminator Loss = 7.5073\n",
      "Epoch 15: Generator Loss = 23.8153, Discriminator Loss = 6.5643\n",
      "Epoch 16: Generator Loss = 19.3827, Discriminator Loss = 8.3392\n",
      "Epoch 17: Generator Loss = 22.8801, Discriminator Loss = 8.4430\n",
      "Epoch 18: Generator Loss = 21.9127, Discriminator Loss = 8.9771\n",
      "Epoch 19: Generator Loss = 27.3832, Discriminator Loss = 7.6881\n",
      "Epoch 20: Generator Loss = 25.6450, Discriminator Loss = 7.1278\n",
      "Epoch 21: Generator Loss = 23.5083, Discriminator Loss = 6.4058\n",
      "Epoch 22: Generator Loss = 22.7921, Discriminator Loss = 6.8970\n",
      "Epoch 23: Generator Loss = 19.1131, Discriminator Loss = 8.5702\n",
      "Epoch 24: Generator Loss = 22.2230, Discriminator Loss = 7.8413\n",
      "Epoch 25: Generator Loss = 16.5681, Discriminator Loss = 8.2554\n",
      "Epoch 26: Generator Loss = 17.7890, Discriminator Loss = 8.2407\n",
      "Epoch 27: Generator Loss = 19.9243, Discriminator Loss = 7.5365\n",
      "Epoch 28: Generator Loss = 20.7453, Discriminator Loss = 7.7592\n",
      "Epoch 29: Generator Loss = 18.8161, Discriminator Loss = 8.2588\n",
      "Epoch 30: Generator Loss = 21.9704, Discriminator Loss = 7.7903\n",
      "Epoch 31: Generator Loss = 18.3377, Discriminator Loss = 10.0484\n",
      "Epoch 32: Generator Loss = 16.1265, Discriminator Loss = 8.3894\n",
      "Epoch 33: Generator Loss = 18.7131, Discriminator Loss = 8.2272\n",
      "Epoch 34: Generator Loss = 19.3390, Discriminator Loss = 8.4786\n",
      "Epoch 35: Generator Loss = 22.9687, Discriminator Loss = 8.2051\n",
      "Epoch 36: Generator Loss = 21.0110, Discriminator Loss = 8.7889\n",
      "Epoch 37: Generator Loss = 14.7435, Discriminator Loss = 8.4124\n",
      "Epoch 38: Generator Loss = 22.1745, Discriminator Loss = 7.0508\n",
      "Epoch 39: Generator Loss = 21.7778, Discriminator Loss = 8.1769\n",
      "Epoch 40: Generator Loss = 23.9790, Discriminator Loss = 7.6024\n",
      "Epoch 41: Generator Loss = 23.2015, Discriminator Loss = 6.4905\n",
      "Epoch 42: Generator Loss = 21.0993, Discriminator Loss = 7.3136\n",
      "Epoch 43: Generator Loss = 15.9555, Discriminator Loss = 8.5734\n",
      "Epoch 44: Generator Loss = 17.3134, Discriminator Loss = 9.3401\n",
      "Epoch 45: Generator Loss = 16.6038, Discriminator Loss = 7.4750\n",
      "Epoch 46: Generator Loss = 19.3941, Discriminator Loss = 7.0171\n",
      "Epoch 47: Generator Loss = 21.7219, Discriminator Loss = 6.0071\n",
      "Epoch 48: Generator Loss = 25.6166, Discriminator Loss = 9.6883\n",
      "Epoch 49: Generator Loss = 17.7049, Discriminator Loss = 7.4208\n",
      "Epoch 50: Generator Loss = 22.1268, Discriminator Loss = 7.4381\n",
      "Epoch 51: Generator Loss = 15.4191, Discriminator Loss = 6.7947\n",
      "Epoch 52: Generator Loss = 26.7527, Discriminator Loss = 6.6702\n",
      "Epoch 53: Generator Loss = 26.1182, Discriminator Loss = 6.9506\n",
      "Epoch 54: Generator Loss = 25.8464, Discriminator Loss = 7.1604\n",
      "Epoch 55: Generator Loss = 22.7597, Discriminator Loss = 7.0986\n",
      "Epoch 56: Generator Loss = 21.5901, Discriminator Loss = 5.8148\n",
      "Epoch 57: Generator Loss = 27.9065, Discriminator Loss = 5.9149\n",
      "Epoch 58: Generator Loss = 29.9570, Discriminator Loss = 5.3300\n",
      "Epoch 59: Generator Loss = 27.6773, Discriminator Loss = 5.2736\n",
      "Epoch 60: Generator Loss = 29.6580, Discriminator Loss = 3.5401\n",
      "Epoch 61: Generator Loss = 36.5137, Discriminator Loss = 4.7356\n",
      "Epoch 62: Generator Loss = 30.9824, Discriminator Loss = 4.2787\n",
      "Epoch 63: Generator Loss = 36.1262, Discriminator Loss = 5.4751\n",
      "Epoch 64: Generator Loss = 40.1438, Discriminator Loss = 2.5282\n",
      "Epoch 65: Generator Loss = 61.9339, Discriminator Loss = 2.4716\n",
      "Epoch 66: Generator Loss = 41.0635, Discriminator Loss = 2.8879\n",
      "Epoch 67: Generator Loss = 42.4820, Discriminator Loss = 3.7284\n",
      "Epoch 68: Generator Loss = 41.0949, Discriminator Loss = 3.8753\n",
      "Epoch 69: Generator Loss = 39.5080, Discriminator Loss = 3.7581\n",
      "Epoch 70: Generator Loss = 36.8045, Discriminator Loss = 2.2290\n",
      "Epoch 71: Generator Loss = 44.8147, Discriminator Loss = 3.3776\n",
      "Epoch 72: Generator Loss = 40.5122, Discriminator Loss = 2.8235\n",
      "Epoch 73: Generator Loss = 48.5899, Discriminator Loss = 3.5047\n",
      "Epoch 74: Generator Loss = 43.7557, Discriminator Loss = 5.1085\n",
      "Epoch 75: Generator Loss = 45.7996, Discriminator Loss = 3.0817\n",
      "Epoch 76: Generator Loss = 38.0520, Discriminator Loss = 2.2258\n",
      "Epoch 77: Generator Loss = 65.6162, Discriminator Loss = 1.4639\n",
      "Epoch 78: Generator Loss = 59.1116, Discriminator Loss = 3.6780\n",
      "Epoch 79: Generator Loss = 51.8365, Discriminator Loss = 3.9434\n",
      "Epoch 80: Generator Loss = 49.4060, Discriminator Loss = 1.8201\n",
      "Epoch 81: Generator Loss = 59.1216, Discriminator Loss = 2.0175\n",
      "Epoch 82: Generator Loss = 49.7800, Discriminator Loss = 1.0200\n",
      "Epoch 83: Generator Loss = 65.3173, Discriminator Loss = 1.8029\n",
      "Epoch 84: Generator Loss = 54.9705, Discriminator Loss = 4.6587\n",
      "Epoch 85: Generator Loss = 61.3895, Discriminator Loss = 1.1475\n",
      "Epoch 86: Generator Loss = 71.8671, Discriminator Loss = 1.2319\n",
      "Epoch 87: Generator Loss = 56.1491, Discriminator Loss = 1.8986\n",
      "Epoch 88: Generator Loss = 54.5128, Discriminator Loss = 0.8609\n",
      "Epoch 89: Generator Loss = 72.9632, Discriminator Loss = 1.4734\n",
      "Epoch 90: Generator Loss = 73.1518, Discriminator Loss = 1.2973\n",
      "Epoch 91: Generator Loss = 64.3449, Discriminator Loss = 1.2466\n",
      "Epoch 92: Generator Loss = 48.5659, Discriminator Loss = 7.8919\n",
      "Epoch 93: Generator Loss = 55.6402, Discriminator Loss = 2.3880\n",
      "Epoch 94: Generator Loss = 61.3485, Discriminator Loss = 1.2343\n",
      "Epoch 95: Generator Loss = 67.5042, Discriminator Loss = 1.1801\n",
      "Epoch 96: Generator Loss = 60.6457, Discriminator Loss = 0.5599\n",
      "Epoch 97: Generator Loss = 76.2551, Discriminator Loss = 0.7585\n",
      "Epoch 98: Generator Loss = 71.4250, Discriminator Loss = 0.6841\n",
      "Epoch 99: Generator Loss = 61.5849, Discriminator Loss = 1.0816\n",
      "Epoch 100: Generator Loss = 66.2243, Discriminator Loss = 0.9835\n",
      "Epoch 101: Generator Loss = 79.3786, Discriminator Loss = 2.1087\n",
      "Epoch 102: Generator Loss = 83.6000, Discriminator Loss = 1.0952\n",
      "Epoch 103: Generator Loss = 61.8163, Discriminator Loss = 1.5532\n",
      "Epoch 104: Generator Loss = 70.9807, Discriminator Loss = 1.0436\n",
      "Epoch 105: Generator Loss = 68.5428, Discriminator Loss = 0.9292\n",
      "Epoch 106: Generator Loss = 72.0638, Discriminator Loss = 0.7007\n",
      "Epoch 107: Generator Loss = 65.1589, Discriminator Loss = 1.0246\n",
      "Epoch 108: Generator Loss = 70.3199, Discriminator Loss = 1.3258\n",
      "Epoch 109: Generator Loss = 71.4932, Discriminator Loss = 0.6888\n",
      "Epoch 110: Generator Loss = 71.5694, Discriminator Loss = 1.3214\n",
      "Epoch 111: Generator Loss = 57.7371, Discriminator Loss = 3.0147\n",
      "Epoch 112: Generator Loss = 65.8895, Discriminator Loss = 1.6077\n",
      "Epoch 113: Generator Loss = 94.2149, Discriminator Loss = 0.9434\n",
      "Epoch 114: Generator Loss = 71.5539, Discriminator Loss = 1.0830\n",
      "Epoch 115: Generator Loss = 67.5289, Discriminator Loss = 0.9279\n",
      "Epoch 116: Generator Loss = 76.5340, Discriminator Loss = 1.7204\n",
      "Epoch 117: Generator Loss = 83.9422, Discriminator Loss = 1.5321\n",
      "Epoch 118: Generator Loss = 73.1996, Discriminator Loss = 0.8630\n",
      "Epoch 119: Generator Loss = 84.5143, Discriminator Loss = 1.6079\n",
      "Epoch 120: Generator Loss = 54.1507, Discriminator Loss = 5.3150\n",
      "Epoch 121: Generator Loss = 60.1515, Discriminator Loss = 1.6182\n",
      "Epoch 122: Generator Loss = 68.7654, Discriminator Loss = 2.2210\n",
      "Epoch 123: Generator Loss = 73.7032, Discriminator Loss = 5.6905\n",
      "Epoch 124: Generator Loss = 78.2482, Discriminator Loss = 0.7897\n",
      "Epoch 125: Generator Loss = 69.7682, Discriminator Loss = 0.8862\n",
      "Epoch 126: Generator Loss = 71.3134, Discriminator Loss = 0.5119\n",
      "Epoch 127: Generator Loss = 82.5695, Discriminator Loss = 0.6478\n",
      "Epoch 128: Generator Loss = 80.8561, Discriminator Loss = 0.7791\n",
      "Epoch 129: Generator Loss = 82.5853, Discriminator Loss = 0.5575\n",
      "Epoch 130: Generator Loss = 75.7734, Discriminator Loss = 0.7245\n",
      "Epoch 131: Generator Loss = 73.3373, Discriminator Loss = 1.2716\n",
      "Epoch 132: Generator Loss = 63.2041, Discriminator Loss = 1.4651\n",
      "Epoch 133: Generator Loss = 85.8014, Discriminator Loss = 0.8539\n",
      "Epoch 134: Generator Loss = 60.2605, Discriminator Loss = 1.1869\n",
      "Epoch 135: Generator Loss = 71.4324, Discriminator Loss = 0.6142\n",
      "Epoch 136: Generator Loss = 72.5062, Discriminator Loss = 0.9015\n",
      "Epoch 137: Generator Loss = 80.2472, Discriminator Loss = 1.3036\n",
      "Epoch 138: Generator Loss = 63.6203, Discriminator Loss = 9.8815\n",
      "Epoch 139: Generator Loss = 42.9397, Discriminator Loss = 10.2599\n",
      "Epoch 140: Generator Loss = 31.8645, Discriminator Loss = 6.2796\n",
      "Epoch 141: Generator Loss = 38.9781, Discriminator Loss = 2.7504\n",
      "Epoch 142: Generator Loss = 60.0487, Discriminator Loss = 1.9377\n",
      "Epoch 143: Generator Loss = 59.9658, Discriminator Loss = 2.3307\n",
      "Epoch 144: Generator Loss = 70.2287, Discriminator Loss = 6.3459\n",
      "Epoch 145: Generator Loss = 58.4015, Discriminator Loss = 2.1348\n",
      "Epoch 146: Generator Loss = 53.3141, Discriminator Loss = 2.1398\n",
      "Epoch 147: Generator Loss = 57.8350, Discriminator Loss = 2.0097\n",
      "Epoch 148: Generator Loss = 64.7417, Discriminator Loss = 2.2861\n",
      "Epoch 149: Generator Loss = 62.3372, Discriminator Loss = 1.7333\n",
      "Epoch 150: Generator Loss = 59.9580, Discriminator Loss = 1.2216\n",
      "Epoch 151: Generator Loss = 65.4342, Discriminator Loss = 0.5866\n",
      "Epoch 152: Generator Loss = 71.1968, Discriminator Loss = 0.7534\n",
      "Epoch 153: Generator Loss = 66.5869, Discriminator Loss = 2.1233\n",
      "Epoch 154: Generator Loss = 66.4933, Discriminator Loss = 2.1457\n",
      "Epoch 155: Generator Loss = 74.6566, Discriminator Loss = 0.8224\n",
      "Epoch 156: Generator Loss = 64.6860, Discriminator Loss = 0.8400\n",
      "Epoch 157: Generator Loss = 67.1281, Discriminator Loss = 0.9140\n",
      "Epoch 158: Generator Loss = 66.3380, Discriminator Loss = 1.1275\n",
      "Epoch 159: Generator Loss = 52.2304, Discriminator Loss = 1.3922\n",
      "Epoch 160: Generator Loss = 57.9184, Discriminator Loss = 1.6633\n",
      "Epoch 161: Generator Loss = 61.2336, Discriminator Loss = 1.3031\n",
      "Epoch 162: Generator Loss = 71.8343, Discriminator Loss = 0.7038\n",
      "Epoch 163: Generator Loss = 79.9703, Discriminator Loss = 0.7295\n",
      "Epoch 164: Generator Loss = 60.5740, Discriminator Loss = 1.3617\n",
      "Epoch 165: Generator Loss = 85.4053, Discriminator Loss = 1.2876\n",
      "Epoch 166: Generator Loss = 74.9815, Discriminator Loss = 6.6835\n",
      "Epoch 167: Generator Loss = 56.4148, Discriminator Loss = 4.0337\n",
      "Epoch 168: Generator Loss = 63.9900, Discriminator Loss = 5.2260\n",
      "Epoch 169: Generator Loss = 66.8151, Discriminator Loss = 4.1265\n",
      "Epoch 170: Generator Loss = 73.2056, Discriminator Loss = 1.8501\n",
      "Epoch 171: Generator Loss = 71.7664, Discriminator Loss = 0.7425\n",
      "Epoch 172: Generator Loss = 67.5610, Discriminator Loss = 1.2897\n",
      "Epoch 173: Generator Loss = 73.4039, Discriminator Loss = 0.9690\n",
      "Epoch 174: Generator Loss = 54.1663, Discriminator Loss = 2.2970\n",
      "Epoch 175: Generator Loss = 64.3890, Discriminator Loss = 2.0473\n",
      "Epoch 176: Generator Loss = 76.7582, Discriminator Loss = 1.6671\n",
      "Epoch 177: Generator Loss = 59.4091, Discriminator Loss = 6.2873\n",
      "Epoch 178: Generator Loss = 50.0371, Discriminator Loss = 2.5764\n",
      "Epoch 179: Generator Loss = 56.3922, Discriminator Loss = 1.7011\n",
      "Epoch 180: Generator Loss = 74.7435, Discriminator Loss = 3.9301\n",
      "Epoch 181: Generator Loss = 71.3434, Discriminator Loss = 0.9363\n",
      "Epoch 182: Generator Loss = 64.0851, Discriminator Loss = 0.7984\n",
      "Epoch 183: Generator Loss = 75.4155, Discriminator Loss = 1.0534\n",
      "Epoch 184: Generator Loss = 61.1451, Discriminator Loss = 1.1297\n",
      "Epoch 185: Generator Loss = 77.7794, Discriminator Loss = 0.6193\n",
      "Epoch 186: Generator Loss = 70.1639, Discriminator Loss = 0.8557\n",
      "Epoch 187: Generator Loss = 77.8773, Discriminator Loss = 0.5256\n",
      "Epoch 188: Generator Loss = 82.1610, Discriminator Loss = 0.7421\n",
      "Epoch 189: Generator Loss = 73.5315, Discriminator Loss = 1.3546\n",
      "Epoch 190: Generator Loss = 60.3411, Discriminator Loss = 12.0231\n",
      "Epoch 191: Generator Loss = 42.2724, Discriminator Loss = 10.3071\n",
      "Epoch 192: Generator Loss = 35.2831, Discriminator Loss = 3.6812\n",
      "Epoch 193: Generator Loss = 47.1784, Discriminator Loss = 3.3236\n",
      "Epoch 194: Generator Loss = 57.5491, Discriminator Loss = 4.4879\n",
      "Epoch 195: Generator Loss = 46.1070, Discriminator Loss = 2.4923\n",
      "Epoch 196: Generator Loss = 52.5614, Discriminator Loss = 2.9875\n",
      "Epoch 197: Generator Loss = 51.8422, Discriminator Loss = 1.9010\n",
      "Epoch 198: Generator Loss = 62.0344, Discriminator Loss = 2.1549\n",
      "Epoch 199: Generator Loss = 60.0496, Discriminator Loss = 3.6099\n",
      "Epoch 200: Generator Loss = 44.1151, Discriminator Loss = 2.4505\n",
      "Epoch 201: Generator Loss = 72.1757, Discriminator Loss = 1.2641\n",
      "Epoch 202: Generator Loss = 53.1136, Discriminator Loss = 1.9968\n",
      "Epoch 203: Generator Loss = 49.2232, Discriminator Loss = 6.4345\n",
      "Epoch 204: Generator Loss = 44.8453, Discriminator Loss = 3.3650\n",
      "Epoch 205: Generator Loss = 58.4298, Discriminator Loss = 5.7296\n",
      "Epoch 206: Generator Loss = 45.8089, Discriminator Loss = 4.2580\n",
      "Epoch 207: Generator Loss = 47.4518, Discriminator Loss = 1.7801\n",
      "Epoch 208: Generator Loss = 78.0818, Discriminator Loss = 4.3246\n",
      "Epoch 209: Generator Loss = 54.3188, Discriminator Loss = 5.7486\n",
      "Epoch 210: Generator Loss = 58.2474, Discriminator Loss = 1.7065\n",
      "Epoch 211: Generator Loss = 55.5515, Discriminator Loss = 1.6559\n",
      "Epoch 212: Generator Loss = 54.3579, Discriminator Loss = 0.8985\n",
      "Epoch 213: Generator Loss = 45.4503, Discriminator Loss = 1.9303\n",
      "Epoch 214: Generator Loss = 62.6244, Discriminator Loss = 8.3285\n",
      "Epoch 215: Generator Loss = 43.5104, Discriminator Loss = 3.4848\n",
      "Epoch 216: Generator Loss = 48.9493, Discriminator Loss = 2.3342\n",
      "Epoch 217: Generator Loss = 53.5569, Discriminator Loss = 1.9143\n",
      "Epoch 218: Generator Loss = 62.7019, Discriminator Loss = 7.2127\n",
      "Epoch 219: Generator Loss = 53.5378, Discriminator Loss = 2.4190\n",
      "Epoch 220: Generator Loss = 69.7044, Discriminator Loss = 1.0112\n",
      "Epoch 221: Generator Loss = 70.0879, Discriminator Loss = 0.8639\n",
      "Epoch 222: Generator Loss = 80.6569, Discriminator Loss = 1.1107\n",
      "Epoch 223: Generator Loss = 59.7539, Discriminator Loss = 0.8985\n",
      "Epoch 224: Generator Loss = 56.2254, Discriminator Loss = 1.1734\n",
      "Epoch 225: Generator Loss = 79.1138, Discriminator Loss = 2.4318\n",
      "Epoch 226: Generator Loss = 65.8928, Discriminator Loss = 1.3748\n",
      "Epoch 227: Generator Loss = 72.5738, Discriminator Loss = 5.2115\n",
      "Epoch 228: Generator Loss = 61.0131, Discriminator Loss = 1.6298\n",
      "Epoch 229: Generator Loss = 54.8664, Discriminator Loss = 1.0785\n",
      "Epoch 230: Generator Loss = 55.0568, Discriminator Loss = 0.5398\n",
      "Epoch 231: Generator Loss = 75.6373, Discriminator Loss = 4.5884\n",
      "Epoch 232: Generator Loss = 75.3980, Discriminator Loss = 2.0310\n",
      "Epoch 233: Generator Loss = 63.4108, Discriminator Loss = 1.0209\n",
      "Epoch 234: Generator Loss = 66.6858, Discriminator Loss = 1.8765\n",
      "Epoch 235: Generator Loss = 63.3526, Discriminator Loss = 1.5448\n",
      "Epoch 236: Generator Loss = 73.9068, Discriminator Loss = 0.8304\n",
      "Epoch 237: Generator Loss = 71.6723, Discriminator Loss = 1.7933\n",
      "Epoch 238: Generator Loss = 98.9947, Discriminator Loss = 4.2849\n",
      "Epoch 239: Generator Loss = 54.3913, Discriminator Loss = 2.9102\n",
      "Epoch 240: Generator Loss = 60.9842, Discriminator Loss = 1.1446\n",
      "Epoch 241: Generator Loss = 67.2065, Discriminator Loss = 2.2282\n",
      "Epoch 242: Generator Loss = 57.5238, Discriminator Loss = 1.2107\n",
      "Epoch 243: Generator Loss = 70.6448, Discriminator Loss = 0.6840\n",
      "Epoch 244: Generator Loss = 79.2151, Discriminator Loss = 0.9029\n",
      "Epoch 245: Generator Loss = 49.3587, Discriminator Loss = 1.2354\n",
      "Epoch 246: Generator Loss = 72.8651, Discriminator Loss = 0.6383\n",
      "Epoch 247: Generator Loss = 95.0790, Discriminator Loss = 3.1365\n",
      "Epoch 248: Generator Loss = 48.2838, Discriminator Loss = 12.2650\n",
      "Epoch 249: Generator Loss = 60.6846, Discriminator Loss = 1.7321\n",
      "Epoch 250: Generator Loss = 53.0210, Discriminator Loss = 1.6297\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.4881 Acc: 0.7565\n",
      "Epoch 2/15 | Train Loss: 0.3886 Acc: 0.8135\n",
      "Epoch 3/15 | Train Loss: 0.3896 Acc: 0.7953\n",
      "Epoch 4/15 | Train Loss: 0.3696 Acc: 0.7876\n",
      "Epoch 5/15 | Train Loss: 0.3352 Acc: 0.8135\n",
      "Epoch 6/15 | Train Loss: 0.3726 Acc: 0.7979\n",
      "Epoch 7/15 | Train Loss: 0.3415 Acc: 0.8472\n",
      "Epoch 8/15 | Train Loss: 0.2919 Acc: 0.8472\n",
      "Epoch 9/15 | Train Loss: 0.2913 Acc: 0.8627\n",
      "Epoch 10/15 | Train Loss: 0.2780 Acc: 0.8653\n",
      "Epoch 11/15 | Train Loss: 0.2667 Acc: 0.8782\n",
      "Epoch 12/15 | Train Loss: 0.2804 Acc: 0.8705\n",
      "Epoch 13/15 | Train Loss: 0.2944 Acc: 0.8497\n",
      "Epoch 14/15 | Train Loss: 0.2775 Acc: 0.8756\n",
      "Epoch 15/15 | Train Loss: 0.2711 Acc: 0.8653\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.5948 Acc: 0.7513\n",
      "Epoch 2/15 | Train Loss: 0.4695 Acc: 0.7746\n",
      "Epoch 3/15 | Train Loss: 0.4191 Acc: 0.7850\n",
      "Epoch 4/15 | Train Loss: 0.3571 Acc: 0.8135\n",
      "Epoch 5/15 | Train Loss: 0.3817 Acc: 0.7953\n",
      "Epoch 6/15 | Train Loss: 0.3417 Acc: 0.8031\n",
      "Epoch 7/15 | Train Loss: 0.3748 Acc: 0.8161\n",
      "Epoch 8/15 | Train Loss: 0.3147 Acc: 0.8290\n",
      "Epoch 9/15 | Train Loss: 0.3257 Acc: 0.8264\n",
      "Epoch 10/15 | Train Loss: 0.3118 Acc: 0.8394\n",
      "Epoch 11/15 | Train Loss: 0.2945 Acc: 0.8679\n",
      "Epoch 12/15 | Train Loss: 0.2967 Acc: 0.8497\n",
      "Epoch 13/15 | Train Loss: 0.2887 Acc: 0.8705\n",
      "Epoch 14/15 | Train Loss: 0.3104 Acc: 0.8497\n",
      "Epoch 15/15 | Train Loss: 0.2790 Acc: 0.8731\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5318 Acc: 0.7513\n",
      "Epoch 2/15 | Train Loss: 0.3968 Acc: 0.7824\n",
      "Epoch 3/15 | Train Loss: 0.3637 Acc: 0.8005\n",
      "Epoch 4/15 | Train Loss: 0.3562 Acc: 0.7979\n",
      "Epoch 5/15 | Train Loss: 0.3433 Acc: 0.8109\n",
      "Epoch 6/15 | Train Loss: 0.3418 Acc: 0.8212\n",
      "Epoch 7/15 | Train Loss: 0.3442 Acc: 0.8187\n",
      "Epoch 8/15 | Train Loss: 0.3582 Acc: 0.8212\n",
      "Epoch 9/15 | Train Loss: 0.3170 Acc: 0.8368\n",
      "Epoch 10/15 | Train Loss: 0.3175 Acc: 0.8575\n",
      "Epoch 11/15 | Train Loss: 0.2919 Acc: 0.8446\n",
      "Epoch 12/15 | Train Loss: 0.2971 Acc: 0.8472\n",
      "Epoch 13/15 | Train Loss: 0.2882 Acc: 0.8549\n",
      "Epoch 14/15 | Train Loss: 0.3086 Acc: 0.8497\n",
      "Epoch 15/15 | Train Loss: 0.2785 Acc: 0.8653\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4982 Acc: 0.7810\n",
      "Epoch 2/15 | Train Loss: 0.4208 Acc: 0.8063\n",
      "Epoch 3/15 | Train Loss: 0.3828 Acc: 0.8476\n",
      "Epoch 4/15 | Train Loss: 0.3853 Acc: 0.8159\n",
      "Epoch 5/15 | Train Loss: 0.3689 Acc: 0.8381\n",
      "Epoch 6/15 | Train Loss: 0.3490 Acc: 0.8444\n",
      "Epoch 7/15 | Train Loss: 0.3447 Acc: 0.8603\n",
      "Epoch 8/15 | Train Loss: 0.2837 Acc: 0.8825\n",
      "Epoch 9/15 | Train Loss: 0.3344 Acc: 0.8571\n",
      "Epoch 10/15 | Train Loss: 0.3249 Acc: 0.8444\n",
      "Epoch 11/15 | Train Loss: 0.3115 Acc: 0.8825\n",
      "Epoch 12/15 | Train Loss: 0.3334 Acc: 0.8571\n",
      "Epoch 13/15 | Train Loss: 0.2746 Acc: 0.8825\n",
      "Epoch 14/15 | Train Loss: 0.2831 Acc: 0.8921\n",
      "Epoch 15/15 | Train Loss: 0.2883 Acc: 0.8762\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4514 Acc: 0.8163\n",
      "Epoch 2/15 | Train Loss: 0.4008 Acc: 0.8516\n",
      "Epoch 3/15 | Train Loss: 0.3575 Acc: 0.8375\n",
      "Epoch 4/15 | Train Loss: 0.3069 Acc: 0.8693\n",
      "Epoch 5/15 | Train Loss: 0.3000 Acc: 0.8834\n",
      "Epoch 6/15 | Train Loss: 0.3224 Acc: 0.8834\n",
      "Epoch 7/15 | Train Loss: 0.3470 Acc: 0.8587\n",
      "Epoch 8/15 | Train Loss: 0.2521 Acc: 0.9011\n",
      "Epoch 9/15 | Train Loss: 0.2988 Acc: 0.8905\n",
      "Epoch 10/15 | Train Loss: 0.2568 Acc: 0.8940\n",
      "Epoch 11/15 | Train Loss: 0.2769 Acc: 0.8905\n",
      "Epoch 12/15 | Train Loss: 0.2360 Acc: 0.9011\n",
      "Epoch 13/15 | Train Loss: 0.2357 Acc: 0.9152\n",
      "Epoch 14/15 | Train Loss: 0.2552 Acc: 0.8869\n",
      "Epoch 15/15 | Train Loss: 0.2328 Acc: 0.9046\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7529 Acc: 0.6220\n",
      "Epoch 2/15 | Train Loss: 0.4711 Acc: 0.7874\n",
      "Epoch 3/15 | Train Loss: 0.6065 Acc: 0.6417\n",
      "Epoch 4/15 | Train Loss: 0.5686 Acc: 0.6890\n",
      "Epoch 5/15 | Train Loss: 0.4842 Acc: 0.7402\n",
      "Epoch 6/15 | Train Loss: 0.5076 Acc: 0.7795\n",
      "Epoch 7/15 | Train Loss: 0.5500 Acc: 0.7165\n",
      "Epoch 8/15 | Train Loss: 0.4481 Acc: 0.7559\n",
      "Epoch 9/15 | Train Loss: 0.4249 Acc: 0.7992\n",
      "Epoch 10/15 | Train Loss: 0.4176 Acc: 0.8425\n",
      "Epoch 11/15 | Train Loss: 0.4139 Acc: 0.7953\n",
      "Epoch 12/15 | Train Loss: 0.3957 Acc: 0.8228\n",
      "Epoch 13/15 | Train Loss: 0.3757 Acc: 0.8189\n",
      "Epoch 14/15 | Train Loss: 0.3897 Acc: 0.8425\n",
      "Epoch 15/15 | Train Loss: 0.4166 Acc: 0.8110\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6619 Acc: 0.6260\n",
      "Epoch 2/15 | Train Loss: 0.6592 Acc: 0.6890\n",
      "Epoch 3/15 | Train Loss: 0.5703 Acc: 0.6929\n",
      "Epoch 4/15 | Train Loss: 0.5242 Acc: 0.7323\n",
      "Epoch 5/15 | Train Loss: 0.5886 Acc: 0.6575\n",
      "Epoch 6/15 | Train Loss: 0.4869 Acc: 0.7677\n",
      "Epoch 7/15 | Train Loss: 0.4754 Acc: 0.7441\n",
      "Epoch 8/15 | Train Loss: 0.4634 Acc: 0.7480\n",
      "Epoch 9/15 | Train Loss: 0.4228 Acc: 0.7992\n",
      "Epoch 10/15 | Train Loss: 0.4887 Acc: 0.7835\n",
      "Epoch 11/15 | Train Loss: 0.4557 Acc: 0.7677\n",
      "Epoch 12/15 | Train Loss: 0.4210 Acc: 0.7717\n",
      "Epoch 13/15 | Train Loss: 0.4254 Acc: 0.7795\n",
      "Epoch 14/15 | Train Loss: 0.4466 Acc: 0.7756\n",
      "Epoch 15/15 | Train Loss: 0.3998 Acc: 0.8268\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7054 Acc: 0.6063\n",
      "Epoch 2/15 | Train Loss: 0.5830 Acc: 0.6732\n",
      "Epoch 3/15 | Train Loss: 0.5399 Acc: 0.7126\n",
      "Epoch 4/15 | Train Loss: 0.5760 Acc: 0.7008\n",
      "Epoch 5/15 | Train Loss: 0.5405 Acc: 0.7244\n",
      "Epoch 6/15 | Train Loss: 0.5561 Acc: 0.6772\n",
      "Epoch 7/15 | Train Loss: 0.4809 Acc: 0.7559\n",
      "Epoch 8/15 | Train Loss: 0.4621 Acc: 0.7638\n",
      "Epoch 9/15 | Train Loss: 0.4626 Acc: 0.7559\n",
      "Epoch 10/15 | Train Loss: 0.4471 Acc: 0.7756\n",
      "Epoch 11/15 | Train Loss: 0.4352 Acc: 0.8031\n",
      "Epoch 12/15 | Train Loss: 0.4292 Acc: 0.7795\n",
      "Epoch 13/15 | Train Loss: 0.4337 Acc: 0.7717\n",
      "Epoch 14/15 | Train Loss: 0.4774 Acc: 0.7598\n",
      "Epoch 15/15 | Train Loss: 0.3848 Acc: 0.8346\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6449 Acc: 0.6667\n",
      "Epoch 2/15 | Train Loss: 0.5430 Acc: 0.7679\n",
      "Epoch 3/15 | Train Loss: 0.4769 Acc: 0.8186\n",
      "Epoch 4/15 | Train Loss: 0.4934 Acc: 0.7764\n",
      "Epoch 5/15 | Train Loss: 0.4291 Acc: 0.7975\n",
      "Epoch 6/15 | Train Loss: 0.4857 Acc: 0.7890\n",
      "Epoch 7/15 | Train Loss: 0.4796 Acc: 0.7848\n",
      "Epoch 8/15 | Train Loss: 0.4067 Acc: 0.8439\n",
      "Epoch 9/15 | Train Loss: 0.4379 Acc: 0.8059\n",
      "Epoch 10/15 | Train Loss: 0.3772 Acc: 0.8734\n",
      "Epoch 11/15 | Train Loss: 0.4218 Acc: 0.8101\n",
      "Epoch 12/15 | Train Loss: 0.4435 Acc: 0.8186\n",
      "Epoch 13/15 | Train Loss: 0.3894 Acc: 0.8481\n",
      "Epoch 14/15 | Train Loss: 0.3828 Acc: 0.8354\n",
      "Epoch 15/15 | Train Loss: 0.3633 Acc: 0.8481\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6592 Acc: 0.6812\n",
      "Epoch 2/15 | Train Loss: 0.4809 Acc: 0.8079\n",
      "Epoch 3/15 | Train Loss: 0.3742 Acc: 0.8515\n",
      "Epoch 4/15 | Train Loss: 0.4139 Acc: 0.8428\n",
      "Epoch 5/15 | Train Loss: 0.3375 Acc: 0.8210\n",
      "Epoch 6/15 | Train Loss: 0.4364 Acc: 0.8384\n",
      "Epoch 7/15 | Train Loss: 0.3308 Acc: 0.8777\n",
      "Epoch 8/15 | Train Loss: 0.3213 Acc: 0.8865\n",
      "Epoch 9/15 | Train Loss: 0.3589 Acc: 0.8690\n",
      "Epoch 10/15 | Train Loss: 0.2777 Acc: 0.8908\n",
      "Epoch 11/15 | Train Loss: 0.3227 Acc: 0.8777\n",
      "Epoch 12/15 | Train Loss: 0.2784 Acc: 0.8865\n",
      "Epoch 13/15 | Train Loss: 0.2726 Acc: 0.8996\n",
      "Epoch 14/15 | Train Loss: 0.2830 Acc: 0.8865\n",
      "Epoch 15/15 | Train Loss: 0.2718 Acc: 0.8908\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6885 Acc: 0.6611\n",
      "Epoch 2/15 | Train Loss: 0.5409 Acc: 0.7349\n",
      "Epoch 3/15 | Train Loss: 0.4710 Acc: 0.7517\n",
      "Epoch 4/15 | Train Loss: 0.5128 Acc: 0.7517\n",
      "Epoch 5/15 | Train Loss: 0.4608 Acc: 0.7383\n",
      "Epoch 6/15 | Train Loss: 0.4418 Acc: 0.7685\n",
      "Epoch 7/15 | Train Loss: 0.4592 Acc: 0.7819\n",
      "Epoch 8/15 | Train Loss: 0.3771 Acc: 0.8154\n",
      "Epoch 9/15 | Train Loss: 0.3315 Acc: 0.8523\n",
      "Epoch 10/15 | Train Loss: 0.3413 Acc: 0.8490\n",
      "Epoch 11/15 | Train Loss: 0.3012 Acc: 0.8893\n",
      "Epoch 12/15 | Train Loss: 0.3451 Acc: 0.8456\n",
      "Epoch 13/15 | Train Loss: 0.3817 Acc: 0.8154\n",
      "Epoch 14/15 | Train Loss: 0.3111 Acc: 0.8658\n",
      "Epoch 15/15 | Train Loss: 0.2695 Acc: 0.8960\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6048 Acc: 0.6879\n",
      "Epoch 2/15 | Train Loss: 0.4499 Acc: 0.7349\n",
      "Epoch 3/15 | Train Loss: 0.5370 Acc: 0.7013\n",
      "Epoch 4/15 | Train Loss: 0.4827 Acc: 0.7383\n",
      "Epoch 5/15 | Train Loss: 0.4423 Acc: 0.7651\n",
      "Epoch 6/15 | Train Loss: 0.4395 Acc: 0.7987\n",
      "Epoch 7/15 | Train Loss: 0.4272 Acc: 0.7785\n",
      "Epoch 8/15 | Train Loss: 0.4286 Acc: 0.7953\n",
      "Epoch 9/15 | Train Loss: 0.4060 Acc: 0.7953\n",
      "Epoch 10/15 | Train Loss: 0.3833 Acc: 0.8188\n",
      "Epoch 11/15 | Train Loss: 0.3782 Acc: 0.8020\n",
      "Epoch 12/15 | Train Loss: 0.3714 Acc: 0.8154\n",
      "Epoch 13/15 | Train Loss: 0.3921 Acc: 0.7953\n",
      "Epoch 14/15 | Train Loss: 0.3596 Acc: 0.8456\n",
      "Epoch 15/15 | Train Loss: 0.3641 Acc: 0.8322\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6200 Acc: 0.7047\n",
      "Epoch 2/15 | Train Loss: 0.5137 Acc: 0.7181\n",
      "Epoch 3/15 | Train Loss: 0.4968 Acc: 0.7416\n",
      "Epoch 4/15 | Train Loss: 0.4642 Acc: 0.7517\n",
      "Epoch 5/15 | Train Loss: 0.4276 Acc: 0.7617\n",
      "Epoch 6/15 | Train Loss: 0.4259 Acc: 0.7852\n",
      "Epoch 7/15 | Train Loss: 0.3955 Acc: 0.7919\n",
      "Epoch 8/15 | Train Loss: 0.4395 Acc: 0.7718\n",
      "Epoch 9/15 | Train Loss: 0.3701 Acc: 0.8221\n",
      "Epoch 10/15 | Train Loss: 0.3747 Acc: 0.8020\n",
      "Epoch 11/15 | Train Loss: 0.3620 Acc: 0.8255\n",
      "Epoch 12/15 | Train Loss: 0.3607 Acc: 0.8289\n",
      "Epoch 13/15 | Train Loss: 0.4391 Acc: 0.8054\n",
      "Epoch 14/15 | Train Loss: 0.3551 Acc: 0.8423\n",
      "Epoch 15/15 | Train Loss: 0.3713 Acc: 0.8289\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5651 Acc: 0.7452\n",
      "Epoch 2/15 | Train Loss: 0.5363 Acc: 0.7414\n",
      "Epoch 3/15 | Train Loss: 0.4863 Acc: 0.8061\n",
      "Epoch 4/15 | Train Loss: 0.4414 Acc: 0.7909\n",
      "Epoch 5/15 | Train Loss: 0.4091 Acc: 0.8023\n",
      "Epoch 6/15 | Train Loss: 0.4401 Acc: 0.8137\n",
      "Epoch 7/15 | Train Loss: 0.4294 Acc: 0.8137\n",
      "Epoch 8/15 | Train Loss: 0.4207 Acc: 0.8137\n",
      "Epoch 9/15 | Train Loss: 0.3581 Acc: 0.8441\n",
      "Epoch 10/15 | Train Loss: 0.3578 Acc: 0.8403\n",
      "Epoch 11/15 | Train Loss: 0.4124 Acc: 0.8327\n",
      "Epoch 12/15 | Train Loss: 0.3501 Acc: 0.8441\n",
      "Epoch 13/15 | Train Loss: 0.3667 Acc: 0.8441\n",
      "Epoch 14/15 | Train Loss: 0.3953 Acc: 0.8289\n",
      "Epoch 15/15 | Train Loss: 0.3493 Acc: 0.8441\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5669 Acc: 0.7733\n",
      "Epoch 2/15 | Train Loss: 0.4756 Acc: 0.8138\n",
      "Epoch 3/15 | Train Loss: 0.3842 Acc: 0.8381\n",
      "Epoch 4/15 | Train Loss: 0.3763 Acc: 0.8219\n",
      "Epoch 5/15 | Train Loss: 0.4177 Acc: 0.8340\n",
      "Epoch 6/15 | Train Loss: 0.3499 Acc: 0.8745\n",
      "Epoch 7/15 | Train Loss: 0.3433 Acc: 0.8502\n",
      "Epoch 8/15 | Train Loss: 0.3189 Acc: 0.8583\n",
      "Epoch 9/15 | Train Loss: 0.3415 Acc: 0.8704\n",
      "Epoch 10/15 | Train Loss: 0.3317 Acc: 0.8421\n",
      "Epoch 11/15 | Train Loss: 0.3059 Acc: 0.8785\n",
      "Epoch 12/15 | Train Loss: 0.2998 Acc: 0.8826\n",
      "Epoch 13/15 | Train Loss: 0.3233 Acc: 0.8543\n",
      "Epoch 14/15 | Train Loss: 0.2833 Acc: 0.8907\n",
      "Epoch 15/15 | Train Loss: 0.2961 Acc: 0.8623\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7264 Acc: 0.6181\n",
      "Epoch 2/15 | Train Loss: 0.6384 Acc: 0.6654\n",
      "Epoch 3/15 | Train Loss: 0.5535 Acc: 0.7283\n",
      "Epoch 4/15 | Train Loss: 0.5320 Acc: 0.7244\n",
      "Epoch 5/15 | Train Loss: 0.5159 Acc: 0.7441\n",
      "Epoch 6/15 | Train Loss: 0.5076 Acc: 0.7323\n",
      "Epoch 7/15 | Train Loss: 0.4768 Acc: 0.7677\n",
      "Epoch 8/15 | Train Loss: 0.4393 Acc: 0.7638\n",
      "Epoch 9/15 | Train Loss: 0.3770 Acc: 0.8228\n",
      "Epoch 10/15 | Train Loss: 0.4125 Acc: 0.8110\n",
      "Epoch 11/15 | Train Loss: 0.3832 Acc: 0.8268\n",
      "Epoch 12/15 | Train Loss: 0.3987 Acc: 0.7835\n",
      "Epoch 13/15 | Train Loss: 0.3718 Acc: 0.8150\n",
      "Epoch 14/15 | Train Loss: 0.3678 Acc: 0.8268\n",
      "Epoch 15/15 | Train Loss: 0.3704 Acc: 0.8346\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7137 Acc: 0.5984\n",
      "Epoch 2/15 | Train Loss: 0.6360 Acc: 0.6417\n",
      "Epoch 3/15 | Train Loss: 0.5473 Acc: 0.7047\n",
      "Epoch 4/15 | Train Loss: 0.5172 Acc: 0.7008\n",
      "Epoch 5/15 | Train Loss: 0.4987 Acc: 0.7441\n",
      "Epoch 6/15 | Train Loss: 0.5596 Acc: 0.6929\n",
      "Epoch 7/15 | Train Loss: 0.4672 Acc: 0.7598\n",
      "Epoch 8/15 | Train Loss: 0.4380 Acc: 0.7992\n",
      "Epoch 9/15 | Train Loss: 0.4535 Acc: 0.7598\n",
      "Epoch 10/15 | Train Loss: 0.3980 Acc: 0.8110\n",
      "Epoch 11/15 | Train Loss: 0.3693 Acc: 0.8465\n",
      "Epoch 12/15 | Train Loss: 0.4221 Acc: 0.7874\n",
      "Epoch 13/15 | Train Loss: 0.4357 Acc: 0.7795\n",
      "Epoch 14/15 | Train Loss: 0.4282 Acc: 0.7835\n",
      "Epoch 15/15 | Train Loss: 0.4646 Acc: 0.7480\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7346 Acc: 0.6299\n",
      "Epoch 2/15 | Train Loss: 0.5791 Acc: 0.6969\n",
      "Epoch 3/15 | Train Loss: 0.6205 Acc: 0.6614\n",
      "Epoch 4/15 | Train Loss: 0.5472 Acc: 0.7480\n",
      "Epoch 5/15 | Train Loss: 0.5745 Acc: 0.7126\n",
      "Epoch 6/15 | Train Loss: 0.4810 Acc: 0.7126\n",
      "Epoch 7/15 | Train Loss: 0.5144 Acc: 0.7323\n",
      "Epoch 8/15 | Train Loss: 0.5168 Acc: 0.7520\n",
      "Epoch 9/15 | Train Loss: 0.4240 Acc: 0.7795\n",
      "Epoch 10/15 | Train Loss: 0.4464 Acc: 0.7795\n",
      "Epoch 11/15 | Train Loss: 0.4226 Acc: 0.8031\n",
      "Epoch 12/15 | Train Loss: 0.3922 Acc: 0.8031\n",
      "Epoch 13/15 | Train Loss: 0.4282 Acc: 0.7717\n",
      "Epoch 14/15 | Train Loss: 0.4248 Acc: 0.7992\n",
      "Epoch 15/15 | Train Loss: 0.3809 Acc: 0.8110\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6546 Acc: 0.6582\n",
      "Epoch 2/15 | Train Loss: 0.5767 Acc: 0.7553\n",
      "Epoch 3/15 | Train Loss: 0.4867 Acc: 0.8017\n",
      "Epoch 4/15 | Train Loss: 0.4805 Acc: 0.8101\n",
      "Epoch 5/15 | Train Loss: 0.4924 Acc: 0.7932\n",
      "Epoch 6/15 | Train Loss: 0.4496 Acc: 0.7932\n",
      "Epoch 7/15 | Train Loss: 0.4288 Acc: 0.8270\n",
      "Epoch 8/15 | Train Loss: 0.4724 Acc: 0.7932\n",
      "Epoch 9/15 | Train Loss: 0.4457 Acc: 0.8059\n",
      "Epoch 10/15 | Train Loss: 0.4288 Acc: 0.8017\n",
      "Epoch 11/15 | Train Loss: 0.3985 Acc: 0.8228\n",
      "Epoch 12/15 | Train Loss: 0.4034 Acc: 0.8397\n",
      "Epoch 13/15 | Train Loss: 0.3936 Acc: 0.8312\n",
      "Epoch 14/15 | Train Loss: 0.3797 Acc: 0.8481\n",
      "Epoch 15/15 | Train Loss: 0.3899 Acc: 0.8439\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5633 Acc: 0.7817\n",
      "Epoch 2/15 | Train Loss: 0.5062 Acc: 0.8210\n",
      "Epoch 3/15 | Train Loss: 0.4277 Acc: 0.8210\n",
      "Epoch 4/15 | Train Loss: 0.4110 Acc: 0.8603\n",
      "Epoch 5/15 | Train Loss: 0.4421 Acc: 0.8472\n",
      "Epoch 6/15 | Train Loss: 0.4548 Acc: 0.8297\n",
      "Epoch 7/15 | Train Loss: 0.3920 Acc: 0.8297\n",
      "Epoch 8/15 | Train Loss: 0.3392 Acc: 0.8821\n",
      "Epoch 9/15 | Train Loss: 0.3339 Acc: 0.8690\n",
      "Epoch 10/15 | Train Loss: 0.3184 Acc: 0.8690\n",
      "Epoch 11/15 | Train Loss: 0.3178 Acc: 0.8952\n",
      "Epoch 12/15 | Train Loss: 0.2777 Acc: 0.8952\n",
      "Epoch 13/15 | Train Loss: 0.2888 Acc: 0.8821\n",
      "Epoch 14/15 | Train Loss: 0.2706 Acc: 0.8996\n",
      "Epoch 15/15 | Train Loss: 0.2728 Acc: 0.8996\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5565 Acc: 0.7383\n",
      "Epoch 2/15 | Train Loss: 0.4690 Acc: 0.7850\n",
      "Epoch 3/15 | Train Loss: 0.3997 Acc: 0.8083\n",
      "Epoch 4/15 | Train Loss: 0.3771 Acc: 0.7772\n",
      "Epoch 5/15 | Train Loss: 0.3290 Acc: 0.8238\n",
      "Epoch 6/15 | Train Loss: 0.3499 Acc: 0.8238\n",
      "Epoch 7/15 | Train Loss: 0.3492 Acc: 0.8135\n",
      "Epoch 8/15 | Train Loss: 0.2912 Acc: 0.8601\n",
      "Epoch 9/15 | Train Loss: 0.2951 Acc: 0.8575\n",
      "Epoch 10/15 | Train Loss: 0.2812 Acc: 0.8756\n",
      "Epoch 11/15 | Train Loss: 0.2839 Acc: 0.8523\n",
      "Epoch 12/15 | Train Loss: 0.2563 Acc: 0.8938\n",
      "Epoch 13/15 | Train Loss: 0.2655 Acc: 0.8756\n",
      "Epoch 14/15 | Train Loss: 0.2334 Acc: 0.8964\n",
      "Epoch 15/15 | Train Loss: 0.2319 Acc: 0.9119\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6462 Acc: 0.7383\n",
      "Epoch 2/15 | Train Loss: 0.4074 Acc: 0.7720\n",
      "Epoch 3/15 | Train Loss: 0.3613 Acc: 0.8083\n",
      "Epoch 4/15 | Train Loss: 0.3751 Acc: 0.8109\n",
      "Epoch 5/15 | Train Loss: 0.3762 Acc: 0.8057\n",
      "Epoch 6/15 | Train Loss: 0.3886 Acc: 0.7979\n",
      "Epoch 7/15 | Train Loss: 0.3204 Acc: 0.8446\n",
      "Epoch 8/15 | Train Loss: 0.4076 Acc: 0.8135\n",
      "Epoch 9/15 | Train Loss: 0.2835 Acc: 0.8601\n",
      "Epoch 10/15 | Train Loss: 0.3220 Acc: 0.8238\n",
      "Epoch 11/15 | Train Loss: 0.2866 Acc: 0.8653\n",
      "Epoch 12/15 | Train Loss: 0.2924 Acc: 0.8446\n",
      "Epoch 13/15 | Train Loss: 0.2653 Acc: 0.8731\n",
      "Epoch 14/15 | Train Loss: 0.2896 Acc: 0.8575\n",
      "Epoch 15/15 | Train Loss: 0.2746 Acc: 0.8575\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6266 Acc: 0.7202\n",
      "Epoch 2/15 | Train Loss: 0.4251 Acc: 0.7772\n",
      "Epoch 3/15 | Train Loss: 0.4412 Acc: 0.7746\n",
      "Epoch 4/15 | Train Loss: 0.3610 Acc: 0.8031\n",
      "Epoch 5/15 | Train Loss: 0.3571 Acc: 0.8109\n",
      "Epoch 6/15 | Train Loss: 0.3323 Acc: 0.8238\n",
      "Epoch 7/15 | Train Loss: 0.3300 Acc: 0.8290\n",
      "Epoch 8/15 | Train Loss: 0.3098 Acc: 0.8342\n",
      "Epoch 9/15 | Train Loss: 0.3222 Acc: 0.8446\n",
      "Epoch 10/15 | Train Loss: 0.3153 Acc: 0.8420\n",
      "Epoch 11/15 | Train Loss: 0.2938 Acc: 0.8549\n",
      "Epoch 12/15 | Train Loss: 0.2769 Acc: 0.8808\n",
      "Epoch 13/15 | Train Loss: 0.3038 Acc: 0.8342\n",
      "Epoch 14/15 | Train Loss: 0.2873 Acc: 0.8523\n",
      "Epoch 15/15 | Train Loss: 0.2972 Acc: 0.8446\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5151 Acc: 0.7587\n",
      "Epoch 2/15 | Train Loss: 0.3984 Acc: 0.8317\n",
      "Epoch 3/15 | Train Loss: 0.3689 Acc: 0.8349\n",
      "Epoch 4/15 | Train Loss: 0.3591 Acc: 0.8381\n",
      "Epoch 5/15 | Train Loss: 0.3330 Acc: 0.8635\n",
      "Epoch 6/15 | Train Loss: 0.3198 Acc: 0.8635\n",
      "Epoch 7/15 | Train Loss: 0.3452 Acc: 0.8254\n",
      "Epoch 8/15 | Train Loss: 0.3173 Acc: 0.8413\n",
      "Epoch 9/15 | Train Loss: 0.3175 Acc: 0.8825\n",
      "Epoch 10/15 | Train Loss: 0.2852 Acc: 0.8698\n",
      "Epoch 11/15 | Train Loss: 0.3281 Acc: 0.8444\n",
      "Epoch 12/15 | Train Loss: 0.2873 Acc: 0.8794\n",
      "Epoch 13/15 | Train Loss: 0.2985 Acc: 0.8762\n",
      "Epoch 14/15 | Train Loss: 0.3113 Acc: 0.8508\n",
      "Epoch 15/15 | Train Loss: 0.2966 Acc: 0.8603\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5262 Acc: 0.7845\n",
      "Epoch 2/15 | Train Loss: 0.4083 Acc: 0.8339\n",
      "Epoch 3/15 | Train Loss: 0.3941 Acc: 0.8445\n",
      "Epoch 4/15 | Train Loss: 0.3245 Acc: 0.8622\n",
      "Epoch 5/15 | Train Loss: 0.2689 Acc: 0.8834\n",
      "Epoch 6/15 | Train Loss: 0.3441 Acc: 0.8622\n",
      "Epoch 7/15 | Train Loss: 0.3402 Acc: 0.8657\n",
      "Epoch 8/15 | Train Loss: 0.2702 Acc: 0.8869\n",
      "Epoch 9/15 | Train Loss: 0.2677 Acc: 0.8905\n",
      "Epoch 10/15 | Train Loss: 0.2615 Acc: 0.8905\n",
      "Epoch 11/15 | Train Loss: 0.3091 Acc: 0.8763\n",
      "Epoch 12/15 | Train Loss: 0.3078 Acc: 0.8587\n",
      "Epoch 13/15 | Train Loss: 0.2442 Acc: 0.9117\n",
      "Epoch 14/15 | Train Loss: 0.2318 Acc: 0.8975\n",
      "Epoch 15/15 | Train Loss: 0.2363 Acc: 0.9046\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7176 Acc: 0.6142\n",
      "Epoch 2/15 | Train Loss: 0.5925 Acc: 0.6693\n",
      "Epoch 3/15 | Train Loss: 0.5303 Acc: 0.7087\n",
      "Epoch 4/15 | Train Loss: 0.4712 Acc: 0.7480\n",
      "Epoch 5/15 | Train Loss: 0.4596 Acc: 0.7402\n",
      "Epoch 6/15 | Train Loss: 0.5158 Acc: 0.7677\n",
      "Epoch 7/15 | Train Loss: 0.4492 Acc: 0.7677\n",
      "Epoch 8/15 | Train Loss: 0.4071 Acc: 0.8228\n",
      "Epoch 9/15 | Train Loss: 0.4283 Acc: 0.7756\n",
      "Epoch 10/15 | Train Loss: 0.3591 Acc: 0.8268\n",
      "Epoch 11/15 | Train Loss: 0.4284 Acc: 0.7992\n",
      "Epoch 12/15 | Train Loss: 0.4008 Acc: 0.8346\n",
      "Epoch 13/15 | Train Loss: 0.4135 Acc: 0.8031\n",
      "Epoch 14/15 | Train Loss: 0.3528 Acc: 0.8268\n",
      "Epoch 15/15 | Train Loss: 0.3614 Acc: 0.8465\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6986 Acc: 0.6654\n",
      "Epoch 2/15 | Train Loss: 0.6942 Acc: 0.6378\n",
      "Epoch 3/15 | Train Loss: 0.5599 Acc: 0.6614\n",
      "Epoch 4/15 | Train Loss: 0.5608 Acc: 0.7008\n",
      "Epoch 5/15 | Train Loss: 0.6188 Acc: 0.6693\n",
      "Epoch 6/15 | Train Loss: 0.5580 Acc: 0.7087\n",
      "Epoch 7/15 | Train Loss: 0.5142 Acc: 0.7244\n",
      "Epoch 8/15 | Train Loss: 0.4700 Acc: 0.7638\n",
      "Epoch 9/15 | Train Loss: 0.4360 Acc: 0.7598\n",
      "Epoch 10/15 | Train Loss: 0.4589 Acc: 0.7559\n",
      "Epoch 11/15 | Train Loss: 0.4612 Acc: 0.7520\n",
      "Epoch 12/15 | Train Loss: 0.4297 Acc: 0.7756\n",
      "Epoch 13/15 | Train Loss: 0.4257 Acc: 0.7913\n",
      "Epoch 14/15 | Train Loss: 0.4314 Acc: 0.8031\n",
      "Epoch 15/15 | Train Loss: 0.4486 Acc: 0.7835\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7021 Acc: 0.6378\n",
      "Epoch 2/15 | Train Loss: 0.6446 Acc: 0.6299\n",
      "Epoch 3/15 | Train Loss: 0.6117 Acc: 0.7008\n",
      "Epoch 4/15 | Train Loss: 0.5238 Acc: 0.7441\n",
      "Epoch 5/15 | Train Loss: 0.4939 Acc: 0.7520\n",
      "Epoch 6/15 | Train Loss: 0.4790 Acc: 0.7992\n",
      "Epoch 7/15 | Train Loss: 0.4959 Acc: 0.7402\n",
      "Epoch 8/15 | Train Loss: 0.5116 Acc: 0.7323\n",
      "Epoch 9/15 | Train Loss: 0.4569 Acc: 0.7756\n",
      "Epoch 10/15 | Train Loss: 0.4068 Acc: 0.8228\n",
      "Epoch 11/15 | Train Loss: 0.4505 Acc: 0.7835\n",
      "Epoch 12/15 | Train Loss: 0.4345 Acc: 0.7992\n",
      "Epoch 13/15 | Train Loss: 0.4571 Acc: 0.7638\n",
      "Epoch 14/15 | Train Loss: 0.4128 Acc: 0.8071\n",
      "Epoch 15/15 | Train Loss: 0.4036 Acc: 0.8110\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6539 Acc: 0.7131\n",
      "Epoch 2/15 | Train Loss: 0.5261 Acc: 0.7764\n",
      "Epoch 3/15 | Train Loss: 0.5514 Acc: 0.7764\n",
      "Epoch 4/15 | Train Loss: 0.4850 Acc: 0.7637\n",
      "Epoch 5/15 | Train Loss: 0.4815 Acc: 0.7848\n",
      "Epoch 6/15 | Train Loss: 0.4714 Acc: 0.7722\n",
      "Epoch 7/15 | Train Loss: 0.4241 Acc: 0.8101\n",
      "Epoch 8/15 | Train Loss: 0.4159 Acc: 0.8143\n",
      "Epoch 9/15 | Train Loss: 0.4269 Acc: 0.7848\n",
      "Epoch 10/15 | Train Loss: 0.3938 Acc: 0.8186\n",
      "Epoch 11/15 | Train Loss: 0.4138 Acc: 0.8143\n",
      "Epoch 12/15 | Train Loss: 0.4105 Acc: 0.8312\n",
      "Epoch 13/15 | Train Loss: 0.4660 Acc: 0.8143\n",
      "Epoch 14/15 | Train Loss: 0.3961 Acc: 0.8228\n",
      "Epoch 15/15 | Train Loss: 0.4266 Acc: 0.8312\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5869 Acc: 0.7511\n",
      "Epoch 2/15 | Train Loss: 0.4858 Acc: 0.8166\n",
      "Epoch 3/15 | Train Loss: 0.4639 Acc: 0.8079\n",
      "Epoch 4/15 | Train Loss: 0.3830 Acc: 0.8646\n",
      "Epoch 5/15 | Train Loss: 0.3939 Acc: 0.8210\n",
      "Epoch 6/15 | Train Loss: 0.4199 Acc: 0.8166\n",
      "Epoch 7/15 | Train Loss: 0.3981 Acc: 0.8603\n",
      "Epoch 8/15 | Train Loss: 0.3245 Acc: 0.8603\n",
      "Epoch 9/15 | Train Loss: 0.3083 Acc: 0.8690\n",
      "Epoch 10/15 | Train Loss: 0.2962 Acc: 0.8690\n",
      "Epoch 11/15 | Train Loss: 0.3146 Acc: 0.8690\n",
      "Epoch 12/15 | Train Loss: 0.2732 Acc: 0.8865\n",
      "Epoch 13/15 | Train Loss: 0.2990 Acc: 0.8865\n",
      "Epoch 14/15 | Train Loss: 0.3651 Acc: 0.8690\n",
      "Epoch 15/15 | Train Loss: 0.3078 Acc: 0.8603\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5226 Acc: 0.7563\n",
      "Epoch 2/15 | Train Loss: 0.4390 Acc: 0.7950\n",
      "Epoch 3/15 | Train Loss: 0.4118 Acc: 0.7790\n",
      "Epoch 4/15 | Train Loss: 0.3521 Acc: 0.8360\n",
      "Epoch 5/15 | Train Loss: 0.3593 Acc: 0.8246\n",
      "Epoch 6/15 | Train Loss: 0.3624 Acc: 0.8178\n",
      "Epoch 7/15 | Train Loss: 0.3296 Acc: 0.8292\n",
      "Epoch 8/15 | Train Loss: 0.2773 Acc: 0.8702\n",
      "Epoch 9/15 | Train Loss: 0.3131 Acc: 0.8451\n",
      "Epoch 10/15 | Train Loss: 0.2980 Acc: 0.8519\n",
      "Epoch 11/15 | Train Loss: 0.2794 Acc: 0.8838\n",
      "Epoch 12/15 | Train Loss: 0.2835 Acc: 0.8497\n",
      "Epoch 13/15 | Train Loss: 0.2757 Acc: 0.8815\n",
      "Epoch 14/15 | Train Loss: 0.2810 Acc: 0.8702\n",
      "Epoch 15/15 | Train Loss: 0.2729 Acc: 0.8679\n",
      "Fold 4 Test Accuracy: 0.6109\n",
      "===== Fold 5 =====\n",
      "Epoch 1: Generator Loss = 10.4278, Discriminator Loss = 8.6638\n",
      "Epoch 2: Generator Loss = 12.4684, Discriminator Loss = 5.3335\n",
      "Epoch 3: Generator Loss = 19.1822, Discriminator Loss = 4.0775\n",
      "Epoch 4: Generator Loss = 25.9127, Discriminator Loss = 4.2167\n",
      "Epoch 5: Generator Loss = 28.5039, Discriminator Loss = 5.6558\n",
      "Epoch 6: Generator Loss = 26.1750, Discriminator Loss = 4.9620\n",
      "Epoch 7: Generator Loss = 27.5218, Discriminator Loss = 5.6498\n",
      "Epoch 8: Generator Loss = 29.9207, Discriminator Loss = 4.1683\n",
      "Epoch 9: Generator Loss = 26.0372, Discriminator Loss = 6.5213\n",
      "Epoch 10: Generator Loss = 26.5688, Discriminator Loss = 5.7550\n",
      "Epoch 11: Generator Loss = 31.6470, Discriminator Loss = 10.8314\n",
      "Epoch 12: Generator Loss = 15.6511, Discriminator Loss = 6.7988\n",
      "Epoch 13: Generator Loss = 22.5157, Discriminator Loss = 13.2851\n",
      "Epoch 14: Generator Loss = 16.2587, Discriminator Loss = 8.8863\n",
      "Epoch 15: Generator Loss = 14.9672, Discriminator Loss = 8.2303\n",
      "Epoch 16: Generator Loss = 18.0516, Discriminator Loss = 8.7361\n",
      "Epoch 17: Generator Loss = 16.1222, Discriminator Loss = 9.4560\n",
      "Epoch 18: Generator Loss = 17.0544, Discriminator Loss = 9.0427\n",
      "Epoch 19: Generator Loss = 17.5988, Discriminator Loss = 8.0451\n",
      "Epoch 20: Generator Loss = 18.0812, Discriminator Loss = 9.8418\n",
      "Epoch 21: Generator Loss = 15.1704, Discriminator Loss = 8.5319\n",
      "Epoch 22: Generator Loss = 15.9350, Discriminator Loss = 9.1374\n",
      "Epoch 23: Generator Loss = 16.0013, Discriminator Loss = 8.2747\n",
      "Epoch 24: Generator Loss = 16.4772, Discriminator Loss = 9.0129\n",
      "Epoch 25: Generator Loss = 16.4254, Discriminator Loss = 8.2854\n",
      "Epoch 26: Generator Loss = 21.3645, Discriminator Loss = 8.7436\n",
      "Epoch 27: Generator Loss = 16.0621, Discriminator Loss = 8.4587\n",
      "Epoch 28: Generator Loss = 17.5824, Discriminator Loss = 8.2090\n",
      "Epoch 29: Generator Loss = 17.0410, Discriminator Loss = 9.2554\n",
      "Epoch 30: Generator Loss = 15.9513, Discriminator Loss = 8.3410\n",
      "Epoch 31: Generator Loss = 13.9649, Discriminator Loss = 8.5278\n",
      "Epoch 32: Generator Loss = 17.8442, Discriminator Loss = 8.2646\n",
      "Epoch 33: Generator Loss = 17.9823, Discriminator Loss = 8.8029\n",
      "Epoch 34: Generator Loss = 17.4365, Discriminator Loss = 8.7775\n",
      "Epoch 35: Generator Loss = 14.0225, Discriminator Loss = 9.1224\n",
      "Epoch 36: Generator Loss = 14.5926, Discriminator Loss = 8.5536\n",
      "Epoch 37: Generator Loss = 17.7517, Discriminator Loss = 7.9212\n",
      "Epoch 38: Generator Loss = 15.4361, Discriminator Loss = 8.5225\n",
      "Epoch 39: Generator Loss = 17.4152, Discriminator Loss = 9.3804\n",
      "Epoch 40: Generator Loss = 16.3245, Discriminator Loss = 7.5650\n",
      "Epoch 41: Generator Loss = 15.9714, Discriminator Loss = 9.3269\n",
      "Epoch 42: Generator Loss = 16.7974, Discriminator Loss = 8.4114\n",
      "Epoch 43: Generator Loss = 16.6784, Discriminator Loss = 7.7564\n",
      "Epoch 44: Generator Loss = 21.1625, Discriminator Loss = 7.6466\n",
      "Epoch 45: Generator Loss = 17.7692, Discriminator Loss = 9.0702\n",
      "Epoch 46: Generator Loss = 20.2246, Discriminator Loss = 7.3791\n",
      "Epoch 47: Generator Loss = 19.6672, Discriminator Loss = 8.6554\n",
      "Epoch 48: Generator Loss = 16.9254, Discriminator Loss = 7.4399\n",
      "Epoch 49: Generator Loss = 19.5679, Discriminator Loss = 6.2535\n",
      "Epoch 50: Generator Loss = 22.3975, Discriminator Loss = 7.5325\n",
      "Epoch 51: Generator Loss = 23.2651, Discriminator Loss = 5.1223\n",
      "Epoch 52: Generator Loss = 21.6473, Discriminator Loss = 5.8212\n",
      "Epoch 53: Generator Loss = 21.5107, Discriminator Loss = 8.4862\n",
      "Epoch 54: Generator Loss = 25.3762, Discriminator Loss = 5.6762\n",
      "Epoch 55: Generator Loss = 32.8984, Discriminator Loss = 4.0491\n",
      "Epoch 56: Generator Loss = 28.8057, Discriminator Loss = 6.2914\n",
      "Epoch 57: Generator Loss = 34.1065, Discriminator Loss = 4.2782\n",
      "Epoch 58: Generator Loss = 30.6036, Discriminator Loss = 3.7603\n",
      "Epoch 59: Generator Loss = 36.8122, Discriminator Loss = 3.3707\n",
      "Epoch 60: Generator Loss = 38.4313, Discriminator Loss = 4.0312\n",
      "Epoch 61: Generator Loss = 37.4220, Discriminator Loss = 2.6492\n",
      "Epoch 62: Generator Loss = 41.9374, Discriminator Loss = 4.3432\n",
      "Epoch 63: Generator Loss = 34.5809, Discriminator Loss = 3.0584\n",
      "Epoch 64: Generator Loss = 49.7318, Discriminator Loss = 3.7218\n",
      "Epoch 65: Generator Loss = 44.6740, Discriminator Loss = 3.3558\n",
      "Epoch 66: Generator Loss = 39.7071, Discriminator Loss = 3.1205\n",
      "Epoch 67: Generator Loss = 54.0130, Discriminator Loss = 4.0995\n",
      "Epoch 68: Generator Loss = 32.8636, Discriminator Loss = 5.1701\n",
      "Epoch 69: Generator Loss = 52.6062, Discriminator Loss = 2.7465\n",
      "Epoch 70: Generator Loss = 52.9748, Discriminator Loss = 1.9960\n",
      "Epoch 71: Generator Loss = 51.8889, Discriminator Loss = 2.0292\n",
      "Epoch 72: Generator Loss = 42.0923, Discriminator Loss = 1.2568\n",
      "Epoch 73: Generator Loss = 54.4603, Discriminator Loss = 1.4397\n",
      "Epoch 74: Generator Loss = 58.1514, Discriminator Loss = 1.0205\n",
      "Epoch 75: Generator Loss = 57.7867, Discriminator Loss = 1.3053\n",
      "Epoch 76: Generator Loss = 56.5105, Discriminator Loss = 1.6419\n",
      "Epoch 77: Generator Loss = 57.4372, Discriminator Loss = 1.2130\n",
      "Epoch 78: Generator Loss = 58.0744, Discriminator Loss = 0.9906\n",
      "Epoch 79: Generator Loss = 49.0300, Discriminator Loss = 1.1392\n",
      "Epoch 80: Generator Loss = 63.0694, Discriminator Loss = 3.8304\n",
      "Epoch 81: Generator Loss = 39.3517, Discriminator Loss = 3.3785\n",
      "Epoch 82: Generator Loss = 63.0606, Discriminator Loss = 1.4483\n",
      "Epoch 83: Generator Loss = 63.4566, Discriminator Loss = 1.3076\n",
      "Epoch 84: Generator Loss = 63.1337, Discriminator Loss = 1.2167\n",
      "Epoch 85: Generator Loss = 79.2051, Discriminator Loss = 0.6685\n",
      "Epoch 86: Generator Loss = 73.4739, Discriminator Loss = 1.2391\n",
      "Epoch 87: Generator Loss = 68.4569, Discriminator Loss = 0.8548\n",
      "Epoch 88: Generator Loss = 71.7655, Discriminator Loss = 1.4089\n",
      "Epoch 89: Generator Loss = 70.2719, Discriminator Loss = 2.2248\n",
      "Epoch 90: Generator Loss = 60.5277, Discriminator Loss = 1.5418\n",
      "Epoch 91: Generator Loss = 70.2891, Discriminator Loss = 1.0272\n",
      "Epoch 92: Generator Loss = 71.2778, Discriminator Loss = 0.8665\n",
      "Epoch 93: Generator Loss = 78.3798, Discriminator Loss = 0.9999\n",
      "Epoch 94: Generator Loss = 74.2063, Discriminator Loss = 0.7091\n",
      "Epoch 95: Generator Loss = 81.4060, Discriminator Loss = 0.4618\n",
      "Epoch 96: Generator Loss = 75.0760, Discriminator Loss = 0.7328\n",
      "Epoch 97: Generator Loss = 94.6465, Discriminator Loss = 0.2977\n",
      "Epoch 98: Generator Loss = 110.8233, Discriminator Loss = 0.7483\n",
      "Epoch 99: Generator Loss = 127.5344, Discriminator Loss = 1.2266\n",
      "Epoch 100: Generator Loss = 113.7938, Discriminator Loss = 0.1121\n",
      "Epoch 101: Generator Loss = 100.6059, Discriminator Loss = 0.0908\n",
      "Epoch 102: Generator Loss = 104.9242, Discriminator Loss = 0.2418\n",
      "Epoch 103: Generator Loss = 91.6366, Discriminator Loss = 0.1306\n",
      "Epoch 104: Generator Loss = 101.7079, Discriminator Loss = 0.1102\n",
      "Epoch 105: Generator Loss = 86.2567, Discriminator Loss = 0.0982\n",
      "Epoch 106: Generator Loss = 85.6278, Discriminator Loss = 0.0968\n",
      "Epoch 107: Generator Loss = 85.8023, Discriminator Loss = 0.0536\n",
      "Epoch 108: Generator Loss = 76.0112, Discriminator Loss = 0.0870\n",
      "Epoch 109: Generator Loss = 86.5612, Discriminator Loss = 0.0524\n",
      "Epoch 110: Generator Loss = 81.0877, Discriminator Loss = 0.0721\n",
      "Epoch 111: Generator Loss = 89.3451, Discriminator Loss = 0.0540\n",
      "Epoch 112: Generator Loss = 90.1674, Discriminator Loss = 0.0681\n",
      "Epoch 113: Generator Loss = 84.7160, Discriminator Loss = 0.0684\n",
      "Epoch 114: Generator Loss = 99.8423, Discriminator Loss = 0.0567\n",
      "Epoch 115: Generator Loss = 94.1255, Discriminator Loss = 0.0540\n",
      "Epoch 116: Generator Loss = 104.8044, Discriminator Loss = 0.0305\n",
      "Epoch 117: Generator Loss = 104.9838, Discriminator Loss = 0.0449\n",
      "Epoch 118: Generator Loss = 108.8390, Discriminator Loss = 0.3862\n",
      "Epoch 119: Generator Loss = 116.7224, Discriminator Loss = 24.2531\n",
      "Epoch 120: Generator Loss = 71.6978, Discriminator Loss = 5.3760\n",
      "Epoch 121: Generator Loss = 66.7959, Discriminator Loss = 1.6967\n",
      "Epoch 122: Generator Loss = 63.4553, Discriminator Loss = 1.3018\n",
      "Epoch 123: Generator Loss = 68.2912, Discriminator Loss = 1.6902\n",
      "Epoch 124: Generator Loss = 60.3662, Discriminator Loss = 3.6401\n",
      "Epoch 125: Generator Loss = 62.4931, Discriminator Loss = 1.3064\n",
      "Epoch 126: Generator Loss = 65.8260, Discriminator Loss = 0.5187\n",
      "Epoch 127: Generator Loss = 61.7989, Discriminator Loss = 0.8969\n",
      "Epoch 128: Generator Loss = 66.6803, Discriminator Loss = 1.1624\n",
      "Epoch 129: Generator Loss = 73.8095, Discriminator Loss = 1.4062\n",
      "Epoch 130: Generator Loss = 62.6604, Discriminator Loss = 1.5170\n",
      "Epoch 131: Generator Loss = 64.5649, Discriminator Loss = 1.2869\n",
      "Epoch 132: Generator Loss = 66.6122, Discriminator Loss = 1.0296\n",
      "Epoch 133: Generator Loss = 75.1147, Discriminator Loss = 1.3527\n",
      "Epoch 134: Generator Loss = 66.2595, Discriminator Loss = 0.9928\n",
      "Epoch 135: Generator Loss = 67.9354, Discriminator Loss = 0.9559\n",
      "Epoch 136: Generator Loss = 69.6869, Discriminator Loss = 0.7328\n",
      "Epoch 137: Generator Loss = 81.6201, Discriminator Loss = 3.2492\n",
      "Epoch 138: Generator Loss = 64.4736, Discriminator Loss = 1.1903\n",
      "Epoch 139: Generator Loss = 72.3265, Discriminator Loss = 1.1452\n",
      "Epoch 140: Generator Loss = 71.1788, Discriminator Loss = 0.9128\n",
      "Epoch 141: Generator Loss = 81.5892, Discriminator Loss = 0.5786\n",
      "Epoch 142: Generator Loss = 84.1481, Discriminator Loss = 0.5288\n",
      "Epoch 143: Generator Loss = 66.2471, Discriminator Loss = 0.7442\n",
      "Epoch 144: Generator Loss = 73.0149, Discriminator Loss = 0.6656\n",
      "Epoch 145: Generator Loss = 83.3873, Discriminator Loss = 0.3224\n",
      "Epoch 146: Generator Loss = 82.3503, Discriminator Loss = 0.7250\n",
      "Epoch 147: Generator Loss = 84.3443, Discriminator Loss = 1.1781\n",
      "Epoch 148: Generator Loss = 77.4194, Discriminator Loss = 0.5864\n",
      "Epoch 149: Generator Loss = 81.2369, Discriminator Loss = 0.5588\n",
      "Epoch 150: Generator Loss = 85.6591, Discriminator Loss = 0.6840\n",
      "Epoch 151: Generator Loss = 77.6110, Discriminator Loss = 0.5600\n",
      "Epoch 152: Generator Loss = 82.5807, Discriminator Loss = 0.8902\n",
      "Epoch 153: Generator Loss = 87.6277, Discriminator Loss = 0.6022\n",
      "Epoch 154: Generator Loss = 88.4998, Discriminator Loss = 1.1550\n",
      "Epoch 155: Generator Loss = 63.7440, Discriminator Loss = 2.1067\n",
      "Epoch 156: Generator Loss = 75.4224, Discriminator Loss = 0.6900\n",
      "Epoch 157: Generator Loss = 79.3113, Discriminator Loss = 0.5487\n",
      "Epoch 158: Generator Loss = 76.9823, Discriminator Loss = 0.6128\n",
      "Epoch 159: Generator Loss = 80.6773, Discriminator Loss = 1.2817\n",
      "Epoch 160: Generator Loss = 81.1534, Discriminator Loss = 0.7914\n",
      "Epoch 161: Generator Loss = 77.1902, Discriminator Loss = 0.9372\n",
      "Epoch 162: Generator Loss = 104.8336, Discriminator Loss = 0.4232\n",
      "Epoch 163: Generator Loss = 92.3884, Discriminator Loss = 0.3887\n",
      "Epoch 164: Generator Loss = 71.2907, Discriminator Loss = 0.3895\n",
      "Epoch 165: Generator Loss = 86.8676, Discriminator Loss = 0.2944\n",
      "Epoch 166: Generator Loss = 89.3313, Discriminator Loss = 0.2515\n",
      "Epoch 167: Generator Loss = 80.5179, Discriminator Loss = 0.3005\n",
      "Epoch 168: Generator Loss = 84.2548, Discriminator Loss = 0.3501\n",
      "Epoch 169: Generator Loss = 104.9336, Discriminator Loss = 0.2815\n",
      "Epoch 170: Generator Loss = 98.0006, Discriminator Loss = 0.2032\n",
      "Epoch 171: Generator Loss = 78.0702, Discriminator Loss = 0.3667\n",
      "Epoch 172: Generator Loss = 78.1233, Discriminator Loss = 0.6496\n",
      "Epoch 173: Generator Loss = 66.7236, Discriminator Loss = 4.8921\n",
      "Epoch 174: Generator Loss = 95.4853, Discriminator Loss = 0.9957\n",
      "Epoch 175: Generator Loss = 59.2155, Discriminator Loss = 2.7374\n",
      "Epoch 176: Generator Loss = 64.9226, Discriminator Loss = 1.1014\n",
      "Epoch 177: Generator Loss = 80.2647, Discriminator Loss = 1.7923\n",
      "Epoch 178: Generator Loss = 80.7478, Discriminator Loss = 1.1014\n",
      "Epoch 179: Generator Loss = 68.2577, Discriminator Loss = 1.6783\n",
      "Epoch 180: Generator Loss = 67.0886, Discriminator Loss = 1.4261\n",
      "Epoch 181: Generator Loss = 80.0783, Discriminator Loss = 0.4508\n",
      "Epoch 182: Generator Loss = 65.3007, Discriminator Loss = 2.1854\n",
      "Epoch 183: Generator Loss = 74.0665, Discriminator Loss = 1.1427\n",
      "Epoch 184: Generator Loss = 78.2314, Discriminator Loss = 1.1731\n",
      "Epoch 185: Generator Loss = 67.2970, Discriminator Loss = 0.8156\n",
      "Epoch 186: Generator Loss = 80.9389, Discriminator Loss = 0.7831\n",
      "Epoch 187: Generator Loss = 84.6929, Discriminator Loss = 1.4136\n",
      "Epoch 188: Generator Loss = 64.6484, Discriminator Loss = 1.9288\n",
      "Epoch 189: Generator Loss = 92.5765, Discriminator Loss = 3.8016\n",
      "Epoch 190: Generator Loss = 60.7359, Discriminator Loss = 3.3514\n",
      "Epoch 191: Generator Loss = 61.6682, Discriminator Loss = 1.7162\n",
      "Epoch 192: Generator Loss = 78.2733, Discriminator Loss = 0.7834\n",
      "Epoch 193: Generator Loss = 71.3434, Discriminator Loss = 1.2212\n",
      "Epoch 194: Generator Loss = 73.8907, Discriminator Loss = 1.0946\n",
      "Epoch 195: Generator Loss = 81.7447, Discriminator Loss = 0.8622\n",
      "Epoch 196: Generator Loss = 56.4581, Discriminator Loss = 0.9027\n",
      "Epoch 197: Generator Loss = 79.8070, Discriminator Loss = 0.8042\n",
      "Epoch 198: Generator Loss = 76.5843, Discriminator Loss = 0.8799\n",
      "Epoch 199: Generator Loss = 67.4533, Discriminator Loss = 0.9151\n",
      "Epoch 200: Generator Loss = 78.3955, Discriminator Loss = 0.7418\n",
      "Epoch 201: Generator Loss = 82.6806, Discriminator Loss = 1.6120\n",
      "Epoch 202: Generator Loss = 68.5950, Discriminator Loss = 0.9820\n",
      "Epoch 203: Generator Loss = 79.6819, Discriminator Loss = 1.3717\n",
      "Epoch 204: Generator Loss = 66.9628, Discriminator Loss = 0.5343\n",
      "Epoch 205: Generator Loss = 89.6801, Discriminator Loss = 0.8470\n",
      "Epoch 206: Generator Loss = 70.7517, Discriminator Loss = 2.9028\n",
      "Epoch 207: Generator Loss = 64.8025, Discriminator Loss = 1.4396\n",
      "Epoch 208: Generator Loss = 62.8975, Discriminator Loss = 2.0044\n",
      "Epoch 209: Generator Loss = 83.3125, Discriminator Loss = 0.7164\n",
      "Epoch 210: Generator Loss = 81.3661, Discriminator Loss = 1.2420\n",
      "Epoch 211: Generator Loss = 60.0198, Discriminator Loss = 6.1429\n",
      "Epoch 212: Generator Loss = 42.4596, Discriminator Loss = 7.8893\n",
      "Epoch 213: Generator Loss = 42.8896, Discriminator Loss = 3.2150\n",
      "Epoch 214: Generator Loss = 65.8409, Discriminator Loss = 1.4250\n",
      "Epoch 215: Generator Loss = 54.0951, Discriminator Loss = 4.4376\n",
      "Epoch 216: Generator Loss = 60.9137, Discriminator Loss = 2.7261\n",
      "Epoch 217: Generator Loss = 50.5352, Discriminator Loss = 2.2903\n",
      "Epoch 218: Generator Loss = 55.8817, Discriminator Loss = 1.3288\n",
      "Epoch 219: Generator Loss = 77.0113, Discriminator Loss = 1.3826\n",
      "Epoch 220: Generator Loss = 61.0217, Discriminator Loss = 1.3818\n",
      "Epoch 221: Generator Loss = 72.2141, Discriminator Loss = 2.1264\n",
      "Epoch 222: Generator Loss = 64.3134, Discriminator Loss = 1.3647\n",
      "Epoch 223: Generator Loss = 62.0188, Discriminator Loss = 1.1284\n",
      "Epoch 224: Generator Loss = 55.1955, Discriminator Loss = 5.8707\n",
      "Epoch 225: Generator Loss = 45.0758, Discriminator Loss = 2.0621\n",
      "Epoch 226: Generator Loss = 71.3513, Discriminator Loss = 2.6194\n",
      "Epoch 227: Generator Loss = 51.9528, Discriminator Loss = 3.3276\n",
      "Epoch 228: Generator Loss = 70.6180, Discriminator Loss = 1.6531\n",
      "Epoch 229: Generator Loss = 67.7765, Discriminator Loss = 2.2091\n",
      "Epoch 230: Generator Loss = 73.6604, Discriminator Loss = 1.5184\n",
      "Epoch 231: Generator Loss = 77.2761, Discriminator Loss = 1.0177\n",
      "Epoch 232: Generator Loss = 73.7996, Discriminator Loss = 2.3182\n",
      "Epoch 233: Generator Loss = 73.8912, Discriminator Loss = 1.0490\n",
      "Epoch 234: Generator Loss = 56.9863, Discriminator Loss = 1.0561\n",
      "Epoch 235: Generator Loss = 70.2025, Discriminator Loss = 0.9416\n",
      "Epoch 236: Generator Loss = 77.0087, Discriminator Loss = 1.0071\n",
      "Epoch 237: Generator Loss = 72.0649, Discriminator Loss = 0.9582\n",
      "Epoch 238: Generator Loss = 70.2697, Discriminator Loss = 0.5790\n",
      "Epoch 239: Generator Loss = 80.4355, Discriminator Loss = 0.4942\n",
      "Epoch 240: Generator Loss = 84.9617, Discriminator Loss = 0.6994\n",
      "Epoch 241: Generator Loss = 112.7757, Discriminator Loss = 0.4428\n",
      "Epoch 242: Generator Loss = 93.3086, Discriminator Loss = 0.2158\n",
      "Epoch 243: Generator Loss = 78.8807, Discriminator Loss = 0.7250\n",
      "Epoch 244: Generator Loss = 76.4614, Discriminator Loss = 1.1618\n",
      "Epoch 245: Generator Loss = 85.5422, Discriminator Loss = 1.3357\n",
      "Epoch 246: Generator Loss = 90.7743, Discriminator Loss = 1.0040\n",
      "Epoch 247: Generator Loss = 61.9440, Discriminator Loss = 6.8718\n",
      "Epoch 248: Generator Loss = 56.4081, Discriminator Loss = 2.9535\n",
      "Epoch 249: Generator Loss = 66.2665, Discriminator Loss = 2.4836\n",
      "Epoch 250: Generator Loss = 80.5924, Discriminator Loss = 0.8755\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.5239 Acc: 0.7318\n",
      "Epoch 2/15 | Train Loss: 0.4125 Acc: 0.7786\n",
      "Epoch 3/15 | Train Loss: 0.3666 Acc: 0.8073\n",
      "Epoch 4/15 | Train Loss: 0.3583 Acc: 0.7891\n",
      "Epoch 5/15 | Train Loss: 0.3361 Acc: 0.8255\n",
      "Epoch 6/15 | Train Loss: 0.3700 Acc: 0.7917\n",
      "Epoch 7/15 | Train Loss: 0.3112 Acc: 0.8438\n",
      "Epoch 8/15 | Train Loss: 0.3028 Acc: 0.8672\n",
      "Epoch 9/15 | Train Loss: 0.2780 Acc: 0.8672\n",
      "Epoch 10/15 | Train Loss: 0.2788 Acc: 0.8672\n",
      "Epoch 11/15 | Train Loss: 0.2745 Acc: 0.8724\n",
      "Epoch 12/15 | Train Loss: 0.2554 Acc: 0.8906\n",
      "Epoch 13/15 | Train Loss: 0.2974 Acc: 0.8620\n",
      "Epoch 14/15 | Train Loss: 0.2682 Acc: 0.8698\n",
      "Epoch 15/15 | Train Loss: 0.2535 Acc: 0.8906\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5477 Acc: 0.7656\n",
      "Epoch 2/15 | Train Loss: 0.4501 Acc: 0.7578\n",
      "Epoch 3/15 | Train Loss: 0.4070 Acc: 0.7552\n",
      "Epoch 4/15 | Train Loss: 0.3820 Acc: 0.8099\n",
      "Epoch 5/15 | Train Loss: 0.3499 Acc: 0.8073\n",
      "Epoch 6/15 | Train Loss: 0.3654 Acc: 0.8307\n",
      "Epoch 7/15 | Train Loss: 0.3552 Acc: 0.8177\n",
      "Epoch 8/15 | Train Loss: 0.3268 Acc: 0.8542\n",
      "Epoch 9/15 | Train Loss: 0.3177 Acc: 0.8464\n",
      "Epoch 10/15 | Train Loss: 0.3469 Acc: 0.8151\n",
      "Epoch 11/15 | Train Loss: 0.3232 Acc: 0.8333\n",
      "Epoch 12/15 | Train Loss: 0.3114 Acc: 0.8542\n",
      "Epoch 13/15 | Train Loss: 0.3028 Acc: 0.8464\n",
      "Epoch 14/15 | Train Loss: 0.2850 Acc: 0.8802\n",
      "Epoch 15/15 | Train Loss: 0.2940 Acc: 0.8438\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5186 Acc: 0.7682\n",
      "Epoch 2/15 | Train Loss: 0.4125 Acc: 0.7500\n",
      "Epoch 3/15 | Train Loss: 0.4021 Acc: 0.7552\n",
      "Epoch 4/15 | Train Loss: 0.3733 Acc: 0.8021\n",
      "Epoch 5/15 | Train Loss: 0.3544 Acc: 0.8151\n",
      "Epoch 6/15 | Train Loss: 0.3476 Acc: 0.7969\n",
      "Epoch 7/15 | Train Loss: 0.3487 Acc: 0.7943\n",
      "Epoch 8/15 | Train Loss: 0.3179 Acc: 0.8307\n",
      "Epoch 9/15 | Train Loss: 0.3405 Acc: 0.8255\n",
      "Epoch 10/15 | Train Loss: 0.2964 Acc: 0.8516\n",
      "Epoch 11/15 | Train Loss: 0.3164 Acc: 0.8464\n",
      "Epoch 12/15 | Train Loss: 0.3119 Acc: 0.8568\n",
      "Epoch 13/15 | Train Loss: 0.3100 Acc: 0.8464\n",
      "Epoch 14/15 | Train Loss: 0.2951 Acc: 0.8698\n",
      "Epoch 15/15 | Train Loss: 0.2777 Acc: 0.8724\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5088 Acc: 0.7651\n",
      "Epoch 2/15 | Train Loss: 0.4387 Acc: 0.8381\n",
      "Epoch 3/15 | Train Loss: 0.4193 Acc: 0.8159\n",
      "Epoch 4/15 | Train Loss: 0.3691 Acc: 0.8508\n",
      "Epoch 5/15 | Train Loss: 0.3724 Acc: 0.8317\n",
      "Epoch 6/15 | Train Loss: 0.3206 Acc: 0.8540\n",
      "Epoch 7/15 | Train Loss: 0.3745 Acc: 0.8603\n",
      "Epoch 8/15 | Train Loss: 0.3544 Acc: 0.8349\n",
      "Epoch 9/15 | Train Loss: 0.3172 Acc: 0.8635\n",
      "Epoch 10/15 | Train Loss: 0.3221 Acc: 0.8603\n",
      "Epoch 11/15 | Train Loss: 0.3297 Acc: 0.8444\n",
      "Epoch 12/15 | Train Loss: 0.3205 Acc: 0.8730\n",
      "Epoch 13/15 | Train Loss: 0.3133 Acc: 0.8540\n",
      "Epoch 14/15 | Train Loss: 0.3353 Acc: 0.8317\n",
      "Epoch 15/15 | Train Loss: 0.3376 Acc: 0.8540\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4322 Acc: 0.8327\n",
      "Epoch 2/15 | Train Loss: 0.4114 Acc: 0.8648\n",
      "Epoch 3/15 | Train Loss: 0.3680 Acc: 0.8327\n",
      "Epoch 4/15 | Train Loss: 0.3748 Acc: 0.8612\n",
      "Epoch 5/15 | Train Loss: 0.2925 Acc: 0.8754\n",
      "Epoch 6/15 | Train Loss: 0.3099 Acc: 0.8648\n",
      "Epoch 7/15 | Train Loss: 0.2948 Acc: 0.8968\n",
      "Epoch 8/15 | Train Loss: 0.2689 Acc: 0.8932\n",
      "Epoch 9/15 | Train Loss: 0.2297 Acc: 0.9181\n",
      "Epoch 10/15 | Train Loss: 0.2319 Acc: 0.9110\n",
      "Epoch 11/15 | Train Loss: 0.2341 Acc: 0.9075\n",
      "Epoch 12/15 | Train Loss: 0.2240 Acc: 0.9110\n",
      "Epoch 13/15 | Train Loss: 0.2144 Acc: 0.9253\n",
      "Epoch 14/15 | Train Loss: 0.2125 Acc: 0.8968\n",
      "Epoch 15/15 | Train Loss: 0.1967 Acc: 0.9146\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7042 Acc: 0.5968\n",
      "Epoch 2/15 | Train Loss: 0.5940 Acc: 0.6759\n",
      "Epoch 3/15 | Train Loss: 0.5316 Acc: 0.6996\n",
      "Epoch 4/15 | Train Loss: 0.5187 Acc: 0.7273\n",
      "Epoch 5/15 | Train Loss: 0.5383 Acc: 0.7194\n",
      "Epoch 6/15 | Train Loss: 0.4868 Acc: 0.7747\n",
      "Epoch 7/15 | Train Loss: 0.4550 Acc: 0.7628\n",
      "Epoch 8/15 | Train Loss: 0.4712 Acc: 0.7866\n",
      "Epoch 9/15 | Train Loss: 0.4397 Acc: 0.7945\n",
      "Epoch 10/15 | Train Loss: 0.3904 Acc: 0.8063\n",
      "Epoch 11/15 | Train Loss: 0.3902 Acc: 0.8261\n",
      "Epoch 12/15 | Train Loss: 0.3717 Acc: 0.8261\n",
      "Epoch 13/15 | Train Loss: 0.3905 Acc: 0.8221\n",
      "Epoch 14/15 | Train Loss: 0.3305 Acc: 0.8775\n",
      "Epoch 15/15 | Train Loss: 0.4028 Acc: 0.7945\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7215 Acc: 0.6482\n",
      "Epoch 2/15 | Train Loss: 0.6100 Acc: 0.6759\n",
      "Epoch 3/15 | Train Loss: 0.5761 Acc: 0.6759\n",
      "Epoch 4/15 | Train Loss: 0.4836 Acc: 0.7628\n",
      "Epoch 5/15 | Train Loss: 0.5559 Acc: 0.6996\n",
      "Epoch 6/15 | Train Loss: 0.5275 Acc: 0.7115\n",
      "Epoch 7/15 | Train Loss: 0.4735 Acc: 0.7628\n",
      "Epoch 8/15 | Train Loss: 0.4601 Acc: 0.7787\n",
      "Epoch 9/15 | Train Loss: 0.4805 Acc: 0.7391\n",
      "Epoch 10/15 | Train Loss: 0.4653 Acc: 0.7510\n",
      "Epoch 11/15 | Train Loss: 0.4678 Acc: 0.7510\n",
      "Epoch 12/15 | Train Loss: 0.4524 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.4536 Acc: 0.7668\n",
      "Epoch 14/15 | Train Loss: 0.4173 Acc: 0.8024\n",
      "Epoch 15/15 | Train Loss: 0.4134 Acc: 0.7826\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6856 Acc: 0.6561\n",
      "Epoch 2/15 | Train Loss: 0.6773 Acc: 0.6245\n",
      "Epoch 3/15 | Train Loss: 0.5814 Acc: 0.6996\n",
      "Epoch 4/15 | Train Loss: 0.5923 Acc: 0.6917\n",
      "Epoch 5/15 | Train Loss: 0.5045 Acc: 0.7431\n",
      "Epoch 6/15 | Train Loss: 0.4675 Acc: 0.7787\n",
      "Epoch 7/15 | Train Loss: 0.4709 Acc: 0.7708\n",
      "Epoch 8/15 | Train Loss: 0.5064 Acc: 0.7668\n",
      "Epoch 9/15 | Train Loss: 0.4322 Acc: 0.7747\n",
      "Epoch 10/15 | Train Loss: 0.4671 Acc: 0.7747\n",
      "Epoch 11/15 | Train Loss: 0.4426 Acc: 0.7589\n",
      "Epoch 12/15 | Train Loss: 0.4309 Acc: 0.7787\n",
      "Epoch 13/15 | Train Loss: 0.4131 Acc: 0.8142\n",
      "Epoch 14/15 | Train Loss: 0.4517 Acc: 0.7708\n",
      "Epoch 15/15 | Train Loss: 0.4342 Acc: 0.7708\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6172 Acc: 0.7426\n",
      "Epoch 2/15 | Train Loss: 0.5671 Acc: 0.7553\n",
      "Epoch 3/15 | Train Loss: 0.5495 Acc: 0.7300\n",
      "Epoch 4/15 | Train Loss: 0.5179 Acc: 0.7637\n",
      "Epoch 5/15 | Train Loss: 0.4930 Acc: 0.8101\n",
      "Epoch 6/15 | Train Loss: 0.4767 Acc: 0.7932\n",
      "Epoch 7/15 | Train Loss: 0.4771 Acc: 0.7595\n",
      "Epoch 8/15 | Train Loss: 0.4547 Acc: 0.8101\n",
      "Epoch 9/15 | Train Loss: 0.4036 Acc: 0.8186\n",
      "Epoch 10/15 | Train Loss: 0.4222 Acc: 0.8101\n",
      "Epoch 11/15 | Train Loss: 0.4130 Acc: 0.8101\n",
      "Epoch 12/15 | Train Loss: 0.4438 Acc: 0.8017\n",
      "Epoch 13/15 | Train Loss: 0.4118 Acc: 0.8270\n",
      "Epoch 14/15 | Train Loss: 0.3784 Acc: 0.8186\n",
      "Epoch 15/15 | Train Loss: 0.3854 Acc: 0.8354\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 1.0046 Acc: 0.5044\n",
      "Epoch 2/15 | Train Loss: 0.4917 Acc: 0.7851\n",
      "Epoch 3/15 | Train Loss: 0.4351 Acc: 0.7982\n",
      "Epoch 4/15 | Train Loss: 0.3979 Acc: 0.8421\n",
      "Epoch 5/15 | Train Loss: 0.3418 Acc: 0.8596\n",
      "Epoch 6/15 | Train Loss: 0.3358 Acc: 0.8640\n",
      "Epoch 7/15 | Train Loss: 0.3640 Acc: 0.8377\n",
      "Epoch 8/15 | Train Loss: 0.3510 Acc: 0.8728\n",
      "Epoch 9/15 | Train Loss: 0.3014 Acc: 0.8904\n",
      "Epoch 10/15 | Train Loss: 0.2877 Acc: 0.9035\n",
      "Epoch 11/15 | Train Loss: 0.2935 Acc: 0.9123\n",
      "Epoch 12/15 | Train Loss: 0.2848 Acc: 0.8728\n",
      "Epoch 13/15 | Train Loss: 0.2466 Acc: 0.9035\n",
      "Epoch 14/15 | Train Loss: 0.2909 Acc: 0.8816\n",
      "Epoch 15/15 | Train Loss: 0.2685 Acc: 0.8947\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.8077 Acc: 0.6869\n",
      "Epoch 2/15 | Train Loss: 0.5500 Acc: 0.7071\n",
      "Epoch 3/15 | Train Loss: 0.4569 Acc: 0.7475\n",
      "Epoch 4/15 | Train Loss: 0.4827 Acc: 0.7306\n",
      "Epoch 5/15 | Train Loss: 0.4180 Acc: 0.7778\n",
      "Epoch 6/15 | Train Loss: 0.4321 Acc: 0.7811\n",
      "Epoch 7/15 | Train Loss: 0.3795 Acc: 0.8350\n",
      "Epoch 8/15 | Train Loss: 0.4478 Acc: 0.7710\n",
      "Epoch 9/15 | Train Loss: 0.3713 Acc: 0.8148\n",
      "Epoch 10/15 | Train Loss: 0.3467 Acc: 0.8350\n",
      "Epoch 11/15 | Train Loss: 0.3556 Acc: 0.8316\n",
      "Epoch 12/15 | Train Loss: 0.3603 Acc: 0.8148\n",
      "Epoch 13/15 | Train Loss: 0.3016 Acc: 0.8788\n",
      "Epoch 14/15 | Train Loss: 0.3063 Acc: 0.8653\n",
      "Epoch 15/15 | Train Loss: 0.3318 Acc: 0.8552\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6138 Acc: 0.6936\n",
      "Epoch 2/15 | Train Loss: 0.5110 Acc: 0.7407\n",
      "Epoch 3/15 | Train Loss: 0.4360 Acc: 0.7811\n",
      "Epoch 4/15 | Train Loss: 0.4703 Acc: 0.7609\n",
      "Epoch 5/15 | Train Loss: 0.4686 Acc: 0.7542\n",
      "Epoch 6/15 | Train Loss: 0.4961 Acc: 0.7071\n",
      "Epoch 7/15 | Train Loss: 0.4307 Acc: 0.7643\n",
      "Epoch 8/15 | Train Loss: 0.4064 Acc: 0.8013\n",
      "Epoch 9/15 | Train Loss: 0.4278 Acc: 0.7778\n",
      "Epoch 10/15 | Train Loss: 0.3729 Acc: 0.8418\n",
      "Epoch 11/15 | Train Loss: 0.3707 Acc: 0.8114\n",
      "Epoch 12/15 | Train Loss: 0.3709 Acc: 0.8350\n",
      "Epoch 13/15 | Train Loss: 0.3651 Acc: 0.8182\n",
      "Epoch 14/15 | Train Loss: 0.3920 Acc: 0.8013\n",
      "Epoch 15/15 | Train Loss: 0.3878 Acc: 0.8283\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6165 Acc: 0.6498\n",
      "Epoch 2/15 | Train Loss: 0.5300 Acc: 0.7104\n",
      "Epoch 3/15 | Train Loss: 0.4776 Acc: 0.7374\n",
      "Epoch 4/15 | Train Loss: 0.4478 Acc: 0.7744\n",
      "Epoch 5/15 | Train Loss: 0.4656 Acc: 0.7677\n",
      "Epoch 6/15 | Train Loss: 0.4577 Acc: 0.7576\n",
      "Epoch 7/15 | Train Loss: 0.4512 Acc: 0.7643\n",
      "Epoch 8/15 | Train Loss: 0.3891 Acc: 0.8148\n",
      "Epoch 9/15 | Train Loss: 0.3935 Acc: 0.7980\n",
      "Epoch 10/15 | Train Loss: 0.3690 Acc: 0.8148\n",
      "Epoch 11/15 | Train Loss: 0.4001 Acc: 0.7912\n",
      "Epoch 12/15 | Train Loss: 0.3701 Acc: 0.8114\n",
      "Epoch 13/15 | Train Loss: 0.3900 Acc: 0.7946\n",
      "Epoch 14/15 | Train Loss: 0.3601 Acc: 0.8215\n",
      "Epoch 15/15 | Train Loss: 0.3856 Acc: 0.7912\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6257 Acc: 0.7452\n",
      "Epoch 2/15 | Train Loss: 0.4898 Acc: 0.7757\n",
      "Epoch 3/15 | Train Loss: 0.4715 Acc: 0.7833\n",
      "Epoch 4/15 | Train Loss: 0.4962 Acc: 0.7490\n",
      "Epoch 5/15 | Train Loss: 0.4535 Acc: 0.8137\n",
      "Epoch 6/15 | Train Loss: 0.4159 Acc: 0.8175\n",
      "Epoch 7/15 | Train Loss: 0.4141 Acc: 0.8061\n",
      "Epoch 8/15 | Train Loss: 0.3982 Acc: 0.8099\n",
      "Epoch 9/15 | Train Loss: 0.3745 Acc: 0.8175\n",
      "Epoch 10/15 | Train Loss: 0.3809 Acc: 0.8327\n",
      "Epoch 11/15 | Train Loss: 0.3589 Acc: 0.8327\n",
      "Epoch 12/15 | Train Loss: 0.3742 Acc: 0.8403\n",
      "Epoch 13/15 | Train Loss: 0.4162 Acc: 0.7757\n",
      "Epoch 14/15 | Train Loss: 0.3774 Acc: 0.8175\n",
      "Epoch 15/15 | Train Loss: 0.3935 Acc: 0.8327\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5344 Acc: 0.7683\n",
      "Epoch 2/15 | Train Loss: 0.4103 Acc: 0.8537\n",
      "Epoch 3/15 | Train Loss: 0.4218 Acc: 0.8415\n",
      "Epoch 4/15 | Train Loss: 0.4141 Acc: 0.8333\n",
      "Epoch 5/15 | Train Loss: 0.4281 Acc: 0.8211\n",
      "Epoch 6/15 | Train Loss: 0.3859 Acc: 0.8537\n",
      "Epoch 7/15 | Train Loss: 0.3060 Acc: 0.8821\n",
      "Epoch 8/15 | Train Loss: 0.3284 Acc: 0.8943\n",
      "Epoch 9/15 | Train Loss: 0.2850 Acc: 0.8902\n",
      "Epoch 10/15 | Train Loss: 0.3102 Acc: 0.8902\n",
      "Epoch 11/15 | Train Loss: 0.3156 Acc: 0.8821\n",
      "Epoch 12/15 | Train Loss: 0.2901 Acc: 0.8943\n",
      "Epoch 13/15 | Train Loss: 0.2907 Acc: 0.9024\n",
      "Epoch 14/15 | Train Loss: 0.2884 Acc: 0.8943\n",
      "Epoch 15/15 | Train Loss: 0.2972 Acc: 0.9024\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7056 Acc: 0.5731\n",
      "Epoch 2/15 | Train Loss: 0.5882 Acc: 0.7115\n",
      "Epoch 3/15 | Train Loss: 0.5289 Acc: 0.7115\n",
      "Epoch 4/15 | Train Loss: 0.5305 Acc: 0.7075\n",
      "Epoch 5/15 | Train Loss: 0.4491 Acc: 0.7866\n",
      "Epoch 6/15 | Train Loss: 0.4619 Acc: 0.7668\n",
      "Epoch 7/15 | Train Loss: 0.4232 Acc: 0.8182\n",
      "Epoch 8/15 | Train Loss: 0.4730 Acc: 0.7668\n",
      "Epoch 9/15 | Train Loss: 0.4195 Acc: 0.8024\n",
      "Epoch 10/15 | Train Loss: 0.4038 Acc: 0.7984\n",
      "Epoch 11/15 | Train Loss: 0.3705 Acc: 0.8182\n",
      "Epoch 12/15 | Train Loss: 0.3834 Acc: 0.8261\n",
      "Epoch 13/15 | Train Loss: 0.3703 Acc: 0.8340\n",
      "Epoch 14/15 | Train Loss: 0.3867 Acc: 0.8063\n",
      "Epoch 15/15 | Train Loss: 0.3776 Acc: 0.8577\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7354 Acc: 0.6403\n",
      "Epoch 2/15 | Train Loss: 0.6127 Acc: 0.6917\n",
      "Epoch 3/15 | Train Loss: 0.5308 Acc: 0.7233\n",
      "Epoch 4/15 | Train Loss: 0.5722 Acc: 0.7075\n",
      "Epoch 5/15 | Train Loss: 0.5031 Acc: 0.7233\n",
      "Epoch 6/15 | Train Loss: 0.5255 Acc: 0.7312\n",
      "Epoch 7/15 | Train Loss: 0.4303 Acc: 0.8063\n",
      "Epoch 8/15 | Train Loss: 0.5154 Acc: 0.7549\n",
      "Epoch 9/15 | Train Loss: 0.4391 Acc: 0.7905\n",
      "Epoch 10/15 | Train Loss: 0.4430 Acc: 0.7826\n",
      "Epoch 11/15 | Train Loss: 0.4295 Acc: 0.7708\n",
      "Epoch 12/15 | Train Loss: 0.4663 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.4166 Acc: 0.7787\n",
      "Epoch 14/15 | Train Loss: 0.4640 Acc: 0.7826\n",
      "Epoch 15/15 | Train Loss: 0.4224 Acc: 0.7984\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7249 Acc: 0.6324\n",
      "Epoch 2/15 | Train Loss: 0.6079 Acc: 0.6917\n",
      "Epoch 3/15 | Train Loss: 0.5946 Acc: 0.6798\n",
      "Epoch 4/15 | Train Loss: 0.5389 Acc: 0.7431\n",
      "Epoch 5/15 | Train Loss: 0.5460 Acc: 0.7352\n",
      "Epoch 6/15 | Train Loss: 0.5166 Acc: 0.7391\n",
      "Epoch 7/15 | Train Loss: 0.4416 Acc: 0.7747\n",
      "Epoch 8/15 | Train Loss: 0.4693 Acc: 0.7866\n",
      "Epoch 9/15 | Train Loss: 0.4571 Acc: 0.7628\n",
      "Epoch 10/15 | Train Loss: 0.4254 Acc: 0.7787\n",
      "Epoch 11/15 | Train Loss: 0.4030 Acc: 0.8103\n",
      "Epoch 12/15 | Train Loss: 0.4570 Acc: 0.7668\n",
      "Epoch 13/15 | Train Loss: 0.4429 Acc: 0.7668\n",
      "Epoch 14/15 | Train Loss: 0.4085 Acc: 0.8024\n",
      "Epoch 15/15 | Train Loss: 0.4012 Acc: 0.7905\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7731 Acc: 0.5696\n",
      "Epoch 2/15 | Train Loss: 0.6096 Acc: 0.7637\n",
      "Epoch 3/15 | Train Loss: 0.5513 Acc: 0.7384\n",
      "Epoch 4/15 | Train Loss: 0.5291 Acc: 0.7553\n",
      "Epoch 5/15 | Train Loss: 0.5746 Acc: 0.7679\n",
      "Epoch 6/15 | Train Loss: 0.4885 Acc: 0.7848\n",
      "Epoch 7/15 | Train Loss: 0.4961 Acc: 0.7975\n",
      "Epoch 8/15 | Train Loss: 0.4740 Acc: 0.7764\n",
      "Epoch 9/15 | Train Loss: 0.4542 Acc: 0.7890\n",
      "Epoch 10/15 | Train Loss: 0.4177 Acc: 0.7932\n",
      "Epoch 11/15 | Train Loss: 0.4352 Acc: 0.7764\n",
      "Epoch 12/15 | Train Loss: 0.4307 Acc: 0.7848\n",
      "Epoch 13/15 | Train Loss: 0.4737 Acc: 0.7975\n",
      "Epoch 14/15 | Train Loss: 0.4391 Acc: 0.7932\n",
      "Epoch 15/15 | Train Loss: 0.4286 Acc: 0.7975\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5536 Acc: 0.7675\n",
      "Epoch 2/15 | Train Loss: 0.4662 Acc: 0.8026\n",
      "Epoch 3/15 | Train Loss: 0.4133 Acc: 0.8333\n",
      "Epoch 4/15 | Train Loss: 0.3874 Acc: 0.8553\n",
      "Epoch 5/15 | Train Loss: 0.3746 Acc: 0.8509\n",
      "Epoch 6/15 | Train Loss: 0.4183 Acc: 0.8333\n",
      "Epoch 7/15 | Train Loss: 0.3621 Acc: 0.8596\n",
      "Epoch 8/15 | Train Loss: 0.3461 Acc: 0.8596\n",
      "Epoch 9/15 | Train Loss: 0.3092 Acc: 0.8904\n",
      "Epoch 10/15 | Train Loss: 0.3471 Acc: 0.8728\n",
      "Epoch 11/15 | Train Loss: 0.2945 Acc: 0.8816\n",
      "Epoch 12/15 | Train Loss: 0.2959 Acc: 0.8904\n",
      "Epoch 13/15 | Train Loss: 0.2789 Acc: 0.8772\n",
      "Epoch 14/15 | Train Loss: 0.3135 Acc: 0.8772\n",
      "Epoch 15/15 | Train Loss: 0.2752 Acc: 0.8947\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5033 Acc: 0.7708\n",
      "Epoch 2/15 | Train Loss: 0.4337 Acc: 0.7708\n",
      "Epoch 3/15 | Train Loss: 0.3754 Acc: 0.7760\n",
      "Epoch 4/15 | Train Loss: 0.3594 Acc: 0.8047\n",
      "Epoch 5/15 | Train Loss: 0.3498 Acc: 0.8073\n",
      "Epoch 6/15 | Train Loss: 0.3079 Acc: 0.8542\n",
      "Epoch 7/15 | Train Loss: 0.3393 Acc: 0.8229\n",
      "Epoch 8/15 | Train Loss: 0.2830 Acc: 0.8828\n",
      "Epoch 9/15 | Train Loss: 0.2705 Acc: 0.8698\n",
      "Epoch 10/15 | Train Loss: 0.2787 Acc: 0.8620\n",
      "Epoch 11/15 | Train Loss: 0.2593 Acc: 0.8750\n",
      "Epoch 12/15 | Train Loss: 0.2478 Acc: 0.8802\n",
      "Epoch 13/15 | Train Loss: 0.2468 Acc: 0.8776\n",
      "Epoch 14/15 | Train Loss: 0.2859 Acc: 0.8516\n",
      "Epoch 15/15 | Train Loss: 0.2534 Acc: 0.8854\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5702 Acc: 0.6927\n",
      "Epoch 2/15 | Train Loss: 0.4314 Acc: 0.7526\n",
      "Epoch 3/15 | Train Loss: 0.3650 Acc: 0.7995\n",
      "Epoch 4/15 | Train Loss: 0.3959 Acc: 0.7656\n",
      "Epoch 5/15 | Train Loss: 0.4022 Acc: 0.7656\n",
      "Epoch 6/15 | Train Loss: 0.3474 Acc: 0.8177\n",
      "Epoch 7/15 | Train Loss: 0.3242 Acc: 0.8125\n",
      "Epoch 8/15 | Train Loss: 0.3166 Acc: 0.8281\n",
      "Epoch 9/15 | Train Loss: 0.3072 Acc: 0.8542\n",
      "Epoch 10/15 | Train Loss: 0.3230 Acc: 0.8359\n",
      "Epoch 11/15 | Train Loss: 0.3048 Acc: 0.8464\n",
      "Epoch 12/15 | Train Loss: 0.2964 Acc: 0.8750\n",
      "Epoch 13/15 | Train Loss: 0.3240 Acc: 0.8255\n",
      "Epoch 14/15 | Train Loss: 0.2926 Acc: 0.8542\n",
      "Epoch 15/15 | Train Loss: 0.2782 Acc: 0.8750\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.7671 Acc: 0.7474\n",
      "Epoch 2/15 | Train Loss: 0.4337 Acc: 0.8021\n",
      "Epoch 3/15 | Train Loss: 0.4108 Acc: 0.7839\n",
      "Epoch 4/15 | Train Loss: 0.3788 Acc: 0.8073\n",
      "Epoch 5/15 | Train Loss: 0.3426 Acc: 0.8359\n",
      "Epoch 6/15 | Train Loss: 0.3973 Acc: 0.7630\n",
      "Epoch 7/15 | Train Loss: 0.3564 Acc: 0.8047\n",
      "Epoch 8/15 | Train Loss: 0.3333 Acc: 0.8490\n",
      "Epoch 9/15 | Train Loss: 0.3303 Acc: 0.8333\n",
      "Epoch 10/15 | Train Loss: 0.3105 Acc: 0.8516\n",
      "Epoch 11/15 | Train Loss: 0.2778 Acc: 0.8750\n",
      "Epoch 12/15 | Train Loss: 0.2800 Acc: 0.8750\n",
      "Epoch 13/15 | Train Loss: 0.2878 Acc: 0.8672\n",
      "Epoch 14/15 | Train Loss: 0.2841 Acc: 0.8516\n",
      "Epoch 15/15 | Train Loss: 0.3167 Acc: 0.8438\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5492 Acc: 0.8127\n",
      "Epoch 2/15 | Train Loss: 0.4342 Acc: 0.8127\n",
      "Epoch 3/15 | Train Loss: 0.4262 Acc: 0.7905\n",
      "Epoch 4/15 | Train Loss: 0.3743 Acc: 0.8286\n",
      "Epoch 5/15 | Train Loss: 0.3598 Acc: 0.8413\n",
      "Epoch 6/15 | Train Loss: 0.3851 Acc: 0.8381\n",
      "Epoch 7/15 | Train Loss: 0.3405 Acc: 0.8540\n",
      "Epoch 8/15 | Train Loss: 0.3575 Acc: 0.8444\n",
      "Epoch 9/15 | Train Loss: 0.3483 Acc: 0.8413\n",
      "Epoch 10/15 | Train Loss: 0.3387 Acc: 0.8381\n",
      "Epoch 11/15 | Train Loss: 0.3152 Acc: 0.8413\n",
      "Epoch 12/15 | Train Loss: 0.3076 Acc: 0.8698\n",
      "Epoch 13/15 | Train Loss: 0.3111 Acc: 0.8508\n",
      "Epoch 14/15 | Train Loss: 0.3241 Acc: 0.8508\n",
      "Epoch 15/15 | Train Loss: 0.3229 Acc: 0.8635\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4838 Acc: 0.8434\n",
      "Epoch 2/15 | Train Loss: 0.3852 Acc: 0.8505\n",
      "Epoch 3/15 | Train Loss: 0.3612 Acc: 0.8754\n",
      "Epoch 4/15 | Train Loss: 0.3236 Acc: 0.8754\n",
      "Epoch 5/15 | Train Loss: 0.3404 Acc: 0.8826\n",
      "Epoch 6/15 | Train Loss: 0.2917 Acc: 0.8968\n",
      "Epoch 7/15 | Train Loss: 0.2873 Acc: 0.8719\n",
      "Epoch 8/15 | Train Loss: 0.2893 Acc: 0.9075\n",
      "Epoch 9/15 | Train Loss: 0.2790 Acc: 0.9039\n",
      "Epoch 10/15 | Train Loss: 0.2599 Acc: 0.9146\n",
      "Epoch 11/15 | Train Loss: 0.2745 Acc: 0.8968\n",
      "Epoch 12/15 | Train Loss: 0.2472 Acc: 0.9004\n",
      "Epoch 13/15 | Train Loss: 0.2398 Acc: 0.9146\n",
      "Epoch 14/15 | Train Loss: 0.2316 Acc: 0.9075\n",
      "Epoch 15/15 | Train Loss: 0.2289 Acc: 0.9004\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7700 Acc: 0.6403\n",
      "Epoch 2/15 | Train Loss: 0.6530 Acc: 0.6561\n",
      "Epoch 3/15 | Train Loss: 0.5898 Acc: 0.7075\n",
      "Epoch 4/15 | Train Loss: 0.5341 Acc: 0.7115\n",
      "Epoch 5/15 | Train Loss: 0.5148 Acc: 0.7431\n",
      "Epoch 6/15 | Train Loss: 0.4876 Acc: 0.7668\n",
      "Epoch 7/15 | Train Loss: 0.4670 Acc: 0.7549\n",
      "Epoch 8/15 | Train Loss: 0.3844 Acc: 0.8024\n",
      "Epoch 9/15 | Train Loss: 0.4781 Acc: 0.7787\n",
      "Epoch 10/15 | Train Loss: 0.4155 Acc: 0.7945\n",
      "Epoch 11/15 | Train Loss: 0.4046 Acc: 0.8063\n",
      "Epoch 12/15 | Train Loss: 0.3632 Acc: 0.8221\n",
      "Epoch 13/15 | Train Loss: 0.3694 Acc: 0.8261\n",
      "Epoch 14/15 | Train Loss: 0.3768 Acc: 0.8142\n",
      "Epoch 15/15 | Train Loss: 0.3797 Acc: 0.8261\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7366 Acc: 0.5692\n",
      "Epoch 2/15 | Train Loss: 0.6051 Acc: 0.6561\n",
      "Epoch 3/15 | Train Loss: 0.5767 Acc: 0.6640\n",
      "Epoch 4/15 | Train Loss: 0.5410 Acc: 0.7115\n",
      "Epoch 5/15 | Train Loss: 0.5360 Acc: 0.6877\n",
      "Epoch 6/15 | Train Loss: 0.5223 Acc: 0.7154\n",
      "Epoch 7/15 | Train Loss: 0.4921 Acc: 0.7391\n",
      "Epoch 8/15 | Train Loss: 0.4644 Acc: 0.7510\n",
      "Epoch 9/15 | Train Loss: 0.3917 Acc: 0.8142\n",
      "Epoch 10/15 | Train Loss: 0.5083 Acc: 0.7470\n",
      "Epoch 11/15 | Train Loss: 0.4554 Acc: 0.7787\n",
      "Epoch 12/15 | Train Loss: 0.4286 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.4146 Acc: 0.7945\n",
      "Epoch 14/15 | Train Loss: 0.3895 Acc: 0.8300\n",
      "Epoch 15/15 | Train Loss: 0.3964 Acc: 0.8142\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7519 Acc: 0.6245\n",
      "Epoch 2/15 | Train Loss: 0.6509 Acc: 0.6364\n",
      "Epoch 3/15 | Train Loss: 0.6427 Acc: 0.6364\n",
      "Epoch 4/15 | Train Loss: 0.5843 Acc: 0.6759\n",
      "Epoch 5/15 | Train Loss: 0.5495 Acc: 0.6680\n",
      "Epoch 6/15 | Train Loss: 0.5104 Acc: 0.7352\n",
      "Epoch 7/15 | Train Loss: 0.5243 Acc: 0.6877\n",
      "Epoch 8/15 | Train Loss: 0.4809 Acc: 0.7708\n",
      "Epoch 9/15 | Train Loss: 0.4342 Acc: 0.7787\n",
      "Epoch 10/15 | Train Loss: 0.4350 Acc: 0.7945\n",
      "Epoch 11/15 | Train Loss: 0.4775 Acc: 0.7391\n",
      "Epoch 12/15 | Train Loss: 0.4420 Acc: 0.7984\n",
      "Epoch 13/15 | Train Loss: 0.4221 Acc: 0.8024\n",
      "Epoch 14/15 | Train Loss: 0.4370 Acc: 0.7787\n",
      "Epoch 15/15 | Train Loss: 0.4323 Acc: 0.7984\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6457 Acc: 0.6709\n",
      "Epoch 2/15 | Train Loss: 0.5371 Acc: 0.7637\n",
      "Epoch 3/15 | Train Loss: 0.5486 Acc: 0.7679\n",
      "Epoch 4/15 | Train Loss: 0.5004 Acc: 0.7637\n",
      "Epoch 5/15 | Train Loss: 0.4665 Acc: 0.8101\n",
      "Epoch 6/15 | Train Loss: 0.5193 Acc: 0.7511\n",
      "Epoch 7/15 | Train Loss: 0.4543 Acc: 0.8017\n",
      "Epoch 8/15 | Train Loss: 0.4896 Acc: 0.7890\n",
      "Epoch 9/15 | Train Loss: 0.4285 Acc: 0.8143\n",
      "Epoch 10/15 | Train Loss: 0.4562 Acc: 0.8101\n",
      "Epoch 11/15 | Train Loss: 0.3853 Acc: 0.8397\n",
      "Epoch 12/15 | Train Loss: 0.4065 Acc: 0.8059\n",
      "Epoch 13/15 | Train Loss: 0.4374 Acc: 0.7975\n",
      "Epoch 14/15 | Train Loss: 0.4441 Acc: 0.7932\n",
      "Epoch 15/15 | Train Loss: 0.4123 Acc: 0.7932\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5218 Acc: 0.7588\n",
      "Epoch 2/15 | Train Loss: 0.4820 Acc: 0.8070\n",
      "Epoch 3/15 | Train Loss: 0.4698 Acc: 0.7851\n",
      "Epoch 4/15 | Train Loss: 0.4117 Acc: 0.8246\n",
      "Epoch 5/15 | Train Loss: 0.4079 Acc: 0.8509\n",
      "Epoch 6/15 | Train Loss: 0.3451 Acc: 0.8640\n",
      "Epoch 7/15 | Train Loss: 0.3688 Acc: 0.8509\n",
      "Epoch 8/15 | Train Loss: 0.3394 Acc: 0.8684\n",
      "Epoch 9/15 | Train Loss: 0.3509 Acc: 0.8640\n",
      "Epoch 10/15 | Train Loss: 0.2813 Acc: 0.8816\n",
      "Epoch 11/15 | Train Loss: 0.3345 Acc: 0.8816\n",
      "Epoch 12/15 | Train Loss: 0.2845 Acc: 0.9035\n",
      "Epoch 13/15 | Train Loss: 0.2901 Acc: 0.8860\n",
      "Epoch 14/15 | Train Loss: 0.2891 Acc: 0.8860\n",
      "Epoch 15/15 | Train Loss: 0.3272 Acc: 0.8816\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.4942 Acc: 0.7643\n",
      "Epoch 2/15 | Train Loss: 0.3972 Acc: 0.7918\n",
      "Epoch 3/15 | Train Loss: 0.3773 Acc: 0.8078\n",
      "Epoch 4/15 | Train Loss: 0.4212 Acc: 0.7780\n",
      "Epoch 5/15 | Train Loss: 0.3586 Acc: 0.8192\n",
      "Epoch 6/15 | Train Loss: 0.3591 Acc: 0.8215\n",
      "Epoch 7/15 | Train Loss: 0.3647 Acc: 0.8261\n",
      "Epoch 8/15 | Train Loss: 0.3168 Acc: 0.8581\n",
      "Epoch 9/15 | Train Loss: 0.3150 Acc: 0.8490\n",
      "Epoch 10/15 | Train Loss: 0.3032 Acc: 0.8558\n",
      "Epoch 11/15 | Train Loss: 0.3145 Acc: 0.8352\n",
      "Epoch 12/15 | Train Loss: 0.3057 Acc: 0.8558\n",
      "Epoch 13/15 | Train Loss: 0.2925 Acc: 0.8787\n",
      "Epoch 14/15 | Train Loss: 0.2979 Acc: 0.8673\n",
      "Epoch 15/15 | Train Loss: 0.2825 Acc: 0.8650\n",
      "Fold 5 Test Accuracy: 0.6020\n",
      "===== Fold 6 =====\n",
      "Epoch 1: Generator Loss = 9.8455, Discriminator Loss = 8.9545\n",
      "Epoch 2: Generator Loss = 11.2567, Discriminator Loss = 7.5655\n",
      "Epoch 3: Generator Loss = 14.0504, Discriminator Loss = 6.6251\n",
      "Epoch 4: Generator Loss = 20.5567, Discriminator Loss = 4.5125\n",
      "Epoch 5: Generator Loss = 28.7053, Discriminator Loss = 3.9463\n",
      "Epoch 6: Generator Loss = 30.8618, Discriminator Loss = 5.7373\n",
      "Epoch 7: Generator Loss = 30.1257, Discriminator Loss = 4.7496\n",
      "Epoch 8: Generator Loss = 31.1952, Discriminator Loss = 6.3207\n",
      "Epoch 9: Generator Loss = 29.5499, Discriminator Loss = 5.6930\n",
      "Epoch 10: Generator Loss = 26.8598, Discriminator Loss = 6.0132\n",
      "Epoch 11: Generator Loss = 30.1101, Discriminator Loss = 5.2516\n",
      "Epoch 12: Generator Loss = 31.7225, Discriminator Loss = 6.9956\n",
      "Epoch 13: Generator Loss = 24.4232, Discriminator Loss = 5.8610\n",
      "Epoch 14: Generator Loss = 27.8197, Discriminator Loss = 8.2275\n",
      "Epoch 15: Generator Loss = 24.2730, Discriminator Loss = 8.3876\n",
      "Epoch 16: Generator Loss = 21.3040, Discriminator Loss = 8.1020\n",
      "Epoch 17: Generator Loss = 21.2473, Discriminator Loss = 6.3763\n",
      "Epoch 18: Generator Loss = 24.5864, Discriminator Loss = 8.1789\n",
      "Epoch 19: Generator Loss = 19.9437, Discriminator Loss = 5.9581\n",
      "Epoch 20: Generator Loss = 27.6077, Discriminator Loss = 5.6368\n",
      "Epoch 21: Generator Loss = 23.0861, Discriminator Loss = 7.2982\n",
      "Epoch 22: Generator Loss = 20.9001, Discriminator Loss = 9.3098\n",
      "Epoch 23: Generator Loss = 16.5484, Discriminator Loss = 7.6783\n",
      "Epoch 24: Generator Loss = 21.4084, Discriminator Loss = 8.1540\n",
      "Epoch 25: Generator Loss = 17.4782, Discriminator Loss = 7.7156\n",
      "Epoch 26: Generator Loss = 18.6638, Discriminator Loss = 7.5910\n",
      "Epoch 27: Generator Loss = 20.1368, Discriminator Loss = 7.6299\n",
      "Epoch 28: Generator Loss = 23.1604, Discriminator Loss = 7.0715\n",
      "Epoch 29: Generator Loss = 19.5818, Discriminator Loss = 7.1534\n",
      "Epoch 30: Generator Loss = 21.6598, Discriminator Loss = 7.6816\n",
      "Epoch 31: Generator Loss = 21.6146, Discriminator Loss = 8.3383\n",
      "Epoch 32: Generator Loss = 15.7393, Discriminator Loss = 8.4411\n",
      "Epoch 33: Generator Loss = 18.3376, Discriminator Loss = 6.6708\n",
      "Epoch 34: Generator Loss = 26.1517, Discriminator Loss = 6.3174\n",
      "Epoch 35: Generator Loss = 19.5321, Discriminator Loss = 8.4061\n",
      "Epoch 36: Generator Loss = 25.3150, Discriminator Loss = 9.1025\n",
      "Epoch 37: Generator Loss = 19.8818, Discriminator Loss = 8.9133\n",
      "Epoch 38: Generator Loss = 16.7583, Discriminator Loss = 8.5302\n",
      "Epoch 39: Generator Loss = 18.6405, Discriminator Loss = 7.6354\n",
      "Epoch 40: Generator Loss = 20.4764, Discriminator Loss = 8.2739\n",
      "Epoch 41: Generator Loss = 16.1264, Discriminator Loss = 8.4520\n",
      "Epoch 42: Generator Loss = 18.2648, Discriminator Loss = 8.2295\n",
      "Epoch 43: Generator Loss = 18.4843, Discriminator Loss = 6.0740\n",
      "Epoch 44: Generator Loss = 19.5121, Discriminator Loss = 9.4476\n",
      "Epoch 45: Generator Loss = 18.1617, Discriminator Loss = 7.6227\n",
      "Epoch 46: Generator Loss = 16.9126, Discriminator Loss = 7.1575\n",
      "Epoch 47: Generator Loss = 21.0166, Discriminator Loss = 5.7955\n",
      "Epoch 48: Generator Loss = 23.3669, Discriminator Loss = 6.5269\n",
      "Epoch 49: Generator Loss = 28.5975, Discriminator Loss = 6.4283\n",
      "Epoch 50: Generator Loss = 22.6312, Discriminator Loss = 6.4881\n",
      "Epoch 51: Generator Loss = 22.0088, Discriminator Loss = 6.1135\n",
      "Epoch 52: Generator Loss = 26.4758, Discriminator Loss = 7.6315\n",
      "Epoch 53: Generator Loss = 25.1107, Discriminator Loss = 5.7783\n",
      "Epoch 54: Generator Loss = 29.9586, Discriminator Loss = 4.9883\n",
      "Epoch 55: Generator Loss = 24.5608, Discriminator Loss = 5.2599\n",
      "Epoch 56: Generator Loss = 27.6671, Discriminator Loss = 4.3396\n",
      "Epoch 57: Generator Loss = 30.0163, Discriminator Loss = 3.7544\n",
      "Epoch 58: Generator Loss = 31.8079, Discriminator Loss = 4.3827\n",
      "Epoch 59: Generator Loss = 35.9633, Discriminator Loss = 3.1114\n",
      "Epoch 60: Generator Loss = 43.7636, Discriminator Loss = 4.1293\n",
      "Epoch 61: Generator Loss = 27.4347, Discriminator Loss = 7.1696\n",
      "Epoch 62: Generator Loss = 30.9029, Discriminator Loss = 4.0725\n",
      "Epoch 63: Generator Loss = 37.7845, Discriminator Loss = 2.8079\n",
      "Epoch 64: Generator Loss = 29.5079, Discriminator Loss = 5.8441\n",
      "Epoch 65: Generator Loss = 37.3134, Discriminator Loss = 2.7292\n",
      "Epoch 66: Generator Loss = 29.9396, Discriminator Loss = 2.7791\n",
      "Epoch 67: Generator Loss = 41.0523, Discriminator Loss = 1.9724\n",
      "Epoch 68: Generator Loss = 40.7103, Discriminator Loss = 2.0198\n",
      "Epoch 69: Generator Loss = 54.2925, Discriminator Loss = 4.9929\n",
      "Epoch 70: Generator Loss = 53.8568, Discriminator Loss = 2.9132\n",
      "Epoch 71: Generator Loss = 36.9659, Discriminator Loss = 6.4267\n",
      "Epoch 72: Generator Loss = 46.6165, Discriminator Loss = 3.3402\n",
      "Epoch 73: Generator Loss = 39.5361, Discriminator Loss = 2.1120\n",
      "Epoch 74: Generator Loss = 58.6511, Discriminator Loss = 1.3665\n",
      "Epoch 75: Generator Loss = 52.1927, Discriminator Loss = 1.1992\n",
      "Epoch 76: Generator Loss = 41.0890, Discriminator Loss = 3.4210\n",
      "Epoch 77: Generator Loss = 46.8892, Discriminator Loss = 4.2663\n",
      "Epoch 78: Generator Loss = 46.0763, Discriminator Loss = 1.7842\n",
      "Epoch 79: Generator Loss = 60.9618, Discriminator Loss = 1.7659\n",
      "Epoch 80: Generator Loss = 49.5310, Discriminator Loss = 1.0163\n",
      "Epoch 81: Generator Loss = 61.2966, Discriminator Loss = 2.0884\n",
      "Epoch 82: Generator Loss = 53.6376, Discriminator Loss = 1.6390\n",
      "Epoch 83: Generator Loss = 57.6285, Discriminator Loss = 1.5326\n",
      "Epoch 84: Generator Loss = 50.9181, Discriminator Loss = 2.5642\n",
      "Epoch 85: Generator Loss = 71.0089, Discriminator Loss = 4.8443\n",
      "Epoch 86: Generator Loss = 54.0196, Discriminator Loss = 1.1552\n",
      "Epoch 87: Generator Loss = 54.6115, Discriminator Loss = 1.4309\n",
      "Epoch 88: Generator Loss = 72.5700, Discriminator Loss = 0.8443\n",
      "Epoch 89: Generator Loss = 70.5795, Discriminator Loss = 0.7622\n",
      "Epoch 90: Generator Loss = 66.2782, Discriminator Loss = 1.0490\n",
      "Epoch 91: Generator Loss = 72.9620, Discriminator Loss = 0.3774\n",
      "Epoch 92: Generator Loss = 82.9696, Discriminator Loss = 0.7023\n",
      "Epoch 93: Generator Loss = 142.3973, Discriminator Loss = 3.2489\n",
      "Epoch 94: Generator Loss = 190.7125, Discriminator Loss = 5.5941\n",
      "Epoch 95: Generator Loss = 177.5531, Discriminator Loss = 0.8168\n",
      "Epoch 96: Generator Loss = 142.0710, Discriminator Loss = 0.1715\n",
      "Epoch 97: Generator Loss = 107.5708, Discriminator Loss = 0.3203\n",
      "Epoch 98: Generator Loss = 96.7976, Discriminator Loss = 8.8716\n",
      "Epoch 99: Generator Loss = 102.7142, Discriminator Loss = 0.9913\n",
      "Epoch 100: Generator Loss = 97.2211, Discriminator Loss = 0.7517\n",
      "Epoch 101: Generator Loss = 86.7472, Discriminator Loss = 0.9504\n",
      "Epoch 102: Generator Loss = 77.7589, Discriminator Loss = 0.4135\n",
      "Epoch 103: Generator Loss = 61.5793, Discriminator Loss = 1.1893\n",
      "Epoch 104: Generator Loss = 73.4704, Discriminator Loss = 1.0513\n",
      "Epoch 105: Generator Loss = 78.5207, Discriminator Loss = 0.9013\n",
      "Epoch 106: Generator Loss = 77.7038, Discriminator Loss = 2.0622\n",
      "Epoch 107: Generator Loss = 72.3750, Discriminator Loss = 4.0645\n",
      "Epoch 108: Generator Loss = 72.3974, Discriminator Loss = 0.9253\n",
      "Epoch 109: Generator Loss = 87.0307, Discriminator Loss = 1.2134\n",
      "Epoch 110: Generator Loss = 81.0115, Discriminator Loss = 0.6684\n",
      "Epoch 111: Generator Loss = 83.2394, Discriminator Loss = 0.6211\n",
      "Epoch 112: Generator Loss = 85.7124, Discriminator Loss = 0.5145\n",
      "Epoch 113: Generator Loss = 74.7360, Discriminator Loss = 1.5134\n",
      "Epoch 114: Generator Loss = 91.0753, Discriminator Loss = 0.6799\n",
      "Epoch 115: Generator Loss = 72.4751, Discriminator Loss = 0.6005\n",
      "Epoch 116: Generator Loss = 78.6330, Discriminator Loss = 0.5839\n",
      "Epoch 117: Generator Loss = 70.6636, Discriminator Loss = 0.4217\n",
      "Epoch 118: Generator Loss = 71.0112, Discriminator Loss = 0.9007\n",
      "Epoch 119: Generator Loss = 74.6050, Discriminator Loss = 0.7867\n",
      "Epoch 120: Generator Loss = 79.2145, Discriminator Loss = 0.8614\n",
      "Epoch 121: Generator Loss = 75.0487, Discriminator Loss = 0.6344\n",
      "Epoch 122: Generator Loss = 83.9731, Discriminator Loss = 0.5313\n",
      "Epoch 123: Generator Loss = 80.2972, Discriminator Loss = 0.9039\n",
      "Epoch 124: Generator Loss = 77.6327, Discriminator Loss = 0.7466\n",
      "Epoch 125: Generator Loss = 72.0655, Discriminator Loss = 0.4150\n",
      "Epoch 126: Generator Loss = 80.8775, Discriminator Loss = 0.5477\n",
      "Epoch 127: Generator Loss = 65.9487, Discriminator Loss = 0.7995\n",
      "Epoch 128: Generator Loss = 65.8525, Discriminator Loss = 0.5891\n",
      "Epoch 129: Generator Loss = 79.0921, Discriminator Loss = 1.3378\n",
      "Epoch 130: Generator Loss = 85.6396, Discriminator Loss = 0.6933\n",
      "Epoch 131: Generator Loss = 82.7136, Discriminator Loss = 0.6037\n",
      "Epoch 132: Generator Loss = 67.8355, Discriminator Loss = 1.0187\n",
      "Epoch 133: Generator Loss = 75.8080, Discriminator Loss = 0.9588\n",
      "Epoch 134: Generator Loss = 86.5815, Discriminator Loss = 0.6577\n",
      "Epoch 135: Generator Loss = 90.9648, Discriminator Loss = 0.5264\n",
      "Epoch 136: Generator Loss = 75.0651, Discriminator Loss = 0.6968\n",
      "Epoch 137: Generator Loss = 82.3171, Discriminator Loss = 2.9587\n",
      "Epoch 138: Generator Loss = 69.4870, Discriminator Loss = 1.1243\n",
      "Epoch 139: Generator Loss = 68.6609, Discriminator Loss = 0.8052\n",
      "Epoch 140: Generator Loss = 74.4960, Discriminator Loss = 0.5511\n",
      "Epoch 141: Generator Loss = 80.7201, Discriminator Loss = 0.9031\n",
      "Epoch 142: Generator Loss = 72.3668, Discriminator Loss = 0.7093\n",
      "Epoch 143: Generator Loss = 73.2553, Discriminator Loss = 0.7214\n",
      "Epoch 144: Generator Loss = 80.1702, Discriminator Loss = 0.7210\n",
      "Epoch 145: Generator Loss = 79.0951, Discriminator Loss = 1.1281\n",
      "Epoch 146: Generator Loss = 108.3090, Discriminator Loss = 0.4124\n",
      "Epoch 147: Generator Loss = 129.2357, Discriminator Loss = 0.3846\n",
      "Epoch 148: Generator Loss = 117.0036, Discriminator Loss = 0.3432\n",
      "Epoch 149: Generator Loss = 134.7312, Discriminator Loss = 1.1656\n",
      "Epoch 150: Generator Loss = 96.3479, Discriminator Loss = 0.2010\n",
      "Epoch 151: Generator Loss = 112.6535, Discriminator Loss = 0.2990\n",
      "Epoch 152: Generator Loss = 90.5342, Discriminator Loss = 0.1517\n",
      "Epoch 153: Generator Loss = 122.8832, Discriminator Loss = 0.4634\n",
      "Epoch 154: Generator Loss = 100.7691, Discriminator Loss = 0.1230\n",
      "Epoch 155: Generator Loss = 98.6875, Discriminator Loss = 0.0761\n",
      "Epoch 156: Generator Loss = 89.5046, Discriminator Loss = 0.0979\n",
      "Epoch 157: Generator Loss = 90.9564, Discriminator Loss = 0.1684\n",
      "Epoch 158: Generator Loss = 82.9310, Discriminator Loss = 0.1170\n",
      "Epoch 159: Generator Loss = 79.5146, Discriminator Loss = 0.1907\n",
      "Epoch 160: Generator Loss = 90.2851, Discriminator Loss = 0.0668\n",
      "Epoch 161: Generator Loss = 111.7896, Discriminator Loss = 0.0562\n",
      "Epoch 162: Generator Loss = 85.1829, Discriminator Loss = 0.1341\n",
      "Epoch 163: Generator Loss = 102.9524, Discriminator Loss = 0.0350\n",
      "Epoch 164: Generator Loss = 103.4054, Discriminator Loss = 0.0279\n",
      "Epoch 165: Generator Loss = 92.6340, Discriminator Loss = 0.0838\n",
      "Epoch 166: Generator Loss = 84.1590, Discriminator Loss = 0.1253\n",
      "Epoch 167: Generator Loss = 104.7107, Discriminator Loss = 0.0775\n",
      "Epoch 168: Generator Loss = 100.6213, Discriminator Loss = 0.0806\n",
      "Epoch 169: Generator Loss = 104.8567, Discriminator Loss = 0.0432\n",
      "Epoch 170: Generator Loss = 102.6792, Discriminator Loss = 0.0529\n",
      "Epoch 171: Generator Loss = 104.2935, Discriminator Loss = 0.4267\n",
      "Epoch 172: Generator Loss = 88.9888, Discriminator Loss = 0.6209\n",
      "Epoch 173: Generator Loss = 96.4805, Discriminator Loss = 20.1451\n",
      "Epoch 174: Generator Loss = 61.9755, Discriminator Loss = 8.2224\n",
      "Epoch 175: Generator Loss = 61.0682, Discriminator Loss = 0.8875\n",
      "Epoch 176: Generator Loss = 81.0878, Discriminator Loss = 1.1620\n",
      "Epoch 177: Generator Loss = 73.9183, Discriminator Loss = 0.2941\n",
      "Epoch 178: Generator Loss = 82.3713, Discriminator Loss = 0.3077\n",
      "Epoch 179: Generator Loss = 82.3222, Discriminator Loss = 0.2343\n",
      "Epoch 180: Generator Loss = 83.8057, Discriminator Loss = 0.1837\n",
      "Epoch 181: Generator Loss = 86.2887, Discriminator Loss = 0.2416\n",
      "Epoch 182: Generator Loss = 82.4387, Discriminator Loss = 0.2846\n",
      "Epoch 183: Generator Loss = 143.6423, Discriminator Loss = 7.6630\n",
      "Epoch 184: Generator Loss = 102.9530, Discriminator Loss = 11.4707\n",
      "Epoch 185: Generator Loss = 73.6196, Discriminator Loss = 0.9945\n",
      "Epoch 186: Generator Loss = 66.1201, Discriminator Loss = 0.8766\n",
      "Epoch 187: Generator Loss = 83.6051, Discriminator Loss = 9.6471\n",
      "Epoch 188: Generator Loss = 70.4089, Discriminator Loss = 2.3475\n",
      "Epoch 189: Generator Loss = 66.8714, Discriminator Loss = 1.9625\n",
      "Epoch 190: Generator Loss = 66.1441, Discriminator Loss = 1.3355\n",
      "Epoch 191: Generator Loss = 70.8819, Discriminator Loss = 1.1987\n",
      "Epoch 192: Generator Loss = 72.1336, Discriminator Loss = 1.0931\n",
      "Epoch 193: Generator Loss = 65.2571, Discriminator Loss = 1.0506\n",
      "Epoch 194: Generator Loss = 61.0315, Discriminator Loss = 1.9323\n",
      "Epoch 195: Generator Loss = 68.3649, Discriminator Loss = 1.0146\n",
      "Epoch 196: Generator Loss = 60.1247, Discriminator Loss = 1.1125\n",
      "Epoch 197: Generator Loss = 54.9901, Discriminator Loss = 3.9815\n",
      "Epoch 198: Generator Loss = 62.9839, Discriminator Loss = 3.8636\n",
      "Epoch 199: Generator Loss = 55.6671, Discriminator Loss = 1.6821\n",
      "Epoch 200: Generator Loss = 55.6231, Discriminator Loss = 1.3135\n",
      "Epoch 201: Generator Loss = 59.7909, Discriminator Loss = 1.2010\n",
      "Epoch 202: Generator Loss = 64.3889, Discriminator Loss = 0.7489\n",
      "Epoch 203: Generator Loss = 65.3288, Discriminator Loss = 0.7853\n",
      "Epoch 204: Generator Loss = 63.6304, Discriminator Loss = 0.8871\n",
      "Epoch 205: Generator Loss = 74.3603, Discriminator Loss = 1.3588\n",
      "Epoch 206: Generator Loss = 60.9989, Discriminator Loss = 0.8442\n",
      "Epoch 207: Generator Loss = 61.7302, Discriminator Loss = 2.3502\n",
      "Epoch 208: Generator Loss = 78.2946, Discriminator Loss = 2.9160\n",
      "Epoch 209: Generator Loss = 53.8575, Discriminator Loss = 3.5227\n",
      "Epoch 210: Generator Loss = 60.5133, Discriminator Loss = 0.9441\n",
      "Epoch 211: Generator Loss = 57.7672, Discriminator Loss = 1.1891\n",
      "Epoch 212: Generator Loss = 84.7633, Discriminator Loss = 0.9558\n",
      "Epoch 213: Generator Loss = 61.4906, Discriminator Loss = 1.2094\n",
      "Epoch 214: Generator Loss = 70.1411, Discriminator Loss = 0.7437\n",
      "Epoch 215: Generator Loss = 83.3023, Discriminator Loss = 1.1305\n",
      "Epoch 216: Generator Loss = 77.5797, Discriminator Loss = 1.2259\n",
      "Epoch 217: Generator Loss = 67.7520, Discriminator Loss = 1.2305\n",
      "Epoch 218: Generator Loss = 85.5696, Discriminator Loss = 1.8146\n",
      "Epoch 219: Generator Loss = 72.3549, Discriminator Loss = 0.8356\n",
      "Epoch 220: Generator Loss = 76.9042, Discriminator Loss = 1.2232\n",
      "Epoch 221: Generator Loss = 96.4222, Discriminator Loss = 0.7207\n",
      "Epoch 222: Generator Loss = 80.7363, Discriminator Loss = 1.6912\n",
      "Epoch 223: Generator Loss = 70.4847, Discriminator Loss = 1.0537\n",
      "Epoch 224: Generator Loss = 75.8924, Discriminator Loss = 0.8397\n",
      "Epoch 225: Generator Loss = 77.7534, Discriminator Loss = 0.9326\n",
      "Epoch 226: Generator Loss = 82.5484, Discriminator Loss = 1.4251\n",
      "Epoch 227: Generator Loss = 71.1224, Discriminator Loss = 1.1684\n",
      "Epoch 228: Generator Loss = 94.9461, Discriminator Loss = 0.8533\n",
      "Epoch 229: Generator Loss = 79.0471, Discriminator Loss = 0.6685\n",
      "Epoch 230: Generator Loss = 72.0123, Discriminator Loss = 0.7313\n",
      "Epoch 231: Generator Loss = 67.1752, Discriminator Loss = 5.0465\n",
      "Epoch 232: Generator Loss = 67.4396, Discriminator Loss = 2.4032\n",
      "Epoch 233: Generator Loss = 55.4430, Discriminator Loss = 1.6311\n",
      "Epoch 234: Generator Loss = 68.5769, Discriminator Loss = 1.4051\n",
      "Epoch 235: Generator Loss = 67.4749, Discriminator Loss = 1.1399\n",
      "Epoch 236: Generator Loss = 68.9712, Discriminator Loss = 1.3174\n",
      "Epoch 237: Generator Loss = 73.2524, Discriminator Loss = 1.6401\n",
      "Epoch 238: Generator Loss = 79.6252, Discriminator Loss = 2.4878\n",
      "Epoch 239: Generator Loss = 79.4310, Discriminator Loss = 0.6824\n",
      "Epoch 240: Generator Loss = 75.9845, Discriminator Loss = 0.9665\n",
      "Epoch 241: Generator Loss = 62.8012, Discriminator Loss = 1.0327\n",
      "Epoch 242: Generator Loss = 65.9158, Discriminator Loss = 1.0999\n",
      "Epoch 243: Generator Loss = 66.1439, Discriminator Loss = 0.9072\n",
      "Epoch 244: Generator Loss = 66.8046, Discriminator Loss = 0.5368\n",
      "Epoch 245: Generator Loss = 81.7610, Discriminator Loss = 0.7577\n",
      "Epoch 246: Generator Loss = 69.8856, Discriminator Loss = 1.2309\n",
      "Epoch 247: Generator Loss = 74.1356, Discriminator Loss = 0.9072\n",
      "Epoch 248: Generator Loss = 70.4658, Discriminator Loss = 0.4787\n",
      "Epoch 249: Generator Loss = 102.9804, Discriminator Loss = 1.1746\n",
      "Epoch 250: Generator Loss = 55.9588, Discriminator Loss = 3.3093\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.4909 Acc: 0.7702\n",
      "Epoch 2/15 | Train Loss: 0.4091 Acc: 0.7885\n",
      "Epoch 3/15 | Train Loss: 0.4226 Acc: 0.7598\n",
      "Epoch 4/15 | Train Loss: 0.3525 Acc: 0.8146\n",
      "Epoch 5/15 | Train Loss: 0.3384 Acc: 0.8303\n",
      "Epoch 6/15 | Train Loss: 0.3069 Acc: 0.8407\n",
      "Epoch 7/15 | Train Loss: 0.2908 Acc: 0.8538\n",
      "Epoch 8/15 | Train Loss: 0.2439 Acc: 0.8956\n",
      "Epoch 9/15 | Train Loss: 0.2256 Acc: 0.8930\n",
      "Epoch 10/15 | Train Loss: 0.2616 Acc: 0.8668\n",
      "Epoch 11/15 | Train Loss: 0.2470 Acc: 0.8877\n",
      "Epoch 12/15 | Train Loss: 0.2732 Acc: 0.8486\n",
      "Epoch 13/15 | Train Loss: 0.2791 Acc: 0.8616\n",
      "Epoch 14/15 | Train Loss: 0.2448 Acc: 0.8877\n",
      "Epoch 15/15 | Train Loss: 0.2553 Acc: 0.8903\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5209 Acc: 0.7389\n",
      "Epoch 2/15 | Train Loss: 0.3758 Acc: 0.7885\n",
      "Epoch 3/15 | Train Loss: 0.3601 Acc: 0.7990\n",
      "Epoch 4/15 | Train Loss: 0.3543 Acc: 0.8172\n",
      "Epoch 5/15 | Train Loss: 0.3585 Acc: 0.8172\n",
      "Epoch 6/15 | Train Loss: 0.3286 Acc: 0.8277\n",
      "Epoch 7/15 | Train Loss: 0.2702 Acc: 0.8642\n",
      "Epoch 8/15 | Train Loss: 0.3429 Acc: 0.8512\n",
      "Epoch 9/15 | Train Loss: 0.3077 Acc: 0.8460\n",
      "Epoch 10/15 | Train Loss: 0.3231 Acc: 0.8460\n",
      "Epoch 11/15 | Train Loss: 0.3103 Acc: 0.8329\n",
      "Epoch 12/15 | Train Loss: 0.2930 Acc: 0.8433\n",
      "Epoch 13/15 | Train Loss: 0.2755 Acc: 0.8590\n",
      "Epoch 14/15 | Train Loss: 0.2626 Acc: 0.8747\n",
      "Epoch 15/15 | Train Loss: 0.2804 Acc: 0.8616\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5038 Acc: 0.7630\n",
      "Epoch 2/15 | Train Loss: 0.3906 Acc: 0.7969\n",
      "Epoch 3/15 | Train Loss: 0.4017 Acc: 0.8151\n",
      "Epoch 4/15 | Train Loss: 0.3542 Acc: 0.7969\n",
      "Epoch 5/15 | Train Loss: 0.3864 Acc: 0.7995\n",
      "Epoch 6/15 | Train Loss: 0.3545 Acc: 0.8151\n",
      "Epoch 7/15 | Train Loss: 0.3387 Acc: 0.8229\n",
      "Epoch 8/15 | Train Loss: 0.3214 Acc: 0.8151\n",
      "Epoch 9/15 | Train Loss: 0.2968 Acc: 0.8490\n",
      "Epoch 10/15 | Train Loss: 0.2932 Acc: 0.8542\n",
      "Epoch 11/15 | Train Loss: 0.2920 Acc: 0.8542\n",
      "Epoch 12/15 | Train Loss: 0.3058 Acc: 0.8568\n",
      "Epoch 13/15 | Train Loss: 0.2683 Acc: 0.8672\n",
      "Epoch 14/15 | Train Loss: 0.2688 Acc: 0.8542\n",
      "Epoch 15/15 | Train Loss: 0.2827 Acc: 0.8568\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5815 Acc: 0.7962\n",
      "Epoch 2/15 | Train Loss: 0.4538 Acc: 0.7930\n",
      "Epoch 3/15 | Train Loss: 0.3954 Acc: 0.8376\n",
      "Epoch 4/15 | Train Loss: 0.3909 Acc: 0.7898\n",
      "Epoch 5/15 | Train Loss: 0.3614 Acc: 0.8503\n",
      "Epoch 6/15 | Train Loss: 0.3625 Acc: 0.8408\n",
      "Epoch 7/15 | Train Loss: 0.3692 Acc: 0.8439\n",
      "Epoch 8/15 | Train Loss: 0.4041 Acc: 0.8312\n",
      "Epoch 9/15 | Train Loss: 0.3153 Acc: 0.8631\n",
      "Epoch 10/15 | Train Loss: 0.2949 Acc: 0.8631\n",
      "Epoch 11/15 | Train Loss: 0.2918 Acc: 0.8854\n",
      "Epoch 12/15 | Train Loss: 0.3046 Acc: 0.8726\n",
      "Epoch 13/15 | Train Loss: 0.3042 Acc: 0.8599\n",
      "Epoch 14/15 | Train Loss: 0.3141 Acc: 0.8631\n",
      "Epoch 15/15 | Train Loss: 0.3278 Acc: 0.8503\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4610 Acc: 0.8107\n",
      "Epoch 2/15 | Train Loss: 0.3470 Acc: 0.8750\n",
      "Epoch 3/15 | Train Loss: 0.2910 Acc: 0.8714\n",
      "Epoch 4/15 | Train Loss: 0.3221 Acc: 0.8750\n",
      "Epoch 5/15 | Train Loss: 0.3196 Acc: 0.8714\n",
      "Epoch 6/15 | Train Loss: 0.3184 Acc: 0.8571\n",
      "Epoch 7/15 | Train Loss: 0.3043 Acc: 0.8643\n",
      "Epoch 8/15 | Train Loss: 0.3005 Acc: 0.8893\n",
      "Epoch 9/15 | Train Loss: 0.2724 Acc: 0.9071\n",
      "Epoch 10/15 | Train Loss: 0.2514 Acc: 0.9214\n",
      "Epoch 11/15 | Train Loss: 0.2446 Acc: 0.9000\n",
      "Epoch 12/15 | Train Loss: 0.2229 Acc: 0.9214\n",
      "Epoch 13/15 | Train Loss: 0.2728 Acc: 0.8929\n",
      "Epoch 14/15 | Train Loss: 0.2234 Acc: 0.9214\n",
      "Epoch 15/15 | Train Loss: 0.1957 Acc: 0.9250\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6966 Acc: 0.6825\n",
      "Epoch 2/15 | Train Loss: 0.5866 Acc: 0.6746\n",
      "Epoch 3/15 | Train Loss: 0.5778 Acc: 0.7103\n",
      "Epoch 4/15 | Train Loss: 0.5365 Acc: 0.7262\n",
      "Epoch 5/15 | Train Loss: 0.4747 Acc: 0.7619\n",
      "Epoch 6/15 | Train Loss: 0.5259 Acc: 0.7500\n",
      "Epoch 7/15 | Train Loss: 0.4271 Acc: 0.8175\n",
      "Epoch 8/15 | Train Loss: 0.4357 Acc: 0.7976\n",
      "Epoch 9/15 | Train Loss: 0.3901 Acc: 0.8294\n",
      "Epoch 10/15 | Train Loss: 0.4292 Acc: 0.7579\n",
      "Epoch 11/15 | Train Loss: 0.3707 Acc: 0.8373\n",
      "Epoch 12/15 | Train Loss: 0.4384 Acc: 0.7738\n",
      "Epoch 13/15 | Train Loss: 0.3492 Acc: 0.8452\n",
      "Epoch 14/15 | Train Loss: 0.3433 Acc: 0.8413\n",
      "Epoch 15/15 | Train Loss: 0.3535 Acc: 0.8333\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7107 Acc: 0.6310\n",
      "Epoch 2/15 | Train Loss: 0.6164 Acc: 0.6548\n",
      "Epoch 3/15 | Train Loss: 0.5468 Acc: 0.7063\n",
      "Epoch 4/15 | Train Loss: 0.5245 Acc: 0.7262\n",
      "Epoch 5/15 | Train Loss: 0.5646 Acc: 0.6984\n",
      "Epoch 6/15 | Train Loss: 0.4505 Acc: 0.7698\n",
      "Epoch 7/15 | Train Loss: 0.5249 Acc: 0.7500\n",
      "Epoch 8/15 | Train Loss: 0.4346 Acc: 0.7738\n",
      "Epoch 9/15 | Train Loss: 0.3999 Acc: 0.8175\n",
      "Epoch 10/15 | Train Loss: 0.4191 Acc: 0.7897\n",
      "Epoch 11/15 | Train Loss: 0.4136 Acc: 0.7937\n",
      "Epoch 12/15 | Train Loss: 0.4139 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3716 Acc: 0.8571\n",
      "Epoch 14/15 | Train Loss: 0.4542 Acc: 0.7857\n",
      "Epoch 15/15 | Train Loss: 0.4122 Acc: 0.8214\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7537 Acc: 0.6285\n",
      "Epoch 2/15 | Train Loss: 0.6238 Acc: 0.6561\n",
      "Epoch 3/15 | Train Loss: 0.5518 Acc: 0.7352\n",
      "Epoch 4/15 | Train Loss: 0.5617 Acc: 0.7391\n",
      "Epoch 5/15 | Train Loss: 0.4979 Acc: 0.7628\n",
      "Epoch 6/15 | Train Loss: 0.4960 Acc: 0.7431\n",
      "Epoch 7/15 | Train Loss: 0.4857 Acc: 0.7549\n",
      "Epoch 8/15 | Train Loss: 0.4516 Acc: 0.7747\n",
      "Epoch 9/15 | Train Loss: 0.4412 Acc: 0.7826\n",
      "Epoch 10/15 | Train Loss: 0.4157 Acc: 0.8142\n",
      "Epoch 11/15 | Train Loss: 0.3654 Acc: 0.8300\n",
      "Epoch 12/15 | Train Loss: 0.4403 Acc: 0.8024\n",
      "Epoch 13/15 | Train Loss: 0.4261 Acc: 0.7905\n",
      "Epoch 14/15 | Train Loss: 0.4484 Acc: 0.7905\n",
      "Epoch 15/15 | Train Loss: 0.4354 Acc: 0.7787\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6317 Acc: 0.6992\n",
      "Epoch 2/15 | Train Loss: 0.4953 Acc: 0.7797\n",
      "Epoch 3/15 | Train Loss: 0.5619 Acc: 0.7373\n",
      "Epoch 4/15 | Train Loss: 0.4893 Acc: 0.7797\n",
      "Epoch 5/15 | Train Loss: 0.5566 Acc: 0.7500\n",
      "Epoch 6/15 | Train Loss: 0.4668 Acc: 0.7924\n",
      "Epoch 7/15 | Train Loss: 0.4268 Acc: 0.7839\n",
      "Epoch 8/15 | Train Loss: 0.3891 Acc: 0.8136\n",
      "Epoch 9/15 | Train Loss: 0.4800 Acc: 0.7881\n",
      "Epoch 10/15 | Train Loss: 0.3751 Acc: 0.8263\n",
      "Epoch 11/15 | Train Loss: 0.3893 Acc: 0.8220\n",
      "Epoch 12/15 | Train Loss: 0.3956 Acc: 0.8347\n",
      "Epoch 13/15 | Train Loss: 0.4092 Acc: 0.7881\n",
      "Epoch 14/15 | Train Loss: 0.3990 Acc: 0.8263\n",
      "Epoch 15/15 | Train Loss: 0.3766 Acc: 0.8432\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.8352 Acc: 0.5727\n",
      "Epoch 2/15 | Train Loss: 0.5163 Acc: 0.8062\n",
      "Epoch 3/15 | Train Loss: 0.3754 Acc: 0.8546\n",
      "Epoch 4/15 | Train Loss: 0.3780 Acc: 0.8458\n",
      "Epoch 5/15 | Train Loss: 0.3776 Acc: 0.8458\n",
      "Epoch 6/15 | Train Loss: 0.3055 Acc: 0.8722\n",
      "Epoch 7/15 | Train Loss: 0.3462 Acc: 0.8590\n",
      "Epoch 8/15 | Train Loss: 0.2760 Acc: 0.8899\n",
      "Epoch 9/15 | Train Loss: 0.2767 Acc: 0.8811\n",
      "Epoch 10/15 | Train Loss: 0.3235 Acc: 0.8811\n",
      "Epoch 11/15 | Train Loss: 0.2810 Acc: 0.8899\n",
      "Epoch 12/15 | Train Loss: 0.2762 Acc: 0.8811\n",
      "Epoch 13/15 | Train Loss: 0.3303 Acc: 0.8722\n",
      "Epoch 14/15 | Train Loss: 0.2727 Acc: 0.8943\n",
      "Epoch 15/15 | Train Loss: 0.2617 Acc: 0.8943\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.5min\n",
      "Epoch 1/15 | Train Loss: 0.7089 Acc: 0.6622\n",
      "Epoch 2/15 | Train Loss: 0.5255 Acc: 0.7264\n",
      "Epoch 3/15 | Train Loss: 0.5162 Acc: 0.7196\n",
      "Epoch 4/15 | Train Loss: 0.4472 Acc: 0.7770\n",
      "Epoch 5/15 | Train Loss: 0.4568 Acc: 0.7669\n",
      "Epoch 6/15 | Train Loss: 0.4479 Acc: 0.8007\n",
      "Epoch 7/15 | Train Loss: 0.3605 Acc: 0.8378\n",
      "Epoch 8/15 | Train Loss: 0.3189 Acc: 0.8682\n",
      "Epoch 9/15 | Train Loss: 0.3317 Acc: 0.8514\n",
      "Epoch 10/15 | Train Loss: 0.3844 Acc: 0.8142\n",
      "Epoch 11/15 | Train Loss: 0.3683 Acc: 0.8311\n",
      "Epoch 12/15 | Train Loss: 0.3118 Acc: 0.8649\n",
      "Epoch 13/15 | Train Loss: 0.2963 Acc: 0.8581\n",
      "Epoch 14/15 | Train Loss: 0.3387 Acc: 0.8311\n",
      "Epoch 15/15 | Train Loss: 0.3362 Acc: 0.8446\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6066 Acc: 0.6926\n",
      "Epoch 2/15 | Train Loss: 0.5241 Acc: 0.6892\n",
      "Epoch 3/15 | Train Loss: 0.4356 Acc: 0.7432\n",
      "Epoch 4/15 | Train Loss: 0.4836 Acc: 0.7432\n",
      "Epoch 5/15 | Train Loss: 0.4568 Acc: 0.7500\n",
      "Epoch 6/15 | Train Loss: 0.4437 Acc: 0.7568\n",
      "Epoch 7/15 | Train Loss: 0.3691 Acc: 0.7939\n",
      "Epoch 8/15 | Train Loss: 0.3831 Acc: 0.8041\n",
      "Epoch 9/15 | Train Loss: 0.3483 Acc: 0.8311\n",
      "Epoch 10/15 | Train Loss: 0.3931 Acc: 0.7736\n",
      "Epoch 11/15 | Train Loss: 0.3591 Acc: 0.8243\n",
      "Epoch 12/15 | Train Loss: 0.3554 Acc: 0.8311\n",
      "Epoch 13/15 | Train Loss: 0.3733 Acc: 0.8041\n",
      "Epoch 14/15 | Train Loss: 0.3500 Acc: 0.8412\n",
      "Epoch 15/15 | Train Loss: 0.3540 Acc: 0.8480\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5999 Acc: 0.6599\n",
      "Epoch 2/15 | Train Loss: 0.4822 Acc: 0.7576\n",
      "Epoch 3/15 | Train Loss: 0.4504 Acc: 0.7643\n",
      "Epoch 4/15 | Train Loss: 0.4134 Acc: 0.7946\n",
      "Epoch 5/15 | Train Loss: 0.4448 Acc: 0.7879\n",
      "Epoch 6/15 | Train Loss: 0.4246 Acc: 0.7643\n",
      "Epoch 7/15 | Train Loss: 0.3957 Acc: 0.8047\n",
      "Epoch 8/15 | Train Loss: 0.3610 Acc: 0.8148\n",
      "Epoch 9/15 | Train Loss: 0.3419 Acc: 0.8350\n",
      "Epoch 10/15 | Train Loss: 0.3549 Acc: 0.8485\n",
      "Epoch 11/15 | Train Loss: 0.3518 Acc: 0.8316\n",
      "Epoch 12/15 | Train Loss: 0.3511 Acc: 0.8283\n",
      "Epoch 13/15 | Train Loss: 0.3556 Acc: 0.8249\n",
      "Epoch 14/15 | Train Loss: 0.3608 Acc: 0.8451\n",
      "Epoch 15/15 | Train Loss: 0.3698 Acc: 0.8047\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6064 Acc: 0.7176\n",
      "Epoch 2/15 | Train Loss: 0.5452 Acc: 0.7634\n",
      "Epoch 3/15 | Train Loss: 0.4836 Acc: 0.7748\n",
      "Epoch 4/15 | Train Loss: 0.4293 Acc: 0.8015\n",
      "Epoch 5/15 | Train Loss: 0.4707 Acc: 0.8092\n",
      "Epoch 6/15 | Train Loss: 0.4408 Acc: 0.8015\n",
      "Epoch 7/15 | Train Loss: 0.4243 Acc: 0.8206\n",
      "Epoch 8/15 | Train Loss: 0.3684 Acc: 0.8244\n",
      "Epoch 9/15 | Train Loss: 0.3621 Acc: 0.8435\n",
      "Epoch 10/15 | Train Loss: 0.3768 Acc: 0.8282\n",
      "Epoch 11/15 | Train Loss: 0.3906 Acc: 0.8244\n",
      "Epoch 12/15 | Train Loss: 0.3620 Acc: 0.8397\n",
      "Epoch 13/15 | Train Loss: 0.3878 Acc: 0.8282\n",
      "Epoch 14/15 | Train Loss: 0.3556 Acc: 0.8282\n",
      "Epoch 15/15 | Train Loss: 0.3435 Acc: 0.8664\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5238 Acc: 0.7755\n",
      "Epoch 2/15 | Train Loss: 0.3627 Acc: 0.8571\n",
      "Epoch 3/15 | Train Loss: 0.3850 Acc: 0.8367\n",
      "Epoch 4/15 | Train Loss: 0.3623 Acc: 0.8612\n",
      "Epoch 5/15 | Train Loss: 0.3316 Acc: 0.8939\n",
      "Epoch 6/15 | Train Loss: 0.3118 Acc: 0.8857\n",
      "Epoch 7/15 | Train Loss: 0.3011 Acc: 0.8776\n",
      "Epoch 8/15 | Train Loss: 0.3418 Acc: 0.8612\n",
      "Epoch 9/15 | Train Loss: 0.2715 Acc: 0.8653\n",
      "Epoch 10/15 | Train Loss: 0.2747 Acc: 0.9061\n",
      "Epoch 11/15 | Train Loss: 0.2692 Acc: 0.8816\n",
      "Epoch 12/15 | Train Loss: 0.2555 Acc: 0.9143\n",
      "Epoch 13/15 | Train Loss: 0.2743 Acc: 0.8980\n",
      "Epoch 14/15 | Train Loss: 0.2764 Acc: 0.9020\n",
      "Epoch 15/15 | Train Loss: 0.2796 Acc: 0.9184\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7141 Acc: 0.5873\n",
      "Epoch 2/15 | Train Loss: 0.5529 Acc: 0.7063\n",
      "Epoch 3/15 | Train Loss: 0.5344 Acc: 0.7302\n",
      "Epoch 4/15 | Train Loss: 0.5042 Acc: 0.7381\n",
      "Epoch 5/15 | Train Loss: 0.4782 Acc: 0.7976\n",
      "Epoch 6/15 | Train Loss: 0.5094 Acc: 0.7341\n",
      "Epoch 7/15 | Train Loss: 0.4489 Acc: 0.7778\n",
      "Epoch 8/15 | Train Loss: 0.4285 Acc: 0.8016\n",
      "Epoch 9/15 | Train Loss: 0.3695 Acc: 0.8492\n",
      "Epoch 10/15 | Train Loss: 0.3913 Acc: 0.7857\n",
      "Epoch 11/15 | Train Loss: 0.3884 Acc: 0.8373\n",
      "Epoch 12/15 | Train Loss: 0.3721 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3280 Acc: 0.8413\n",
      "Epoch 14/15 | Train Loss: 0.3597 Acc: 0.8413\n",
      "Epoch 15/15 | Train Loss: 0.4193 Acc: 0.8095\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7029 Acc: 0.6151\n",
      "Epoch 2/15 | Train Loss: 0.6266 Acc: 0.6667\n",
      "Epoch 3/15 | Train Loss: 0.5975 Acc: 0.6786\n",
      "Epoch 4/15 | Train Loss: 0.5108 Acc: 0.7103\n",
      "Epoch 5/15 | Train Loss: 0.5158 Acc: 0.7341\n",
      "Epoch 6/15 | Train Loss: 0.4830 Acc: 0.7659\n",
      "Epoch 7/15 | Train Loss: 0.4462 Acc: 0.7857\n",
      "Epoch 8/15 | Train Loss: 0.4777 Acc: 0.7500\n",
      "Epoch 9/15 | Train Loss: 0.4155 Acc: 0.7937\n",
      "Epoch 10/15 | Train Loss: 0.4670 Acc: 0.7698\n",
      "Epoch 11/15 | Train Loss: 0.4185 Acc: 0.7976\n",
      "Epoch 12/15 | Train Loss: 0.4394 Acc: 0.7976\n",
      "Epoch 13/15 | Train Loss: 0.4129 Acc: 0.8095\n",
      "Epoch 14/15 | Train Loss: 0.3979 Acc: 0.8333\n",
      "Epoch 15/15 | Train Loss: 0.3840 Acc: 0.8254\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6994 Acc: 0.6443\n",
      "Epoch 2/15 | Train Loss: 0.5686 Acc: 0.6798\n",
      "Epoch 3/15 | Train Loss: 0.5543 Acc: 0.7036\n",
      "Epoch 4/15 | Train Loss: 0.5333 Acc: 0.7154\n",
      "Epoch 5/15 | Train Loss: 0.4949 Acc: 0.7431\n",
      "Epoch 6/15 | Train Loss: 0.4998 Acc: 0.7549\n",
      "Epoch 7/15 | Train Loss: 0.4829 Acc: 0.7668\n",
      "Epoch 8/15 | Train Loss: 0.4752 Acc: 0.7747\n",
      "Epoch 9/15 | Train Loss: 0.4703 Acc: 0.7549\n",
      "Epoch 10/15 | Train Loss: 0.4261 Acc: 0.7747\n",
      "Epoch 11/15 | Train Loss: 0.3962 Acc: 0.8221\n",
      "Epoch 12/15 | Train Loss: 0.3759 Acc: 0.8340\n",
      "Epoch 13/15 | Train Loss: 0.3765 Acc: 0.8379\n",
      "Epoch 14/15 | Train Loss: 0.3883 Acc: 0.8221\n",
      "Epoch 15/15 | Train Loss: 0.3998 Acc: 0.7747\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7518 Acc: 0.5890\n",
      "Epoch 2/15 | Train Loss: 0.5926 Acc: 0.7246\n",
      "Epoch 3/15 | Train Loss: 0.5192 Acc: 0.7119\n",
      "Epoch 4/15 | Train Loss: 0.4796 Acc: 0.7669\n",
      "Epoch 5/15 | Train Loss: 0.4916 Acc: 0.7542\n",
      "Epoch 6/15 | Train Loss: 0.4873 Acc: 0.8136\n",
      "Epoch 7/15 | Train Loss: 0.4282 Acc: 0.8305\n",
      "Epoch 8/15 | Train Loss: 0.4176 Acc: 0.8220\n",
      "Epoch 9/15 | Train Loss: 0.3791 Acc: 0.8178\n",
      "Epoch 10/15 | Train Loss: 0.4478 Acc: 0.7966\n",
      "Epoch 11/15 | Train Loss: 0.4513 Acc: 0.7797\n",
      "Epoch 12/15 | Train Loss: 0.4088 Acc: 0.8136\n",
      "Epoch 13/15 | Train Loss: 0.3939 Acc: 0.8051\n",
      "Epoch 14/15 | Train Loss: 0.4135 Acc: 0.8559\n",
      "Epoch 15/15 | Train Loss: 0.3934 Acc: 0.8178\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.5min\n",
      "Epoch 1/15 | Train Loss: 0.5368 Acc: 0.8018\n",
      "Epoch 2/15 | Train Loss: 0.4620 Acc: 0.8062\n",
      "Epoch 3/15 | Train Loss: 0.4203 Acc: 0.8458\n",
      "Epoch 4/15 | Train Loss: 0.4052 Acc: 0.8282\n",
      "Epoch 5/15 | Train Loss: 0.3986 Acc: 0.8326\n",
      "Epoch 6/15 | Train Loss: 0.3472 Acc: 0.8634\n",
      "Epoch 7/15 | Train Loss: 0.3487 Acc: 0.8811\n",
      "Epoch 8/15 | Train Loss: 0.4317 Acc: 0.8370\n",
      "Epoch 9/15 | Train Loss: 0.3267 Acc: 0.8899\n",
      "Epoch 10/15 | Train Loss: 0.3693 Acc: 0.8546\n",
      "Epoch 11/15 | Train Loss: 0.2771 Acc: 0.8811\n",
      "Epoch 12/15 | Train Loss: 0.3099 Acc: 0.8767\n",
      "Epoch 13/15 | Train Loss: 0.3587 Acc: 0.8634\n",
      "Epoch 14/15 | Train Loss: 0.3220 Acc: 0.8678\n",
      "Epoch 15/15 | Train Loss: 0.2574 Acc: 0.8943\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.5min\n",
      "Epoch 1/15 | Train Loss: 0.5456 Acc: 0.7598\n",
      "Epoch 2/15 | Train Loss: 0.4158 Acc: 0.7598\n",
      "Epoch 3/15 | Train Loss: 0.3640 Acc: 0.8042\n",
      "Epoch 4/15 | Train Loss: 0.3367 Acc: 0.8303\n",
      "Epoch 5/15 | Train Loss: 0.3427 Acc: 0.8381\n",
      "Epoch 6/15 | Train Loss: 0.3126 Acc: 0.8512\n",
      "Epoch 7/15 | Train Loss: 0.2730 Acc: 0.8747\n",
      "Epoch 8/15 | Train Loss: 0.2868 Acc: 0.8851\n",
      "Epoch 9/15 | Train Loss: 0.2364 Acc: 0.9008\n",
      "Epoch 10/15 | Train Loss: 0.2668 Acc: 0.8825\n",
      "Epoch 11/15 | Train Loss: 0.2501 Acc: 0.8825\n",
      "Epoch 12/15 | Train Loss: 0.2513 Acc: 0.8851\n",
      "Epoch 13/15 | Train Loss: 0.2645 Acc: 0.8642\n",
      "Epoch 14/15 | Train Loss: 0.2350 Acc: 0.8877\n",
      "Epoch 15/15 | Train Loss: 0.2411 Acc: 0.8851\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5265 Acc: 0.7493\n",
      "Epoch 2/15 | Train Loss: 0.4171 Acc: 0.7807\n",
      "Epoch 3/15 | Train Loss: 0.3686 Acc: 0.8016\n",
      "Epoch 4/15 | Train Loss: 0.3571 Acc: 0.8251\n",
      "Epoch 5/15 | Train Loss: 0.3538 Acc: 0.8094\n",
      "Epoch 6/15 | Train Loss: 0.3742 Acc: 0.8094\n",
      "Epoch 7/15 | Train Loss: 0.3980 Acc: 0.7885\n",
      "Epoch 8/15 | Train Loss: 0.3036 Acc: 0.8355\n",
      "Epoch 9/15 | Train Loss: 0.2804 Acc: 0.8668\n",
      "Epoch 10/15 | Train Loss: 0.2799 Acc: 0.8590\n",
      "Epoch 11/15 | Train Loss: 0.2668 Acc: 0.8616\n",
      "Epoch 12/15 | Train Loss: 0.2854 Acc: 0.8460\n",
      "Epoch 13/15 | Train Loss: 0.2689 Acc: 0.8799\n",
      "Epoch 14/15 | Train Loss: 0.2676 Acc: 0.8799\n",
      "Epoch 15/15 | Train Loss: 0.2426 Acc: 0.9008\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5016 Acc: 0.7448\n",
      "Epoch 2/15 | Train Loss: 0.4207 Acc: 0.7734\n",
      "Epoch 3/15 | Train Loss: 0.3997 Acc: 0.7682\n",
      "Epoch 4/15 | Train Loss: 0.3625 Acc: 0.7812\n",
      "Epoch 5/15 | Train Loss: 0.3325 Acc: 0.8411\n",
      "Epoch 6/15 | Train Loss: 0.3153 Acc: 0.8255\n",
      "Epoch 7/15 | Train Loss: 0.3294 Acc: 0.8438\n",
      "Epoch 8/15 | Train Loss: 0.3357 Acc: 0.8151\n",
      "Epoch 9/15 | Train Loss: 0.3048 Acc: 0.8516\n",
      "Epoch 10/15 | Train Loss: 0.2761 Acc: 0.8568\n",
      "Epoch 11/15 | Train Loss: 0.2673 Acc: 0.8828\n",
      "Epoch 12/15 | Train Loss: 0.2598 Acc: 0.8698\n",
      "Epoch 13/15 | Train Loss: 0.2998 Acc: 0.8385\n",
      "Epoch 14/15 | Train Loss: 0.2690 Acc: 0.8750\n",
      "Epoch 15/15 | Train Loss: 0.2984 Acc: 0.8542\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4740 Acc: 0.7771\n",
      "Epoch 2/15 | Train Loss: 0.4075 Acc: 0.8153\n",
      "Epoch 3/15 | Train Loss: 0.3929 Acc: 0.8280\n",
      "Epoch 4/15 | Train Loss: 0.3636 Acc: 0.8503\n",
      "Epoch 5/15 | Train Loss: 0.3037 Acc: 0.8567\n",
      "Epoch 6/15 | Train Loss: 0.3707 Acc: 0.8312\n",
      "Epoch 7/15 | Train Loss: 0.3244 Acc: 0.8567\n",
      "Epoch 8/15 | Train Loss: 0.3307 Acc: 0.8567\n",
      "Epoch 9/15 | Train Loss: 0.3085 Acc: 0.8631\n",
      "Epoch 10/15 | Train Loss: 0.2733 Acc: 0.8854\n",
      "Epoch 11/15 | Train Loss: 0.3501 Acc: 0.8471\n",
      "Epoch 12/15 | Train Loss: 0.3078 Acc: 0.8535\n",
      "Epoch 13/15 | Train Loss: 0.2889 Acc: 0.8694\n",
      "Epoch 14/15 | Train Loss: 0.2787 Acc: 0.8885\n",
      "Epoch 15/15 | Train Loss: 0.2823 Acc: 0.8790\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4609 Acc: 0.8107\n",
      "Epoch 2/15 | Train Loss: 0.4000 Acc: 0.8571\n",
      "Epoch 3/15 | Train Loss: 0.3519 Acc: 0.8643\n",
      "Epoch 4/15 | Train Loss: 0.3235 Acc: 0.8714\n",
      "Epoch 5/15 | Train Loss: 0.3334 Acc: 0.8750\n",
      "Epoch 6/15 | Train Loss: 0.2866 Acc: 0.8786\n",
      "Epoch 7/15 | Train Loss: 0.2386 Acc: 0.8964\n",
      "Epoch 8/15 | Train Loss: 0.2864 Acc: 0.9000\n",
      "Epoch 9/15 | Train Loss: 0.2411 Acc: 0.8929\n",
      "Epoch 10/15 | Train Loss: 0.2172 Acc: 0.9214\n",
      "Epoch 11/15 | Train Loss: 0.2564 Acc: 0.9107\n",
      "Epoch 12/15 | Train Loss: 0.2255 Acc: 0.9036\n",
      "Epoch 13/15 | Train Loss: 0.1920 Acc: 0.9250\n",
      "Epoch 14/15 | Train Loss: 0.2049 Acc: 0.9321\n",
      "Epoch 15/15 | Train Loss: 0.2035 Acc: 0.9036\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.8756 Acc: 0.6349\n",
      "Epoch 2/15 | Train Loss: 0.5823 Acc: 0.7262\n",
      "Epoch 3/15 | Train Loss: 0.5391 Acc: 0.7103\n",
      "Epoch 4/15 | Train Loss: 0.5842 Acc: 0.7024\n",
      "Epoch 5/15 | Train Loss: 0.4789 Acc: 0.7738\n",
      "Epoch 6/15 | Train Loss: 0.4221 Acc: 0.8056\n",
      "Epoch 7/15 | Train Loss: 0.4550 Acc: 0.7778\n",
      "Epoch 8/15 | Train Loss: 0.4229 Acc: 0.7738\n",
      "Epoch 9/15 | Train Loss: 0.3760 Acc: 0.8214\n",
      "Epoch 10/15 | Train Loss: 0.3903 Acc: 0.8413\n",
      "Epoch 11/15 | Train Loss: 0.3506 Acc: 0.8690\n",
      "Epoch 12/15 | Train Loss: 0.3856 Acc: 0.8294\n",
      "Epoch 13/15 | Train Loss: 0.3746 Acc: 0.8373\n",
      "Epoch 14/15 | Train Loss: 0.4181 Acc: 0.7976\n",
      "Epoch 15/15 | Train Loss: 0.3579 Acc: 0.8135\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7642 Acc: 0.6270\n",
      "Epoch 2/15 | Train Loss: 0.6067 Acc: 0.6825\n",
      "Epoch 3/15 | Train Loss: 0.5546 Acc: 0.7063\n",
      "Epoch 4/15 | Train Loss: 0.5404 Acc: 0.7063\n",
      "Epoch 5/15 | Train Loss: 0.5460 Acc: 0.7143\n",
      "Epoch 6/15 | Train Loss: 0.5331 Acc: 0.6786\n",
      "Epoch 7/15 | Train Loss: 0.5215 Acc: 0.7460\n",
      "Epoch 8/15 | Train Loss: 0.4794 Acc: 0.7302\n",
      "Epoch 9/15 | Train Loss: 0.4286 Acc: 0.7937\n",
      "Epoch 10/15 | Train Loss: 0.3993 Acc: 0.7897\n",
      "Epoch 11/15 | Train Loss: 0.4388 Acc: 0.7738\n",
      "Epoch 12/15 | Train Loss: 0.4133 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3839 Acc: 0.8452\n",
      "Epoch 14/15 | Train Loss: 0.4224 Acc: 0.8056\n",
      "Epoch 15/15 | Train Loss: 0.4429 Acc: 0.7540\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6747 Acc: 0.6245\n",
      "Epoch 2/15 | Train Loss: 0.6045 Acc: 0.6680\n",
      "Epoch 3/15 | Train Loss: 0.5636 Acc: 0.6917\n",
      "Epoch 4/15 | Train Loss: 0.5505 Acc: 0.7233\n",
      "Epoch 5/15 | Train Loss: 0.4661 Acc: 0.7589\n",
      "Epoch 6/15 | Train Loss: 0.4477 Acc: 0.7668\n",
      "Epoch 7/15 | Train Loss: 0.4956 Acc: 0.7866\n",
      "Epoch 8/15 | Train Loss: 0.4400 Acc: 0.7866\n",
      "Epoch 9/15 | Train Loss: 0.4183 Acc: 0.7984\n",
      "Epoch 10/15 | Train Loss: 0.4321 Acc: 0.7945\n",
      "Epoch 11/15 | Train Loss: 0.3893 Acc: 0.7984\n",
      "Epoch 12/15 | Train Loss: 0.4372 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.4844 Acc: 0.7312\n",
      "Epoch 14/15 | Train Loss: 0.3860 Acc: 0.8300\n",
      "Epoch 15/15 | Train Loss: 0.3719 Acc: 0.8538\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6313 Acc: 0.6949\n",
      "Epoch 2/15 | Train Loss: 0.5386 Acc: 0.7246\n",
      "Epoch 3/15 | Train Loss: 0.5403 Acc: 0.7712\n",
      "Epoch 4/15 | Train Loss: 0.5127 Acc: 0.7924\n",
      "Epoch 5/15 | Train Loss: 0.5112 Acc: 0.7966\n",
      "Epoch 6/15 | Train Loss: 0.4725 Acc: 0.8093\n",
      "Epoch 7/15 | Train Loss: 0.4632 Acc: 0.8093\n",
      "Epoch 8/15 | Train Loss: 0.4022 Acc: 0.8432\n",
      "Epoch 9/15 | Train Loss: 0.4035 Acc: 0.8136\n",
      "Epoch 10/15 | Train Loss: 0.4099 Acc: 0.8093\n",
      "Epoch 11/15 | Train Loss: 0.4453 Acc: 0.8051\n",
      "Epoch 12/15 | Train Loss: 0.4044 Acc: 0.7881\n",
      "Epoch 13/15 | Train Loss: 0.3956 Acc: 0.8220\n",
      "Epoch 14/15 | Train Loss: 0.4018 Acc: 0.8136\n",
      "Epoch 15/15 | Train Loss: 0.3402 Acc: 0.8390\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5443 Acc: 0.7841\n",
      "Epoch 2/15 | Train Loss: 0.4454 Acc: 0.8326\n",
      "Epoch 3/15 | Train Loss: 0.4084 Acc: 0.8414\n",
      "Epoch 4/15 | Train Loss: 0.4047 Acc: 0.8370\n",
      "Epoch 5/15 | Train Loss: 0.4132 Acc: 0.8282\n",
      "Epoch 6/15 | Train Loss: 0.4214 Acc: 0.8326\n",
      "Epoch 7/15 | Train Loss: 0.3291 Acc: 0.8546\n",
      "Epoch 8/15 | Train Loss: 0.3476 Acc: 0.8634\n",
      "Epoch 9/15 | Train Loss: 0.3528 Acc: 0.8678\n",
      "Epoch 10/15 | Train Loss: 0.2568 Acc: 0.9119\n",
      "Epoch 11/15 | Train Loss: 0.3104 Acc: 0.8899\n",
      "Epoch 12/15 | Train Loss: 0.3176 Acc: 0.8767\n",
      "Epoch 13/15 | Train Loss: 0.2899 Acc: 0.8811\n",
      "Epoch 14/15 | Train Loss: 0.2911 Acc: 0.8767\n",
      "Epoch 15/15 | Train Loss: 0.2615 Acc: 0.8943\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.5min\n",
      "Epoch 1/15 | Train Loss: 0.5053 Acc: 0.7798\n",
      "Epoch 2/15 | Train Loss: 0.4656 Acc: 0.7867\n",
      "Epoch 3/15 | Train Loss: 0.3904 Acc: 0.7798\n",
      "Epoch 4/15 | Train Loss: 0.3673 Acc: 0.8028\n",
      "Epoch 5/15 | Train Loss: 0.3305 Acc: 0.8349\n",
      "Epoch 6/15 | Train Loss: 0.3711 Acc: 0.8211\n",
      "Epoch 7/15 | Train Loss: 0.3399 Acc: 0.8096\n",
      "Epoch 8/15 | Train Loss: 0.3096 Acc: 0.8532\n",
      "Epoch 9/15 | Train Loss: 0.3026 Acc: 0.8440\n",
      "Epoch 10/15 | Train Loss: 0.2838 Acc: 0.8876\n",
      "Epoch 11/15 | Train Loss: 0.2824 Acc: 0.8716\n",
      "Epoch 12/15 | Train Loss: 0.2673 Acc: 0.8807\n",
      "Epoch 13/15 | Train Loss: 0.2676 Acc: 0.8853\n",
      "Epoch 14/15 | Train Loss: 0.2713 Acc: 0.8807\n",
      "Epoch 15/15 | Train Loss: 0.2432 Acc: 0.9037\n",
      "Fold 6 Test Accuracy: 0.7159\n",
      "===== Fold 7 =====\n",
      "Epoch 1: Generator Loss = 9.9714, Discriminator Loss = 8.9068\n",
      "Epoch 2: Generator Loss = 11.7684, Discriminator Loss = 7.2323\n",
      "Epoch 3: Generator Loss = 17.2658, Discriminator Loss = 5.1557\n",
      "Epoch 4: Generator Loss = 25.6594, Discriminator Loss = 4.0362\n",
      "Epoch 5: Generator Loss = 28.6369, Discriminator Loss = 4.1217\n",
      "Epoch 6: Generator Loss = 29.9361, Discriminator Loss = 5.8886\n",
      "Epoch 7: Generator Loss = 33.6654, Discriminator Loss = 6.7328\n",
      "Epoch 8: Generator Loss = 22.3155, Discriminator Loss = 7.6517\n",
      "Epoch 9: Generator Loss = 22.9510, Discriminator Loss = 8.1961\n",
      "Epoch 10: Generator Loss = 20.4796, Discriminator Loss = 6.1569\n",
      "Epoch 11: Generator Loss = 17.4183, Discriminator Loss = 8.0675\n",
      "Epoch 12: Generator Loss = 22.4282, Discriminator Loss = 7.0245\n",
      "Epoch 13: Generator Loss = 19.3236, Discriminator Loss = 7.7447\n",
      "Epoch 14: Generator Loss = 21.9205, Discriminator Loss = 9.2476\n",
      "Epoch 15: Generator Loss = 21.6504, Discriminator Loss = 7.3313\n",
      "Epoch 16: Generator Loss = 20.1511, Discriminator Loss = 9.0904\n",
      "Epoch 17: Generator Loss = 18.9432, Discriminator Loss = 8.1815\n",
      "Epoch 18: Generator Loss = 16.3218, Discriminator Loss = 8.7447\n",
      "Epoch 19: Generator Loss = 18.9600, Discriminator Loss = 8.8067\n",
      "Epoch 20: Generator Loss = 16.1648, Discriminator Loss = 8.4280\n",
      "Epoch 21: Generator Loss = 17.2251, Discriminator Loss = 8.7536\n",
      "Epoch 22: Generator Loss = 17.4112, Discriminator Loss = 9.9065\n",
      "Epoch 23: Generator Loss = 17.8152, Discriminator Loss = 7.8494\n",
      "Epoch 24: Generator Loss = 14.9039, Discriminator Loss = 8.5106\n",
      "Epoch 25: Generator Loss = 15.8976, Discriminator Loss = 8.5084\n",
      "Epoch 26: Generator Loss = 14.8428, Discriminator Loss = 8.6829\n",
      "Epoch 27: Generator Loss = 15.5847, Discriminator Loss = 8.2389\n",
      "Epoch 28: Generator Loss = 16.2398, Discriminator Loss = 6.6635\n",
      "Epoch 29: Generator Loss = 16.4921, Discriminator Loss = 7.6812\n",
      "Epoch 30: Generator Loss = 16.2227, Discriminator Loss = 7.9608\n",
      "Epoch 31: Generator Loss = 16.8617, Discriminator Loss = 8.0884\n",
      "Epoch 32: Generator Loss = 18.0154, Discriminator Loss = 9.1978\n",
      "Epoch 33: Generator Loss = 17.8308, Discriminator Loss = 9.0805\n",
      "Epoch 34: Generator Loss = 18.9477, Discriminator Loss = 8.1314\n",
      "Epoch 35: Generator Loss = 16.8831, Discriminator Loss = 8.6934\n",
      "Epoch 36: Generator Loss = 14.2560, Discriminator Loss = 7.6118\n",
      "Epoch 37: Generator Loss = 16.8586, Discriminator Loss = 9.5037\n",
      "Epoch 38: Generator Loss = 16.7013, Discriminator Loss = 7.8908\n",
      "Epoch 39: Generator Loss = 19.4071, Discriminator Loss = 7.4070\n",
      "Epoch 40: Generator Loss = 23.3410, Discriminator Loss = 6.6049\n",
      "Epoch 41: Generator Loss = 24.1207, Discriminator Loss = 9.3062\n",
      "Epoch 42: Generator Loss = 18.2326, Discriminator Loss = 7.4666\n",
      "Epoch 43: Generator Loss = 16.2858, Discriminator Loss = 7.7506\n",
      "Epoch 44: Generator Loss = 22.9728, Discriminator Loss = 7.1821\n",
      "Epoch 45: Generator Loss = 19.6304, Discriminator Loss = 7.2339\n",
      "Epoch 46: Generator Loss = 25.1407, Discriminator Loss = 6.8842\n",
      "Epoch 47: Generator Loss = 23.0277, Discriminator Loss = 7.6788\n",
      "Epoch 48: Generator Loss = 21.3507, Discriminator Loss = 5.8925\n",
      "Epoch 49: Generator Loss = 24.2304, Discriminator Loss = 7.4813\n",
      "Epoch 50: Generator Loss = 19.1299, Discriminator Loss = 5.5339\n",
      "Epoch 51: Generator Loss = 24.2370, Discriminator Loss = 7.2610\n",
      "Epoch 52: Generator Loss = 23.8459, Discriminator Loss = 7.0462\n",
      "Epoch 53: Generator Loss = 30.8799, Discriminator Loss = 5.4757\n",
      "Epoch 54: Generator Loss = 24.4418, Discriminator Loss = 6.3699\n",
      "Epoch 55: Generator Loss = 26.4407, Discriminator Loss = 5.5612\n",
      "Epoch 56: Generator Loss = 24.0109, Discriminator Loss = 5.8633\n",
      "Epoch 57: Generator Loss = 26.9572, Discriminator Loss = 4.1146\n",
      "Epoch 58: Generator Loss = 33.6010, Discriminator Loss = 3.6470\n",
      "Epoch 59: Generator Loss = 28.6847, Discriminator Loss = 3.4078\n",
      "Epoch 60: Generator Loss = 34.3088, Discriminator Loss = 4.7574\n",
      "Epoch 61: Generator Loss = 27.8131, Discriminator Loss = 5.3961\n",
      "Epoch 62: Generator Loss = 28.6881, Discriminator Loss = 4.9243\n",
      "Epoch 63: Generator Loss = 30.8430, Discriminator Loss = 3.6901\n",
      "Epoch 64: Generator Loss = 29.9173, Discriminator Loss = 3.1830\n",
      "Epoch 65: Generator Loss = 35.6717, Discriminator Loss = 3.7019\n",
      "Epoch 66: Generator Loss = 43.1892, Discriminator Loss = 4.4453\n",
      "Epoch 67: Generator Loss = 28.1587, Discriminator Loss = 4.6141\n",
      "Epoch 68: Generator Loss = 31.1087, Discriminator Loss = 3.8977\n",
      "Epoch 69: Generator Loss = 36.9192, Discriminator Loss = 3.8742\n",
      "Epoch 70: Generator Loss = 40.3701, Discriminator Loss = 3.5343\n",
      "Epoch 71: Generator Loss = 43.3349, Discriminator Loss = 2.7041\n",
      "Epoch 72: Generator Loss = 41.2642, Discriminator Loss = 3.1770\n",
      "Epoch 73: Generator Loss = 46.9400, Discriminator Loss = 3.2273\n",
      "Epoch 74: Generator Loss = 42.8866, Discriminator Loss = 1.8698\n",
      "Epoch 75: Generator Loss = 52.1969, Discriminator Loss = 1.7937\n",
      "Epoch 76: Generator Loss = 50.0735, Discriminator Loss = 2.0640\n",
      "Epoch 77: Generator Loss = 50.4366, Discriminator Loss = 1.6842\n",
      "Epoch 78: Generator Loss = 47.5877, Discriminator Loss = 2.9336\n",
      "Epoch 79: Generator Loss = 39.4137, Discriminator Loss = 2.5842\n",
      "Epoch 80: Generator Loss = 62.4252, Discriminator Loss = 1.5086\n",
      "Epoch 81: Generator Loss = 52.8368, Discriminator Loss = 2.0274\n",
      "Epoch 82: Generator Loss = 56.2733, Discriminator Loss = 1.9756\n",
      "Epoch 83: Generator Loss = 45.0938, Discriminator Loss = 3.8943\n",
      "Epoch 84: Generator Loss = 47.0474, Discriminator Loss = 1.5248\n",
      "Epoch 85: Generator Loss = 61.3204, Discriminator Loss = 1.9499\n",
      "Epoch 86: Generator Loss = 42.7525, Discriminator Loss = 5.4336\n",
      "Epoch 87: Generator Loss = 52.3166, Discriminator Loss = 1.8294\n",
      "Epoch 88: Generator Loss = 46.1312, Discriminator Loss = 1.8971\n",
      "Epoch 89: Generator Loss = 53.1601, Discriminator Loss = 2.2535\n",
      "Epoch 90: Generator Loss = 54.3772, Discriminator Loss = 1.8937\n",
      "Epoch 91: Generator Loss = 69.6402, Discriminator Loss = 1.3455\n",
      "Epoch 92: Generator Loss = 59.6052, Discriminator Loss = 1.5092\n",
      "Epoch 93: Generator Loss = 56.9208, Discriminator Loss = 1.1154\n",
      "Epoch 94: Generator Loss = 60.6779, Discriminator Loss = 1.3848\n",
      "Epoch 95: Generator Loss = 49.8748, Discriminator Loss = 1.4630\n",
      "Epoch 96: Generator Loss = 59.4311, Discriminator Loss = 1.1833\n",
      "Epoch 97: Generator Loss = 62.8264, Discriminator Loss = 1.1486\n",
      "Epoch 98: Generator Loss = 58.4926, Discriminator Loss = 2.5762\n",
      "Epoch 99: Generator Loss = 59.4981, Discriminator Loss = 2.5480\n",
      "Epoch 100: Generator Loss = 72.3327, Discriminator Loss = 1.1588\n",
      "Epoch 101: Generator Loss = 63.4392, Discriminator Loss = 0.9466\n",
      "Epoch 102: Generator Loss = 58.2489, Discriminator Loss = 1.2192\n",
      "Epoch 103: Generator Loss = 59.3031, Discriminator Loss = 0.9615\n",
      "Epoch 104: Generator Loss = 73.6126, Discriminator Loss = 0.5370\n",
      "Epoch 105: Generator Loss = 63.4072, Discriminator Loss = 0.7750\n",
      "Epoch 106: Generator Loss = 55.6860, Discriminator Loss = 1.4292\n",
      "Epoch 107: Generator Loss = 70.7825, Discriminator Loss = 0.8624\n",
      "Epoch 108: Generator Loss = 75.7040, Discriminator Loss = 0.7155\n",
      "Epoch 109: Generator Loss = 63.6788, Discriminator Loss = 1.1153\n",
      "Epoch 110: Generator Loss = 58.5165, Discriminator Loss = 3.6178\n",
      "Epoch 111: Generator Loss = 61.0592, Discriminator Loss = 1.6053\n",
      "Epoch 112: Generator Loss = 67.7064, Discriminator Loss = 1.5665\n",
      "Epoch 113: Generator Loss = 58.1678, Discriminator Loss = 2.5561\n",
      "Epoch 114: Generator Loss = 68.8196, Discriminator Loss = 1.8575\n",
      "Epoch 115: Generator Loss = 65.8626, Discriminator Loss = 0.8073\n",
      "Epoch 116: Generator Loss = 57.9249, Discriminator Loss = 0.9700\n",
      "Epoch 117: Generator Loss = 76.0431, Discriminator Loss = 0.6886\n",
      "Epoch 118: Generator Loss = 68.9043, Discriminator Loss = 0.7846\n",
      "Epoch 119: Generator Loss = 87.0544, Discriminator Loss = 0.8193\n",
      "Epoch 120: Generator Loss = 65.9877, Discriminator Loss = 1.5280\n",
      "Epoch 121: Generator Loss = 76.1589, Discriminator Loss = 0.7012\n",
      "Epoch 122: Generator Loss = 74.8700, Discriminator Loss = 1.2095\n",
      "Epoch 123: Generator Loss = 74.9242, Discriminator Loss = 0.6686\n",
      "Epoch 124: Generator Loss = 59.9504, Discriminator Loss = 1.3972\n",
      "Epoch 125: Generator Loss = 71.7981, Discriminator Loss = 1.2307\n",
      "Epoch 126: Generator Loss = 80.5962, Discriminator Loss = 0.5380\n",
      "Epoch 127: Generator Loss = 78.6360, Discriminator Loss = 0.5978\n",
      "Epoch 128: Generator Loss = 82.0071, Discriminator Loss = 1.7219\n",
      "Epoch 129: Generator Loss = 70.8049, Discriminator Loss = 0.9624\n",
      "Epoch 130: Generator Loss = 64.8281, Discriminator Loss = 1.0082\n",
      "Epoch 131: Generator Loss = 69.2040, Discriminator Loss = 1.0515\n",
      "Epoch 132: Generator Loss = 75.8430, Discriminator Loss = 0.6987\n",
      "Epoch 133: Generator Loss = 80.7140, Discriminator Loss = 0.4809\n",
      "Epoch 134: Generator Loss = 74.3464, Discriminator Loss = 0.5346\n",
      "Epoch 135: Generator Loss = 89.8981, Discriminator Loss = 0.8682\n",
      "Epoch 136: Generator Loss = 67.2680, Discriminator Loss = 0.9364\n",
      "Epoch 137: Generator Loss = 74.6395, Discriminator Loss = 0.8806\n",
      "Epoch 138: Generator Loss = 60.1188, Discriminator Loss = 9.5321\n",
      "Epoch 139: Generator Loss = 44.7613, Discriminator Loss = 3.6185\n",
      "Epoch 140: Generator Loss = 65.6664, Discriminator Loss = 1.8286\n",
      "Epoch 141: Generator Loss = 61.6158, Discriminator Loss = 1.1413\n",
      "Epoch 142: Generator Loss = 60.7146, Discriminator Loss = 0.9724\n",
      "Epoch 143: Generator Loss = 85.3766, Discriminator Loss = 0.9704\n",
      "Epoch 144: Generator Loss = 77.9039, Discriminator Loss = 0.7197\n",
      "Epoch 145: Generator Loss = 67.7407, Discriminator Loss = 0.9791\n",
      "Epoch 146: Generator Loss = 88.8635, Discriminator Loss = 0.5401\n",
      "Epoch 147: Generator Loss = 78.3109, Discriminator Loss = 0.8225\n",
      "Epoch 148: Generator Loss = 66.7880, Discriminator Loss = 0.9772\n",
      "Epoch 149: Generator Loss = 71.5992, Discriminator Loss = 3.7098\n",
      "Epoch 150: Generator Loss = 52.4753, Discriminator Loss = 1.3259\n",
      "Epoch 151: Generator Loss = 76.2059, Discriminator Loss = 1.0775\n",
      "Epoch 152: Generator Loss = 74.2157, Discriminator Loss = 0.5859\n",
      "Epoch 153: Generator Loss = 65.0220, Discriminator Loss = 1.3553\n",
      "Epoch 154: Generator Loss = 61.3135, Discriminator Loss = 0.7321\n",
      "Epoch 155: Generator Loss = 70.4825, Discriminator Loss = 0.6953\n",
      "Epoch 156: Generator Loss = 64.3567, Discriminator Loss = 0.4918\n",
      "Epoch 157: Generator Loss = 92.8949, Discriminator Loss = 0.3047\n",
      "Epoch 158: Generator Loss = 85.1256, Discriminator Loss = 0.6686\n",
      "Epoch 159: Generator Loss = 83.8886, Discriminator Loss = 0.5315\n",
      "Epoch 160: Generator Loss = 63.1531, Discriminator Loss = 0.6051\n",
      "Epoch 161: Generator Loss = 64.6986, Discriminator Loss = 16.3160\n",
      "Epoch 162: Generator Loss = 73.9983, Discriminator Loss = 5.2673\n",
      "Epoch 163: Generator Loss = 52.2335, Discriminator Loss = 2.7483\n",
      "Epoch 164: Generator Loss = 69.0078, Discriminator Loss = 1.4187\n",
      "Epoch 165: Generator Loss = 78.0334, Discriminator Loss = 0.9397\n",
      "Epoch 166: Generator Loss = 63.2059, Discriminator Loss = 0.8471\n",
      "Epoch 167: Generator Loss = 63.8819, Discriminator Loss = 0.4221\n",
      "Epoch 168: Generator Loss = 85.6582, Discriminator Loss = 0.5776\n",
      "Epoch 169: Generator Loss = 72.0209, Discriminator Loss = 0.6083\n",
      "Epoch 170: Generator Loss = 67.4706, Discriminator Loss = 0.8554\n",
      "Epoch 171: Generator Loss = 73.9613, Discriminator Loss = 0.9099\n",
      "Epoch 172: Generator Loss = 72.2838, Discriminator Loss = 0.5444\n",
      "Epoch 173: Generator Loss = 90.8594, Discriminator Loss = 0.3235\n",
      "Epoch 174: Generator Loss = 69.7590, Discriminator Loss = 0.6578\n",
      "Epoch 175: Generator Loss = 75.5242, Discriminator Loss = 0.5120\n",
      "Epoch 176: Generator Loss = 90.3981, Discriminator Loss = 0.7471\n",
      "Epoch 177: Generator Loss = 74.7890, Discriminator Loss = 0.4694\n",
      "Epoch 178: Generator Loss = 83.1057, Discriminator Loss = 0.3681\n",
      "Epoch 179: Generator Loss = 96.1086, Discriminator Loss = 0.3376\n",
      "Epoch 180: Generator Loss = 85.7964, Discriminator Loss = 0.5123\n",
      "Epoch 181: Generator Loss = 103.2652, Discriminator Loss = 1.7327\n",
      "Epoch 182: Generator Loss = 88.5380, Discriminator Loss = 0.7416\n",
      "Epoch 183: Generator Loss = 80.4451, Discriminator Loss = 0.5034\n",
      "Epoch 184: Generator Loss = 96.7836, Discriminator Loss = 0.3656\n",
      "Epoch 185: Generator Loss = 87.1157, Discriminator Loss = 0.7091\n",
      "Epoch 186: Generator Loss = 64.6347, Discriminator Loss = 1.2933\n",
      "Epoch 187: Generator Loss = 80.8216, Discriminator Loss = 0.5200\n",
      "Epoch 188: Generator Loss = 90.4298, Discriminator Loss = 0.3665\n",
      "Epoch 189: Generator Loss = 83.5777, Discriminator Loss = 0.5198\n",
      "Epoch 190: Generator Loss = 90.5798, Discriminator Loss = 2.3975\n",
      "Epoch 191: Generator Loss = 80.9146, Discriminator Loss = 1.9550\n",
      "Epoch 192: Generator Loss = 68.4717, Discriminator Loss = 0.8229\n",
      "Epoch 193: Generator Loss = 78.0993, Discriminator Loss = 0.5195\n",
      "Epoch 194: Generator Loss = 79.0043, Discriminator Loss = 0.4443\n",
      "Epoch 195: Generator Loss = 85.4105, Discriminator Loss = 0.3844\n",
      "Epoch 196: Generator Loss = 86.1603, Discriminator Loss = 1.1206\n",
      "Epoch 197: Generator Loss = 82.4971, Discriminator Loss = 0.6721\n",
      "Epoch 198: Generator Loss = 89.3795, Discriminator Loss = 0.6403\n",
      "Epoch 199: Generator Loss = 88.1221, Discriminator Loss = 0.3708\n",
      "Epoch 200: Generator Loss = 89.1429, Discriminator Loss = 0.4014\n",
      "Epoch 201: Generator Loss = 86.9221, Discriminator Loss = 0.5229\n",
      "Epoch 202: Generator Loss = 88.6437, Discriminator Loss = 0.6412\n",
      "Epoch 203: Generator Loss = 79.3675, Discriminator Loss = 0.6008\n",
      "Epoch 204: Generator Loss = 112.7619, Discriminator Loss = 0.4376\n",
      "Epoch 205: Generator Loss = 90.3958, Discriminator Loss = 0.4153\n",
      "Epoch 206: Generator Loss = 87.9165, Discriminator Loss = 0.2469\n",
      "Epoch 207: Generator Loss = 97.0939, Discriminator Loss = 1.7436\n",
      "Epoch 208: Generator Loss = 79.8740, Discriminator Loss = 1.0916\n",
      "Epoch 209: Generator Loss = 90.1657, Discriminator Loss = 0.9167\n",
      "Epoch 210: Generator Loss = 77.0825, Discriminator Loss = 0.7797\n",
      "Epoch 211: Generator Loss = 68.0903, Discriminator Loss = 0.4241\n",
      "Epoch 212: Generator Loss = 85.5656, Discriminator Loss = 0.4837\n",
      "Epoch 213: Generator Loss = 88.3516, Discriminator Loss = 0.8737\n",
      "Epoch 214: Generator Loss = 66.9068, Discriminator Loss = 3.5317\n",
      "Epoch 215: Generator Loss = 74.4302, Discriminator Loss = 0.8405\n",
      "Epoch 216: Generator Loss = 90.8202, Discriminator Loss = 0.6278\n",
      "Epoch 217: Generator Loss = 96.1314, Discriminator Loss = 0.7084\n",
      "Epoch 218: Generator Loss = 87.0503, Discriminator Loss = 1.5603\n",
      "Epoch 219: Generator Loss = 94.3307, Discriminator Loss = 0.2862\n",
      "Epoch 220: Generator Loss = 84.4582, Discriminator Loss = 1.6549\n",
      "Epoch 221: Generator Loss = 79.7010, Discriminator Loss = 0.8635\n",
      "Epoch 222: Generator Loss = 91.4875, Discriminator Loss = 0.3562\n",
      "Epoch 223: Generator Loss = 92.3479, Discriminator Loss = 0.5606\n",
      "Epoch 224: Generator Loss = 84.6223, Discriminator Loss = 0.3558\n",
      "Epoch 225: Generator Loss = 94.6585, Discriminator Loss = 0.4991\n",
      "Epoch 226: Generator Loss = 84.8239, Discriminator Loss = 0.4919\n",
      "Epoch 227: Generator Loss = 93.5738, Discriminator Loss = 0.4706\n",
      "Epoch 228: Generator Loss = 92.2969, Discriminator Loss = 0.5436\n",
      "Epoch 229: Generator Loss = 129.2200, Discriminator Loss = 0.3820\n",
      "Epoch 230: Generator Loss = 113.5009, Discriminator Loss = 1.3623\n",
      "Epoch 231: Generator Loss = 86.6428, Discriminator Loss = 0.5766\n",
      "Epoch 232: Generator Loss = 100.2886, Discriminator Loss = 0.6726\n",
      "Epoch 233: Generator Loss = 73.8699, Discriminator Loss = 0.1538\n",
      "Epoch 234: Generator Loss = 95.4770, Discriminator Loss = 0.2284\n",
      "Epoch 235: Generator Loss = 95.5056, Discriminator Loss = 0.8524\n",
      "Epoch 236: Generator Loss = 90.4983, Discriminator Loss = 1.5987\n",
      "Epoch 237: Generator Loss = 85.7151, Discriminator Loss = 22.4566\n",
      "Epoch 238: Generator Loss = 71.3577, Discriminator Loss = 16.0013\n",
      "Epoch 239: Generator Loss = 44.9816, Discriminator Loss = 6.7142\n",
      "Epoch 240: Generator Loss = 42.5208, Discriminator Loss = 3.4424\n",
      "Epoch 241: Generator Loss = 66.5555, Discriminator Loss = 1.8838\n",
      "Epoch 242: Generator Loss = 62.8487, Discriminator Loss = 2.2241\n",
      "Epoch 243: Generator Loss = 86.2783, Discriminator Loss = 7.5294\n",
      "Epoch 244: Generator Loss = 59.6422, Discriminator Loss = 3.1995\n",
      "Epoch 245: Generator Loss = 51.0420, Discriminator Loss = 1.7305\n",
      "Epoch 246: Generator Loss = 64.1175, Discriminator Loss = 1.0384\n",
      "Epoch 247: Generator Loss = 69.5446, Discriminator Loss = 1.0072\n",
      "Epoch 248: Generator Loss = 69.3792, Discriminator Loss = 1.7769\n",
      "Epoch 249: Generator Loss = 58.1510, Discriminator Loss = 1.0935\n",
      "Epoch 250: Generator Loss = 74.1519, Discriminator Loss = 1.1000\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.6274 Acc: 0.7154\n",
      "Epoch 2/15 | Train Loss: 0.3882 Acc: 0.7963\n",
      "Epoch 3/15 | Train Loss: 0.3453 Acc: 0.8329\n",
      "Epoch 4/15 | Train Loss: 0.3855 Acc: 0.8016\n",
      "Epoch 5/15 | Train Loss: 0.3306 Acc: 0.8277\n",
      "Epoch 6/15 | Train Loss: 0.3407 Acc: 0.8277\n",
      "Epoch 7/15 | Train Loss: 0.2988 Acc: 0.8695\n",
      "Epoch 8/15 | Train Loss: 0.2946 Acc: 0.8695\n",
      "Epoch 9/15 | Train Loss: 0.2723 Acc: 0.8721\n",
      "Epoch 10/15 | Train Loss: 0.2737 Acc: 0.8773\n",
      "Epoch 11/15 | Train Loss: 0.2307 Acc: 0.8851\n",
      "Epoch 12/15 | Train Loss: 0.2455 Acc: 0.9060\n",
      "Epoch 13/15 | Train Loss: 0.2263 Acc: 0.9164\n",
      "Epoch 14/15 | Train Loss: 0.2188 Acc: 0.8903\n",
      "Epoch 15/15 | Train Loss: 0.2452 Acc: 0.8799\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4933 Acc: 0.7755\n",
      "Epoch 2/15 | Train Loss: 0.4052 Acc: 0.7885\n",
      "Epoch 3/15 | Train Loss: 0.3705 Acc: 0.7781\n",
      "Epoch 4/15 | Train Loss: 0.3539 Acc: 0.8120\n",
      "Epoch 5/15 | Train Loss: 0.3452 Acc: 0.8355\n",
      "Epoch 6/15 | Train Loss: 0.3362 Acc: 0.8120\n",
      "Epoch 7/15 | Train Loss: 0.3224 Acc: 0.8355\n",
      "Epoch 8/15 | Train Loss: 0.2635 Acc: 0.8799\n",
      "Epoch 9/15 | Train Loss: 0.3284 Acc: 0.8355\n",
      "Epoch 10/15 | Train Loss: 0.2576 Acc: 0.8642\n",
      "Epoch 11/15 | Train Loss: 0.2847 Acc: 0.8668\n",
      "Epoch 12/15 | Train Loss: 0.2578 Acc: 0.8642\n",
      "Epoch 13/15 | Train Loss: 0.2708 Acc: 0.8590\n",
      "Epoch 14/15 | Train Loss: 0.2945 Acc: 0.8590\n",
      "Epoch 15/15 | Train Loss: 0.2753 Acc: 0.8773\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5282 Acc: 0.7578\n",
      "Epoch 2/15 | Train Loss: 0.4383 Acc: 0.7760\n",
      "Epoch 3/15 | Train Loss: 0.3845 Acc: 0.7969\n",
      "Epoch 4/15 | Train Loss: 0.3778 Acc: 0.8047\n",
      "Epoch 5/15 | Train Loss: 0.3308 Acc: 0.8359\n",
      "Epoch 6/15 | Train Loss: 0.3294 Acc: 0.8411\n",
      "Epoch 7/15 | Train Loss: 0.3232 Acc: 0.8281\n",
      "Epoch 8/15 | Train Loss: 0.2687 Acc: 0.8958\n",
      "Epoch 9/15 | Train Loss: 0.3110 Acc: 0.8411\n",
      "Epoch 10/15 | Train Loss: 0.2686 Acc: 0.8958\n",
      "Epoch 11/15 | Train Loss: 0.2545 Acc: 0.8880\n",
      "Epoch 12/15 | Train Loss: 0.2791 Acc: 0.8646\n",
      "Epoch 13/15 | Train Loss: 0.2768 Acc: 0.8672\n",
      "Epoch 14/15 | Train Loss: 0.2714 Acc: 0.8646\n",
      "Epoch 15/15 | Train Loss: 0.2649 Acc: 0.8594\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5443 Acc: 0.7898\n",
      "Epoch 2/15 | Train Loss: 0.4039 Acc: 0.8057\n",
      "Epoch 3/15 | Train Loss: 0.4000 Acc: 0.8280\n",
      "Epoch 4/15 | Train Loss: 0.3949 Acc: 0.8153\n",
      "Epoch 5/15 | Train Loss: 0.4017 Acc: 0.8217\n",
      "Epoch 6/15 | Train Loss: 0.3482 Acc: 0.8503\n",
      "Epoch 7/15 | Train Loss: 0.3164 Acc: 0.8567\n",
      "Epoch 8/15 | Train Loss: 0.3197 Acc: 0.8535\n",
      "Epoch 9/15 | Train Loss: 0.2806 Acc: 0.8854\n",
      "Epoch 10/15 | Train Loss: 0.2810 Acc: 0.8854\n",
      "Epoch 11/15 | Train Loss: 0.3045 Acc: 0.8854\n",
      "Epoch 12/15 | Train Loss: 0.2904 Acc: 0.8694\n",
      "Epoch 13/15 | Train Loss: 0.2647 Acc: 0.8694\n",
      "Epoch 14/15 | Train Loss: 0.3073 Acc: 0.8439\n",
      "Epoch 15/15 | Train Loss: 0.2927 Acc: 0.8885\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4786 Acc: 0.7893\n",
      "Epoch 2/15 | Train Loss: 0.3605 Acc: 0.8679\n",
      "Epoch 3/15 | Train Loss: 0.3282 Acc: 0.8714\n",
      "Epoch 4/15 | Train Loss: 0.3250 Acc: 0.8750\n",
      "Epoch 5/15 | Train Loss: 0.2877 Acc: 0.8821\n",
      "Epoch 6/15 | Train Loss: 0.2264 Acc: 0.9107\n",
      "Epoch 7/15 | Train Loss: 0.2731 Acc: 0.8893\n",
      "Epoch 8/15 | Train Loss: 0.2818 Acc: 0.8821\n",
      "Epoch 9/15 | Train Loss: 0.2421 Acc: 0.8929\n",
      "Epoch 10/15 | Train Loss: 0.2669 Acc: 0.8893\n",
      "Epoch 11/15 | Train Loss: 0.2275 Acc: 0.9036\n",
      "Epoch 12/15 | Train Loss: 0.2189 Acc: 0.9107\n",
      "Epoch 13/15 | Train Loss: 0.2217 Acc: 0.9250\n",
      "Epoch 14/15 | Train Loss: 0.2116 Acc: 0.9286\n",
      "Epoch 15/15 | Train Loss: 0.2257 Acc: 0.9107\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7073 Acc: 0.6389\n",
      "Epoch 2/15 | Train Loss: 0.6151 Acc: 0.7103\n",
      "Epoch 3/15 | Train Loss: 0.5183 Acc: 0.7381\n",
      "Epoch 4/15 | Train Loss: 0.5154 Acc: 0.7421\n",
      "Epoch 5/15 | Train Loss: 0.4821 Acc: 0.7500\n",
      "Epoch 6/15 | Train Loss: 0.4810 Acc: 0.7937\n",
      "Epoch 7/15 | Train Loss: 0.4361 Acc: 0.7698\n",
      "Epoch 8/15 | Train Loss: 0.4614 Acc: 0.7897\n",
      "Epoch 9/15 | Train Loss: 0.4319 Acc: 0.7738\n",
      "Epoch 10/15 | Train Loss: 0.3553 Acc: 0.8532\n",
      "Epoch 11/15 | Train Loss: 0.3885 Acc: 0.8254\n",
      "Epoch 12/15 | Train Loss: 0.3534 Acc: 0.8492\n",
      "Epoch 13/15 | Train Loss: 0.3224 Acc: 0.8690\n",
      "Epoch 14/15 | Train Loss: 0.3619 Acc: 0.8571\n",
      "Epoch 15/15 | Train Loss: 0.3496 Acc: 0.8730\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6692 Acc: 0.6587\n",
      "Epoch 2/15 | Train Loss: 0.6486 Acc: 0.6468\n",
      "Epoch 3/15 | Train Loss: 0.5601 Acc: 0.7222\n",
      "Epoch 4/15 | Train Loss: 0.5417 Acc: 0.7103\n",
      "Epoch 5/15 | Train Loss: 0.4811 Acc: 0.7540\n",
      "Epoch 6/15 | Train Loss: 0.5046 Acc: 0.7659\n",
      "Epoch 7/15 | Train Loss: 0.4416 Acc: 0.8016\n",
      "Epoch 8/15 | Train Loss: 0.3823 Acc: 0.8373\n",
      "Epoch 9/15 | Train Loss: 0.4809 Acc: 0.7619\n",
      "Epoch 10/15 | Train Loss: 0.4344 Acc: 0.7857\n",
      "Epoch 11/15 | Train Loss: 0.4131 Acc: 0.8016\n",
      "Epoch 12/15 | Train Loss: 0.3899 Acc: 0.8175\n",
      "Epoch 13/15 | Train Loss: 0.3586 Acc: 0.8333\n",
      "Epoch 14/15 | Train Loss: 0.3847 Acc: 0.8135\n",
      "Epoch 15/15 | Train Loss: 0.3922 Acc: 0.8056\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6666 Acc: 0.6719\n",
      "Epoch 2/15 | Train Loss: 0.5822 Acc: 0.6759\n",
      "Epoch 3/15 | Train Loss: 0.5187 Acc: 0.7391\n",
      "Epoch 4/15 | Train Loss: 0.5947 Acc: 0.6601\n",
      "Epoch 5/15 | Train Loss: 0.5460 Acc: 0.7352\n",
      "Epoch 6/15 | Train Loss: 0.5198 Acc: 0.7549\n",
      "Epoch 7/15 | Train Loss: 0.4633 Acc: 0.7589\n",
      "Epoch 8/15 | Train Loss: 0.4927 Acc: 0.7549\n",
      "Epoch 9/15 | Train Loss: 0.4186 Acc: 0.8221\n",
      "Epoch 10/15 | Train Loss: 0.3571 Acc: 0.8379\n",
      "Epoch 11/15 | Train Loss: 0.4066 Acc: 0.8221\n",
      "Epoch 12/15 | Train Loss: 0.4154 Acc: 0.8221\n",
      "Epoch 13/15 | Train Loss: 0.4137 Acc: 0.8221\n",
      "Epoch 14/15 | Train Loss: 0.3576 Acc: 0.8696\n",
      "Epoch 15/15 | Train Loss: 0.3941 Acc: 0.8379\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6775 Acc: 0.6568\n",
      "Epoch 2/15 | Train Loss: 0.5182 Acc: 0.7881\n",
      "Epoch 3/15 | Train Loss: 0.5383 Acc: 0.7585\n",
      "Epoch 4/15 | Train Loss: 0.4600 Acc: 0.7924\n",
      "Epoch 5/15 | Train Loss: 0.4565 Acc: 0.7754\n",
      "Epoch 6/15 | Train Loss: 0.4455 Acc: 0.8136\n",
      "Epoch 7/15 | Train Loss: 0.4313 Acc: 0.8093\n",
      "Epoch 8/15 | Train Loss: 0.4641 Acc: 0.7839\n",
      "Epoch 9/15 | Train Loss: 0.4447 Acc: 0.8008\n",
      "Epoch 10/15 | Train Loss: 0.3802 Acc: 0.8305\n",
      "Epoch 11/15 | Train Loss: 0.4069 Acc: 0.8559\n",
      "Epoch 12/15 | Train Loss: 0.4092 Acc: 0.8305\n",
      "Epoch 13/15 | Train Loss: 0.3785 Acc: 0.8432\n",
      "Epoch 14/15 | Train Loss: 0.4179 Acc: 0.8178\n",
      "Epoch 15/15 | Train Loss: 0.4159 Acc: 0.8347\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5646 Acc: 0.7137\n",
      "Epoch 2/15 | Train Loss: 0.3961 Acc: 0.8282\n",
      "Epoch 3/15 | Train Loss: 0.4633 Acc: 0.8194\n",
      "Epoch 4/15 | Train Loss: 0.3599 Acc: 0.8546\n",
      "Epoch 5/15 | Train Loss: 0.4260 Acc: 0.8370\n",
      "Epoch 6/15 | Train Loss: 0.3640 Acc: 0.8546\n",
      "Epoch 7/15 | Train Loss: 0.3894 Acc: 0.8414\n",
      "Epoch 8/15 | Train Loss: 0.3401 Acc: 0.8811\n",
      "Epoch 9/15 | Train Loss: 0.3338 Acc: 0.8722\n",
      "Epoch 10/15 | Train Loss: 0.2386 Acc: 0.9163\n",
      "Epoch 11/15 | Train Loss: 0.2567 Acc: 0.9075\n",
      "Epoch 12/15 | Train Loss: 0.2648 Acc: 0.8899\n",
      "Epoch 13/15 | Train Loss: 0.2636 Acc: 0.8987\n",
      "Epoch 14/15 | Train Loss: 0.2714 Acc: 0.8722\n",
      "Epoch 15/15 | Train Loss: 0.2709 Acc: 0.8987\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.5min\n",
      "Epoch 1/15 | Train Loss: 0.7044 Acc: 0.6689\n",
      "Epoch 2/15 | Train Loss: 0.5415 Acc: 0.7534\n",
      "Epoch 3/15 | Train Loss: 0.4752 Acc: 0.7804\n",
      "Epoch 4/15 | Train Loss: 0.5233 Acc: 0.7162\n",
      "Epoch 5/15 | Train Loss: 0.4371 Acc: 0.7905\n",
      "Epoch 6/15 | Train Loss: 0.3892 Acc: 0.8041\n",
      "Epoch 7/15 | Train Loss: 0.3468 Acc: 0.8345\n",
      "Epoch 8/15 | Train Loss: 0.3963 Acc: 0.8176\n",
      "Epoch 9/15 | Train Loss: 0.3460 Acc: 0.8480\n",
      "Epoch 10/15 | Train Loss: 0.3339 Acc: 0.8446\n",
      "Epoch 11/15 | Train Loss: 0.3238 Acc: 0.8750\n",
      "Epoch 12/15 | Train Loss: 0.2970 Acc: 0.8649\n",
      "Epoch 13/15 | Train Loss: 0.3222 Acc: 0.8547\n",
      "Epoch 14/15 | Train Loss: 0.2973 Acc: 0.8851\n",
      "Epoch 15/15 | Train Loss: 0.2845 Acc: 0.8750\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5661 Acc: 0.6892\n",
      "Epoch 2/15 | Train Loss: 0.4903 Acc: 0.7432\n",
      "Epoch 3/15 | Train Loss: 0.4514 Acc: 0.7365\n",
      "Epoch 4/15 | Train Loss: 0.4147 Acc: 0.7804\n",
      "Epoch 5/15 | Train Loss: 0.4620 Acc: 0.7601\n",
      "Epoch 6/15 | Train Loss: 0.4235 Acc: 0.7500\n",
      "Epoch 7/15 | Train Loss: 0.4086 Acc: 0.7939\n",
      "Epoch 8/15 | Train Loss: 0.4065 Acc: 0.7872\n",
      "Epoch 9/15 | Train Loss: 0.3599 Acc: 0.8209\n",
      "Epoch 10/15 | Train Loss: 0.3947 Acc: 0.7973\n",
      "Epoch 11/15 | Train Loss: 0.3475 Acc: 0.8176\n",
      "Epoch 12/15 | Train Loss: 0.3347 Acc: 0.8480\n",
      "Epoch 13/15 | Train Loss: 0.3464 Acc: 0.8311\n",
      "Epoch 14/15 | Train Loss: 0.3627 Acc: 0.8277\n",
      "Epoch 15/15 | Train Loss: 0.3963 Acc: 0.8041\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6539 Acc: 0.6330\n",
      "Epoch 2/15 | Train Loss: 0.4966 Acc: 0.7475\n",
      "Epoch 3/15 | Train Loss: 0.5407 Acc: 0.7037\n",
      "Epoch 4/15 | Train Loss: 0.4708 Acc: 0.7576\n",
      "Epoch 5/15 | Train Loss: 0.4031 Acc: 0.8081\n",
      "Epoch 6/15 | Train Loss: 0.4177 Acc: 0.7845\n",
      "Epoch 7/15 | Train Loss: 0.4063 Acc: 0.8047\n",
      "Epoch 8/15 | Train Loss: 0.3570 Acc: 0.8182\n",
      "Epoch 9/15 | Train Loss: 0.3622 Acc: 0.8182\n",
      "Epoch 10/15 | Train Loss: 0.3214 Acc: 0.8552\n",
      "Epoch 11/15 | Train Loss: 0.2970 Acc: 0.8788\n",
      "Epoch 12/15 | Train Loss: 0.3178 Acc: 0.8316\n",
      "Epoch 13/15 | Train Loss: 0.3340 Acc: 0.8114\n",
      "Epoch 14/15 | Train Loss: 0.3227 Acc: 0.8451\n",
      "Epoch 15/15 | Train Loss: 0.3459 Acc: 0.8316\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5682 Acc: 0.7443\n",
      "Epoch 2/15 | Train Loss: 0.4721 Acc: 0.8015\n",
      "Epoch 3/15 | Train Loss: 0.4434 Acc: 0.8168\n",
      "Epoch 4/15 | Train Loss: 0.4642 Acc: 0.7824\n",
      "Epoch 5/15 | Train Loss: 0.4788 Acc: 0.7710\n",
      "Epoch 6/15 | Train Loss: 0.3875 Acc: 0.8206\n",
      "Epoch 7/15 | Train Loss: 0.4046 Acc: 0.8244\n",
      "Epoch 8/15 | Train Loss: 0.3565 Acc: 0.8321\n",
      "Epoch 9/15 | Train Loss: 0.3240 Acc: 0.8626\n",
      "Epoch 10/15 | Train Loss: 0.3816 Acc: 0.8168\n",
      "Epoch 11/15 | Train Loss: 0.3311 Acc: 0.8473\n",
      "Epoch 12/15 | Train Loss: 0.3553 Acc: 0.8511\n",
      "Epoch 13/15 | Train Loss: 0.3718 Acc: 0.8359\n",
      "Epoch 14/15 | Train Loss: 0.3715 Acc: 0.8244\n",
      "Epoch 15/15 | Train Loss: 0.3292 Acc: 0.8550\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6108 Acc: 0.6898\n",
      "Epoch 2/15 | Train Loss: 0.4082 Acc: 0.8449\n",
      "Epoch 3/15 | Train Loss: 0.3990 Acc: 0.8612\n",
      "Epoch 4/15 | Train Loss: 0.3450 Acc: 0.8612\n",
      "Epoch 5/15 | Train Loss: 0.3530 Acc: 0.8653\n",
      "Epoch 6/15 | Train Loss: 0.3464 Acc: 0.8571\n",
      "Epoch 7/15 | Train Loss: 0.3374 Acc: 0.8571\n",
      "Epoch 8/15 | Train Loss: 0.3086 Acc: 0.8816\n",
      "Epoch 9/15 | Train Loss: 0.2663 Acc: 0.8939\n",
      "Epoch 10/15 | Train Loss: 0.2603 Acc: 0.9020\n",
      "Epoch 11/15 | Train Loss: 0.2775 Acc: 0.8980\n",
      "Epoch 12/15 | Train Loss: 0.2866 Acc: 0.8531\n",
      "Epoch 13/15 | Train Loss: 0.2542 Acc: 0.8939\n",
      "Epoch 14/15 | Train Loss: 0.2434 Acc: 0.9102\n",
      "Epoch 15/15 | Train Loss: 0.3019 Acc: 0.8694\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7334 Acc: 0.6429\n",
      "Epoch 2/15 | Train Loss: 0.6782 Acc: 0.6429\n",
      "Epoch 3/15 | Train Loss: 0.5382 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.5129 Acc: 0.7341\n",
      "Epoch 5/15 | Train Loss: 0.4716 Acc: 0.7579\n",
      "Epoch 6/15 | Train Loss: 0.4006 Acc: 0.7857\n",
      "Epoch 7/15 | Train Loss: 0.4198 Acc: 0.8333\n",
      "Epoch 8/15 | Train Loss: 0.3976 Acc: 0.8175\n",
      "Epoch 9/15 | Train Loss: 0.3900 Acc: 0.8214\n",
      "Epoch 10/15 | Train Loss: 0.3871 Acc: 0.8254\n",
      "Epoch 11/15 | Train Loss: 0.3452 Acc: 0.8135\n",
      "Epoch 12/15 | Train Loss: 0.3462 Acc: 0.8611\n",
      "Epoch 13/15 | Train Loss: 0.2993 Acc: 0.8770\n",
      "Epoch 14/15 | Train Loss: 0.3490 Acc: 0.8452\n",
      "Epoch 15/15 | Train Loss: 0.3097 Acc: 0.8690\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7707 Acc: 0.5952\n",
      "Epoch 2/15 | Train Loss: 0.5787 Acc: 0.7222\n",
      "Epoch 3/15 | Train Loss: 0.6423 Acc: 0.6587\n",
      "Epoch 4/15 | Train Loss: 0.5919 Acc: 0.6905\n",
      "Epoch 5/15 | Train Loss: 0.4796 Acc: 0.7897\n",
      "Epoch 6/15 | Train Loss: 0.4456 Acc: 0.7659\n",
      "Epoch 7/15 | Train Loss: 0.4897 Acc: 0.7698\n",
      "Epoch 8/15 | Train Loss: 0.4865 Acc: 0.7500\n",
      "Epoch 9/15 | Train Loss: 0.4280 Acc: 0.8175\n",
      "Epoch 10/15 | Train Loss: 0.4186 Acc: 0.8135\n",
      "Epoch 11/15 | Train Loss: 0.4272 Acc: 0.7738\n",
      "Epoch 12/15 | Train Loss: 0.4932 Acc: 0.7778\n",
      "Epoch 13/15 | Train Loss: 0.3537 Acc: 0.8452\n",
      "Epoch 14/15 | Train Loss: 0.4295 Acc: 0.8056\n",
      "Epoch 15/15 | Train Loss: 0.3875 Acc: 0.8214\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6775 Acc: 0.6047\n",
      "Epoch 2/15 | Train Loss: 0.6101 Acc: 0.6640\n",
      "Epoch 3/15 | Train Loss: 0.5192 Acc: 0.7233\n",
      "Epoch 4/15 | Train Loss: 0.5054 Acc: 0.7431\n",
      "Epoch 5/15 | Train Loss: 0.5341 Acc: 0.7194\n",
      "Epoch 6/15 | Train Loss: 0.5019 Acc: 0.7510\n",
      "Epoch 7/15 | Train Loss: 0.4537 Acc: 0.7866\n",
      "Epoch 8/15 | Train Loss: 0.4131 Acc: 0.8142\n",
      "Epoch 9/15 | Train Loss: 0.4614 Acc: 0.7549\n",
      "Epoch 10/15 | Train Loss: 0.4238 Acc: 0.7984\n",
      "Epoch 11/15 | Train Loss: 0.4704 Acc: 0.7510\n",
      "Epoch 12/15 | Train Loss: 0.4181 Acc: 0.8063\n",
      "Epoch 13/15 | Train Loss: 0.3956 Acc: 0.8261\n",
      "Epoch 14/15 | Train Loss: 0.3731 Acc: 0.8458\n",
      "Epoch 15/15 | Train Loss: 0.4215 Acc: 0.7708\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7007 Acc: 0.6314\n",
      "Epoch 2/15 | Train Loss: 0.5818 Acc: 0.7373\n",
      "Epoch 3/15 | Train Loss: 0.5239 Acc: 0.7585\n",
      "Epoch 4/15 | Train Loss: 0.4822 Acc: 0.7669\n",
      "Epoch 5/15 | Train Loss: 0.5045 Acc: 0.7754\n",
      "Epoch 6/15 | Train Loss: 0.4804 Acc: 0.7881\n",
      "Epoch 7/15 | Train Loss: 0.4283 Acc: 0.7966\n",
      "Epoch 8/15 | Train Loss: 0.4497 Acc: 0.8008\n",
      "Epoch 9/15 | Train Loss: 0.4068 Acc: 0.8347\n",
      "Epoch 10/15 | Train Loss: 0.3933 Acc: 0.8136\n",
      "Epoch 11/15 | Train Loss: 0.3875 Acc: 0.8347\n",
      "Epoch 12/15 | Train Loss: 0.4259 Acc: 0.8220\n",
      "Epoch 13/15 | Train Loss: 0.3825 Acc: 0.8178\n",
      "Epoch 14/15 | Train Loss: 0.3830 Acc: 0.8602\n",
      "Epoch 15/15 | Train Loss: 0.3743 Acc: 0.8432\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5312 Acc: 0.7357\n",
      "Epoch 2/15 | Train Loss: 0.4459 Acc: 0.8370\n",
      "Epoch 3/15 | Train Loss: 0.4522 Acc: 0.8458\n",
      "Epoch 4/15 | Train Loss: 0.3775 Acc: 0.8546\n",
      "Epoch 5/15 | Train Loss: 0.3841 Acc: 0.8370\n",
      "Epoch 6/15 | Train Loss: 0.3536 Acc: 0.8767\n",
      "Epoch 7/15 | Train Loss: 0.3273 Acc: 0.8370\n",
      "Epoch 8/15 | Train Loss: 0.2716 Acc: 0.8987\n",
      "Epoch 9/15 | Train Loss: 0.3232 Acc: 0.8899\n",
      "Epoch 10/15 | Train Loss: 0.3243 Acc: 0.8634\n",
      "Epoch 11/15 | Train Loss: 0.3223 Acc: 0.8722\n",
      "Epoch 12/15 | Train Loss: 0.2960 Acc: 0.8767\n",
      "Epoch 13/15 | Train Loss: 0.2947 Acc: 0.8811\n",
      "Epoch 14/15 | Train Loss: 0.2446 Acc: 0.9119\n",
      "Epoch 15/15 | Train Loss: 0.2870 Acc: 0.8987\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6175 Acc: 0.7546\n",
      "Epoch 2/15 | Train Loss: 0.4532 Acc: 0.7676\n",
      "Epoch 3/15 | Train Loss: 0.3837 Acc: 0.7885\n",
      "Epoch 4/15 | Train Loss: 0.3429 Acc: 0.8068\n",
      "Epoch 5/15 | Train Loss: 0.2959 Acc: 0.8773\n",
      "Epoch 6/15 | Train Loss: 0.3452 Acc: 0.8303\n",
      "Epoch 7/15 | Train Loss: 0.3283 Acc: 0.8277\n",
      "Epoch 8/15 | Train Loss: 0.2829 Acc: 0.8564\n",
      "Epoch 9/15 | Train Loss: 0.2302 Acc: 0.8851\n",
      "Epoch 10/15 | Train Loss: 0.2592 Acc: 0.8695\n",
      "Epoch 11/15 | Train Loss: 0.2581 Acc: 0.8799\n",
      "Epoch 12/15 | Train Loss: 0.2478 Acc: 0.8903\n",
      "Epoch 13/15 | Train Loss: 0.2483 Acc: 0.8825\n",
      "Epoch 14/15 | Train Loss: 0.2206 Acc: 0.9138\n",
      "Epoch 15/15 | Train Loss: 0.2176 Acc: 0.9086\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5099 Acc: 0.7493\n",
      "Epoch 2/15 | Train Loss: 0.3614 Acc: 0.7990\n",
      "Epoch 3/15 | Train Loss: 0.3841 Acc: 0.7963\n",
      "Epoch 4/15 | Train Loss: 0.3602 Acc: 0.8120\n",
      "Epoch 5/15 | Train Loss: 0.3339 Acc: 0.8407\n",
      "Epoch 6/15 | Train Loss: 0.3699 Acc: 0.8042\n",
      "Epoch 7/15 | Train Loss: 0.3502 Acc: 0.8120\n",
      "Epoch 8/15 | Train Loss: 0.3103 Acc: 0.8486\n",
      "Epoch 9/15 | Train Loss: 0.2776 Acc: 0.8668\n",
      "Epoch 10/15 | Train Loss: 0.2994 Acc: 0.8616\n",
      "Epoch 11/15 | Train Loss: 0.2876 Acc: 0.8695\n",
      "Epoch 12/15 | Train Loss: 0.2741 Acc: 0.8747\n",
      "Epoch 13/15 | Train Loss: 0.2961 Acc: 0.8433\n",
      "Epoch 14/15 | Train Loss: 0.2659 Acc: 0.8695\n",
      "Epoch 15/15 | Train Loss: 0.2613 Acc: 0.8825\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5211 Acc: 0.7448\n",
      "Epoch 2/15 | Train Loss: 0.4233 Acc: 0.7891\n",
      "Epoch 3/15 | Train Loss: 0.4180 Acc: 0.7760\n",
      "Epoch 4/15 | Train Loss: 0.3479 Acc: 0.8125\n",
      "Epoch 5/15 | Train Loss: 0.3362 Acc: 0.8203\n",
      "Epoch 6/15 | Train Loss: 0.3614 Acc: 0.8151\n",
      "Epoch 7/15 | Train Loss: 0.3244 Acc: 0.8359\n",
      "Epoch 8/15 | Train Loss: 0.3227 Acc: 0.8333\n",
      "Epoch 9/15 | Train Loss: 0.3029 Acc: 0.8516\n",
      "Epoch 10/15 | Train Loss: 0.2725 Acc: 0.8724\n",
      "Epoch 11/15 | Train Loss: 0.2971 Acc: 0.8568\n",
      "Epoch 12/15 | Train Loss: 0.3110 Acc: 0.8594\n",
      "Epoch 13/15 | Train Loss: 0.2609 Acc: 0.8828\n",
      "Epoch 14/15 | Train Loss: 0.2745 Acc: 0.8750\n",
      "Epoch 15/15 | Train Loss: 0.2393 Acc: 0.9062\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5061 Acc: 0.7611\n",
      "Epoch 2/15 | Train Loss: 0.3897 Acc: 0.8217\n",
      "Epoch 3/15 | Train Loss: 0.3550 Acc: 0.8726\n",
      "Epoch 4/15 | Train Loss: 0.3616 Acc: 0.8344\n",
      "Epoch 5/15 | Train Loss: 0.3871 Acc: 0.8280\n",
      "Epoch 6/15 | Train Loss: 0.3269 Acc: 0.8631\n",
      "Epoch 7/15 | Train Loss: 0.3355 Acc: 0.8535\n",
      "Epoch 8/15 | Train Loss: 0.3105 Acc: 0.8631\n",
      "Epoch 9/15 | Train Loss: 0.2815 Acc: 0.8662\n",
      "Epoch 10/15 | Train Loss: 0.2654 Acc: 0.8822\n",
      "Epoch 11/15 | Train Loss: 0.2772 Acc: 0.8822\n",
      "Epoch 12/15 | Train Loss: 0.2843 Acc: 0.8949\n",
      "Epoch 13/15 | Train Loss: 0.2984 Acc: 0.8758\n",
      "Epoch 14/15 | Train Loss: 0.2948 Acc: 0.8758\n",
      "Epoch 15/15 | Train Loss: 0.2726 Acc: 0.8917\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4827 Acc: 0.8536\n",
      "Epoch 2/15 | Train Loss: 0.3808 Acc: 0.8250\n",
      "Epoch 3/15 | Train Loss: 0.3543 Acc: 0.8750\n",
      "Epoch 4/15 | Train Loss: 0.3471 Acc: 0.8821\n",
      "Epoch 5/15 | Train Loss: 0.3214 Acc: 0.8750\n",
      "Epoch 6/15 | Train Loss: 0.2919 Acc: 0.8929\n",
      "Epoch 7/15 | Train Loss: 0.2875 Acc: 0.8929\n",
      "Epoch 8/15 | Train Loss: 0.2859 Acc: 0.8821\n",
      "Epoch 9/15 | Train Loss: 0.2257 Acc: 0.9179\n",
      "Epoch 10/15 | Train Loss: 0.2447 Acc: 0.9214\n",
      "Epoch 11/15 | Train Loss: 0.2440 Acc: 0.9000\n",
      "Epoch 12/15 | Train Loss: 0.2356 Acc: 0.9000\n",
      "Epoch 13/15 | Train Loss: 0.2422 Acc: 0.9179\n",
      "Epoch 14/15 | Train Loss: 0.2345 Acc: 0.9250\n",
      "Epoch 15/15 | Train Loss: 0.2026 Acc: 0.9357\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6605 Acc: 0.6667\n",
      "Epoch 2/15 | Train Loss: 0.5445 Acc: 0.7143\n",
      "Epoch 3/15 | Train Loss: 0.5225 Acc: 0.7659\n",
      "Epoch 4/15 | Train Loss: 0.5012 Acc: 0.7579\n",
      "Epoch 5/15 | Train Loss: 0.4837 Acc: 0.7778\n",
      "Epoch 6/15 | Train Loss: 0.5038 Acc: 0.7619\n",
      "Epoch 7/15 | Train Loss: 0.4322 Acc: 0.7857\n",
      "Epoch 8/15 | Train Loss: 0.3867 Acc: 0.8056\n",
      "Epoch 9/15 | Train Loss: 0.4074 Acc: 0.7857\n",
      "Epoch 10/15 | Train Loss: 0.3738 Acc: 0.8452\n",
      "Epoch 11/15 | Train Loss: 0.3925 Acc: 0.8333\n",
      "Epoch 12/15 | Train Loss: 0.3351 Acc: 0.8611\n",
      "Epoch 13/15 | Train Loss: 0.3315 Acc: 0.8532\n",
      "Epoch 14/15 | Train Loss: 0.3148 Acc: 0.8611\n",
      "Epoch 15/15 | Train Loss: 0.3316 Acc: 0.8571\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7384 Acc: 0.6032\n",
      "Epoch 2/15 | Train Loss: 0.6111 Acc: 0.6905\n",
      "Epoch 3/15 | Train Loss: 0.5223 Acc: 0.7183\n",
      "Epoch 4/15 | Train Loss: 0.5423 Acc: 0.7024\n",
      "Epoch 5/15 | Train Loss: 0.5043 Acc: 0.7540\n",
      "Epoch 6/15 | Train Loss: 0.4534 Acc: 0.7817\n",
      "Epoch 7/15 | Train Loss: 0.4419 Acc: 0.7738\n",
      "Epoch 8/15 | Train Loss: 0.4493 Acc: 0.7857\n",
      "Epoch 9/15 | Train Loss: 0.4335 Acc: 0.7619\n",
      "Epoch 10/15 | Train Loss: 0.4287 Acc: 0.7897\n",
      "Epoch 11/15 | Train Loss: 0.4429 Acc: 0.8016\n",
      "Epoch 12/15 | Train Loss: 0.4172 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3765 Acc: 0.8413\n",
      "Epoch 14/15 | Train Loss: 0.3873 Acc: 0.8135\n",
      "Epoch 15/15 | Train Loss: 0.3816 Acc: 0.8056\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6783 Acc: 0.6482\n",
      "Epoch 2/15 | Train Loss: 0.6352 Acc: 0.6443\n",
      "Epoch 3/15 | Train Loss: 0.5820 Acc: 0.7233\n",
      "Epoch 4/15 | Train Loss: 0.5184 Acc: 0.7194\n",
      "Epoch 5/15 | Train Loss: 0.5436 Acc: 0.6917\n",
      "Epoch 6/15 | Train Loss: 0.4147 Acc: 0.8103\n",
      "Epoch 7/15 | Train Loss: 0.4675 Acc: 0.7905\n",
      "Epoch 8/15 | Train Loss: 0.4964 Acc: 0.7312\n",
      "Epoch 9/15 | Train Loss: 0.4366 Acc: 0.8103\n",
      "Epoch 10/15 | Train Loss: 0.4622 Acc: 0.7549\n",
      "Epoch 11/15 | Train Loss: 0.4668 Acc: 0.7589\n",
      "Epoch 12/15 | Train Loss: 0.4463 Acc: 0.7826\n",
      "Epoch 13/15 | Train Loss: 0.3887 Acc: 0.8221\n",
      "Epoch 14/15 | Train Loss: 0.3857 Acc: 0.8142\n",
      "Epoch 15/15 | Train Loss: 0.3965 Acc: 0.8221\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6112 Acc: 0.6653\n",
      "Epoch 2/15 | Train Loss: 0.5249 Acc: 0.7797\n",
      "Epoch 3/15 | Train Loss: 0.5105 Acc: 0.7542\n",
      "Epoch 4/15 | Train Loss: 0.4854 Acc: 0.7585\n",
      "Epoch 5/15 | Train Loss: 0.4216 Acc: 0.7881\n",
      "Epoch 6/15 | Train Loss: 0.4921 Acc: 0.7839\n",
      "Epoch 7/15 | Train Loss: 0.4634 Acc: 0.7839\n",
      "Epoch 8/15 | Train Loss: 0.4442 Acc: 0.7966\n",
      "Epoch 9/15 | Train Loss: 0.3698 Acc: 0.8263\n",
      "Epoch 10/15 | Train Loss: 0.4265 Acc: 0.8220\n",
      "Epoch 11/15 | Train Loss: 0.3776 Acc: 0.8390\n",
      "Epoch 12/15 | Train Loss: 0.3584 Acc: 0.8305\n",
      "Epoch 13/15 | Train Loss: 0.3266 Acc: 0.8517\n",
      "Epoch 14/15 | Train Loss: 0.3554 Acc: 0.8559\n",
      "Epoch 15/15 | Train Loss: 0.4116 Acc: 0.8263\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.4886 Acc: 0.7974\n",
      "Epoch 2/15 | Train Loss: 0.4216 Acc: 0.8326\n",
      "Epoch 3/15 | Train Loss: 0.4103 Acc: 0.8546\n",
      "Epoch 4/15 | Train Loss: 0.3909 Acc: 0.8282\n",
      "Epoch 5/15 | Train Loss: 0.3799 Acc: 0.8634\n",
      "Epoch 6/15 | Train Loss: 0.3856 Acc: 0.8238\n",
      "Epoch 7/15 | Train Loss: 0.3358 Acc: 0.8590\n",
      "Epoch 8/15 | Train Loss: 0.2995 Acc: 0.8855\n",
      "Epoch 9/15 | Train Loss: 0.3131 Acc: 0.8634\n",
      "Epoch 10/15 | Train Loss: 0.3111 Acc: 0.8943\n",
      "Epoch 11/15 | Train Loss: 0.2364 Acc: 0.8987\n",
      "Epoch 12/15 | Train Loss: 0.3248 Acc: 0.8811\n",
      "Epoch 13/15 | Train Loss: 0.2536 Acc: 0.9031\n",
      "Epoch 14/15 | Train Loss: 0.2542 Acc: 0.9075\n",
      "Epoch 15/15 | Train Loss: 0.2615 Acc: 0.9075\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5273 Acc: 0.7362\n",
      "Epoch 2/15 | Train Loss: 0.3886 Acc: 0.8234\n",
      "Epoch 3/15 | Train Loss: 0.3954 Acc: 0.8005\n",
      "Epoch 4/15 | Train Loss: 0.3559 Acc: 0.8303\n",
      "Epoch 5/15 | Train Loss: 0.3181 Acc: 0.8417\n",
      "Epoch 6/15 | Train Loss: 0.3433 Acc: 0.8234\n",
      "Epoch 7/15 | Train Loss: 0.3271 Acc: 0.8486\n",
      "Epoch 8/15 | Train Loss: 0.3040 Acc: 0.8601\n",
      "Epoch 9/15 | Train Loss: 0.2690 Acc: 0.8716\n",
      "Epoch 10/15 | Train Loss: 0.2798 Acc: 0.8693\n",
      "Epoch 11/15 | Train Loss: 0.2766 Acc: 0.8693\n",
      "Epoch 12/15 | Train Loss: 0.2797 Acc: 0.8830\n",
      "Epoch 13/15 | Train Loss: 0.2720 Acc: 0.8853\n",
      "Epoch 14/15 | Train Loss: 0.2614 Acc: 0.8876\n",
      "Epoch 15/15 | Train Loss: 0.2598 Acc: 0.8830\n",
      "Fold 7 Test Accuracy: 0.5795\n",
      "===== Fold 8 =====\n",
      "Epoch 1: Generator Loss = 9.7821, Discriminator Loss = 8.7727\n",
      "Epoch 2: Generator Loss = 10.5485, Discriminator Loss = 8.3433\n",
      "Epoch 3: Generator Loss = 16.5651, Discriminator Loss = 5.6799\n",
      "Epoch 4: Generator Loss = 20.1779, Discriminator Loss = 7.0633\n",
      "Epoch 5: Generator Loss = 21.8184, Discriminator Loss = 5.7899\n",
      "Epoch 6: Generator Loss = 24.0720, Discriminator Loss = 6.4973\n",
      "Epoch 7: Generator Loss = 26.0699, Discriminator Loss = 5.0997\n",
      "Epoch 8: Generator Loss = 28.8037, Discriminator Loss = 4.8067\n",
      "Epoch 9: Generator Loss = 27.7610, Discriminator Loss = 3.9936\n",
      "Epoch 10: Generator Loss = 23.5198, Discriminator Loss = 7.8450\n",
      "Epoch 11: Generator Loss = 24.0802, Discriminator Loss = 10.8947\n",
      "Epoch 12: Generator Loss = 20.6014, Discriminator Loss = 7.1368\n",
      "Epoch 13: Generator Loss = 16.0696, Discriminator Loss = 9.6293\n",
      "Epoch 14: Generator Loss = 13.1277, Discriminator Loss = 9.0490\n",
      "Epoch 15: Generator Loss = 15.4033, Discriminator Loss = 9.7302\n",
      "Epoch 16: Generator Loss = 14.9084, Discriminator Loss = 8.1649\n",
      "Epoch 17: Generator Loss = 19.6705, Discriminator Loss = 7.7383\n",
      "Epoch 18: Generator Loss = 21.2503, Discriminator Loss = 9.3601\n",
      "Epoch 19: Generator Loss = 22.3437, Discriminator Loss = 7.2834\n",
      "Epoch 20: Generator Loss = 18.1885, Discriminator Loss = 7.0705\n",
      "Epoch 21: Generator Loss = 19.1499, Discriminator Loss = 8.7929\n",
      "Epoch 22: Generator Loss = 22.0372, Discriminator Loss = 9.6955\n",
      "Epoch 23: Generator Loss = 19.3316, Discriminator Loss = 7.2699\n",
      "Epoch 24: Generator Loss = 20.3177, Discriminator Loss = 7.9325\n",
      "Epoch 25: Generator Loss = 18.7560, Discriminator Loss = 8.7593\n",
      "Epoch 26: Generator Loss = 20.5018, Discriminator Loss = 8.7049\n",
      "Epoch 27: Generator Loss = 24.9821, Discriminator Loss = 8.4516\n",
      "Epoch 28: Generator Loss = 25.0734, Discriminator Loss = 7.2832\n",
      "Epoch 29: Generator Loss = 22.9938, Discriminator Loss = 7.4438\n",
      "Epoch 30: Generator Loss = 18.9510, Discriminator Loss = 7.3391\n",
      "Epoch 31: Generator Loss = 21.4267, Discriminator Loss = 8.5296\n",
      "Epoch 32: Generator Loss = 20.7727, Discriminator Loss = 7.8666\n",
      "Epoch 33: Generator Loss = 15.2085, Discriminator Loss = 9.2935\n",
      "Epoch 34: Generator Loss = 19.6727, Discriminator Loss = 7.3925\n",
      "Epoch 35: Generator Loss = 18.3605, Discriminator Loss = 8.7624\n",
      "Epoch 36: Generator Loss = 15.1956, Discriminator Loss = 7.0708\n",
      "Epoch 37: Generator Loss = 19.5379, Discriminator Loss = 7.7551\n",
      "Epoch 38: Generator Loss = 17.8070, Discriminator Loss = 8.2606\n",
      "Epoch 39: Generator Loss = 18.7964, Discriminator Loss = 7.6292\n",
      "Epoch 40: Generator Loss = 14.9938, Discriminator Loss = 8.5041\n",
      "Epoch 41: Generator Loss = 18.0600, Discriminator Loss = 9.1979\n",
      "Epoch 42: Generator Loss = 16.1311, Discriminator Loss = 7.4673\n",
      "Epoch 43: Generator Loss = 18.3180, Discriminator Loss = 7.3228\n",
      "Epoch 44: Generator Loss = 17.1233, Discriminator Loss = 8.7288\n",
      "Epoch 45: Generator Loss = 14.7796, Discriminator Loss = 8.6307\n",
      "Epoch 46: Generator Loss = 17.6077, Discriminator Loss = 9.0115\n",
      "Epoch 47: Generator Loss = 16.5264, Discriminator Loss = 7.2681\n",
      "Epoch 48: Generator Loss = 18.0271, Discriminator Loss = 8.2699\n",
      "Epoch 49: Generator Loss = 14.8097, Discriminator Loss = 8.1684\n",
      "Epoch 50: Generator Loss = 19.1940, Discriminator Loss = 6.8660\n",
      "Epoch 51: Generator Loss = 15.2313, Discriminator Loss = 8.1770\n",
      "Epoch 52: Generator Loss = 19.8641, Discriminator Loss = 9.0448\n",
      "Epoch 53: Generator Loss = 19.3421, Discriminator Loss = 7.8760\n",
      "Epoch 54: Generator Loss = 15.9520, Discriminator Loss = 6.7902\n",
      "Epoch 55: Generator Loss = 20.8627, Discriminator Loss = 6.9300\n",
      "Epoch 56: Generator Loss = 18.8682, Discriminator Loss = 8.4295\n",
      "Epoch 57: Generator Loss = 17.2508, Discriminator Loss = 7.0390\n",
      "Epoch 58: Generator Loss = 25.9233, Discriminator Loss = 5.5866\n",
      "Epoch 59: Generator Loss = 21.6839, Discriminator Loss = 10.5968\n",
      "Epoch 60: Generator Loss = 15.5181, Discriminator Loss = 7.3592\n",
      "Epoch 61: Generator Loss = 22.1953, Discriminator Loss = 6.5839\n",
      "Epoch 62: Generator Loss = 23.6928, Discriminator Loss = 7.6563\n",
      "Epoch 63: Generator Loss = 21.0324, Discriminator Loss = 7.0664\n",
      "Epoch 64: Generator Loss = 21.9354, Discriminator Loss = 5.1339\n",
      "Epoch 65: Generator Loss = 27.0611, Discriminator Loss = 6.7018\n",
      "Epoch 66: Generator Loss = 25.0295, Discriminator Loss = 4.7739\n",
      "Epoch 67: Generator Loss = 27.6455, Discriminator Loss = 6.1375\n",
      "Epoch 68: Generator Loss = 29.7826, Discriminator Loss = 4.0550\n",
      "Epoch 69: Generator Loss = 36.2188, Discriminator Loss = 4.1459\n",
      "Epoch 70: Generator Loss = 36.4932, Discriminator Loss = 5.9194\n",
      "Epoch 71: Generator Loss = 20.3585, Discriminator Loss = 6.3972\n",
      "Epoch 72: Generator Loss = 40.3406, Discriminator Loss = 3.6952\n",
      "Epoch 73: Generator Loss = 25.4471, Discriminator Loss = 7.0237\n",
      "Epoch 74: Generator Loss = 34.6792, Discriminator Loss = 3.2686\n",
      "Epoch 75: Generator Loss = 30.3283, Discriminator Loss = 4.3206\n",
      "Epoch 76: Generator Loss = 34.0509, Discriminator Loss = 3.6663\n",
      "Epoch 77: Generator Loss = 33.8218, Discriminator Loss = 3.4283\n",
      "Epoch 78: Generator Loss = 35.7976, Discriminator Loss = 2.3170\n",
      "Epoch 79: Generator Loss = 42.0167, Discriminator Loss = 4.1784\n",
      "Epoch 80: Generator Loss = 44.3794, Discriminator Loss = 2.2960\n",
      "Epoch 81: Generator Loss = 51.0016, Discriminator Loss = 1.9891\n",
      "Epoch 82: Generator Loss = 49.6359, Discriminator Loss = 1.6500\n",
      "Epoch 83: Generator Loss = 47.4338, Discriminator Loss = 4.2547\n",
      "Epoch 84: Generator Loss = 39.1936, Discriminator Loss = 3.0276\n",
      "Epoch 85: Generator Loss = 41.0760, Discriminator Loss = 2.1562\n",
      "Epoch 86: Generator Loss = 54.0365, Discriminator Loss = 1.3805\n",
      "Epoch 87: Generator Loss = 51.9897, Discriminator Loss = 1.7258\n",
      "Epoch 88: Generator Loss = 49.4652, Discriminator Loss = 2.1246\n",
      "Epoch 89: Generator Loss = 54.6078, Discriminator Loss = 1.7697\n",
      "Epoch 90: Generator Loss = 74.0371, Discriminator Loss = 1.2735\n",
      "Epoch 91: Generator Loss = 75.6493, Discriminator Loss = 1.2211\n",
      "Epoch 92: Generator Loss = 72.6402, Discriminator Loss = 1.1616\n",
      "Epoch 93: Generator Loss = 62.6481, Discriminator Loss = 2.6044\n",
      "Epoch 94: Generator Loss = 59.4902, Discriminator Loss = 4.3429\n",
      "Epoch 95: Generator Loss = 60.1467, Discriminator Loss = 2.0203\n",
      "Epoch 96: Generator Loss = 50.0875, Discriminator Loss = 1.3060\n",
      "Epoch 97: Generator Loss = 64.5245, Discriminator Loss = 1.1160\n",
      "Epoch 98: Generator Loss = 81.3686, Discriminator Loss = 2.5060\n",
      "Epoch 99: Generator Loss = 55.8291, Discriminator Loss = 1.5055\n",
      "Epoch 100: Generator Loss = 62.4888, Discriminator Loss = 2.3177\n",
      "Epoch 101: Generator Loss = 58.0550, Discriminator Loss = 1.2037\n",
      "Epoch 102: Generator Loss = 76.4569, Discriminator Loss = 1.4005\n",
      "Epoch 103: Generator Loss = 68.1657, Discriminator Loss = 0.7328\n",
      "Epoch 104: Generator Loss = 51.6017, Discriminator Loss = 0.9232\n",
      "Epoch 105: Generator Loss = 72.4203, Discriminator Loss = 1.0037\n",
      "Epoch 106: Generator Loss = 75.4415, Discriminator Loss = 1.3106\n",
      "Epoch 107: Generator Loss = 58.6178, Discriminator Loss = 1.5398\n",
      "Epoch 108: Generator Loss = 59.5701, Discriminator Loss = 1.6362\n",
      "Epoch 109: Generator Loss = 78.9809, Discriminator Loss = 3.4236\n",
      "Epoch 110: Generator Loss = 51.3117, Discriminator Loss = 3.7182\n",
      "Epoch 111: Generator Loss = 56.7544, Discriminator Loss = 2.0610\n",
      "Epoch 112: Generator Loss = 77.9412, Discriminator Loss = 0.6728\n",
      "Epoch 113: Generator Loss = 75.1033, Discriminator Loss = 0.5864\n",
      "Epoch 114: Generator Loss = 70.4147, Discriminator Loss = 0.8446\n",
      "Epoch 115: Generator Loss = 77.4725, Discriminator Loss = 1.1243\n",
      "Epoch 116: Generator Loss = 84.2181, Discriminator Loss = 0.8597\n",
      "Epoch 117: Generator Loss = 96.9657, Discriminator Loss = 1.5366\n",
      "Epoch 118: Generator Loss = 110.6422, Discriminator Loss = 1.1162\n",
      "Epoch 119: Generator Loss = 83.6464, Discriminator Loss = 3.4008\n",
      "Epoch 120: Generator Loss = 54.1038, Discriminator Loss = 1.9243\n",
      "Epoch 121: Generator Loss = 74.2767, Discriminator Loss = 2.0476\n",
      "Epoch 122: Generator Loss = 74.0336, Discriminator Loss = 0.9285\n",
      "Epoch 123: Generator Loss = 52.3975, Discriminator Loss = 2.7586\n",
      "Epoch 124: Generator Loss = 79.9261, Discriminator Loss = 1.6611\n",
      "Epoch 125: Generator Loss = 69.0007, Discriminator Loss = 1.4714\n",
      "Epoch 126: Generator Loss = 75.6359, Discriminator Loss = 0.7613\n",
      "Epoch 127: Generator Loss = 68.7826, Discriminator Loss = 2.4712\n",
      "Epoch 128: Generator Loss = 87.8610, Discriminator Loss = 1.9320\n",
      "Epoch 129: Generator Loss = 57.3695, Discriminator Loss = 1.8679\n",
      "Epoch 130: Generator Loss = 72.1226, Discriminator Loss = 0.8771\n",
      "Epoch 131: Generator Loss = 84.2667, Discriminator Loss = 0.3721\n",
      "Epoch 132: Generator Loss = 73.8519, Discriminator Loss = 0.9898\n",
      "Epoch 133: Generator Loss = 89.3822, Discriminator Loss = 0.5796\n",
      "Epoch 134: Generator Loss = 59.2100, Discriminator Loss = 6.3630\n",
      "Epoch 135: Generator Loss = 84.6613, Discriminator Loss = 1.9838\n",
      "Epoch 136: Generator Loss = 80.6443, Discriminator Loss = 0.5857\n",
      "Epoch 137: Generator Loss = 76.5169, Discriminator Loss = 0.6452\n",
      "Epoch 138: Generator Loss = 97.3217, Discriminator Loss = 1.3026\n",
      "Epoch 139: Generator Loss = 80.0865, Discriminator Loss = 0.6333\n",
      "Epoch 140: Generator Loss = 69.7124, Discriminator Loss = 1.0227\n",
      "Epoch 141: Generator Loss = 88.5905, Discriminator Loss = 0.7300\n",
      "Epoch 142: Generator Loss = 87.0012, Discriminator Loss = 0.5020\n",
      "Epoch 143: Generator Loss = 89.7006, Discriminator Loss = 0.5426\n",
      "Epoch 144: Generator Loss = 80.0188, Discriminator Loss = 0.3207\n",
      "Epoch 145: Generator Loss = 70.9601, Discriminator Loss = 0.4806\n",
      "Epoch 146: Generator Loss = 72.9044, Discriminator Loss = 1.4813\n",
      "Epoch 147: Generator Loss = 71.2019, Discriminator Loss = 1.5928\n",
      "Epoch 148: Generator Loss = 79.1522, Discriminator Loss = 0.4909\n",
      "Epoch 149: Generator Loss = 78.9416, Discriminator Loss = 0.6736\n",
      "Epoch 150: Generator Loss = 71.3373, Discriminator Loss = 0.7168\n",
      "Epoch 151: Generator Loss = 103.0607, Discriminator Loss = 0.5362\n",
      "Epoch 152: Generator Loss = 90.0811, Discriminator Loss = 0.6071\n",
      "Epoch 153: Generator Loss = 98.6174, Discriminator Loss = 0.5185\n",
      "Epoch 154: Generator Loss = 89.0770, Discriminator Loss = 2.0688\n",
      "Epoch 155: Generator Loss = 71.6753, Discriminator Loss = 0.9123\n",
      "Epoch 156: Generator Loss = 91.5018, Discriminator Loss = 0.4979\n",
      "Epoch 157: Generator Loss = 104.6231, Discriminator Loss = 9.6707\n",
      "Epoch 158: Generator Loss = 46.8693, Discriminator Loss = 8.5896\n",
      "Epoch 159: Generator Loss = 50.9127, Discriminator Loss = 2.3684\n",
      "Epoch 160: Generator Loss = 55.3254, Discriminator Loss = 1.8414\n",
      "Epoch 161: Generator Loss = 54.3320, Discriminator Loss = 2.0422\n",
      "Epoch 162: Generator Loss = 53.7414, Discriminator Loss = 6.9585\n",
      "Epoch 163: Generator Loss = 55.8976, Discriminator Loss = 5.9056\n",
      "Epoch 164: Generator Loss = 47.4456, Discriminator Loss = 4.0598\n",
      "Epoch 165: Generator Loss = 61.9573, Discriminator Loss = 1.7093\n",
      "Epoch 166: Generator Loss = 57.2335, Discriminator Loss = 1.9389\n",
      "Epoch 167: Generator Loss = 51.5886, Discriminator Loss = 2.9865\n",
      "Epoch 168: Generator Loss = 58.0761, Discriminator Loss = 1.8557\n",
      "Epoch 169: Generator Loss = 67.3685, Discriminator Loss = 1.4421\n",
      "Epoch 170: Generator Loss = 58.6045, Discriminator Loss = 1.8616\n",
      "Epoch 171: Generator Loss = 72.6241, Discriminator Loss = 2.1430\n",
      "Epoch 172: Generator Loss = 54.1510, Discriminator Loss = 1.1939\n",
      "Epoch 173: Generator Loss = 53.8895, Discriminator Loss = 0.6222\n",
      "Epoch 174: Generator Loss = 75.7838, Discriminator Loss = 0.9574\n",
      "Epoch 175: Generator Loss = 57.7870, Discriminator Loss = 1.8800\n",
      "Epoch 176: Generator Loss = 81.6391, Discriminator Loss = 2.0885\n",
      "Epoch 177: Generator Loss = 71.3903, Discriminator Loss = 1.9820\n",
      "Epoch 178: Generator Loss = 59.7388, Discriminator Loss = 1.1329\n",
      "Epoch 179: Generator Loss = 62.4051, Discriminator Loss = 1.3433\n",
      "Epoch 180: Generator Loss = 68.2016, Discriminator Loss = 1.3654\n",
      "Epoch 181: Generator Loss = 60.4527, Discriminator Loss = 1.2504\n",
      "Epoch 182: Generator Loss = 69.6117, Discriminator Loss = 0.9340\n",
      "Epoch 183: Generator Loss = 52.4109, Discriminator Loss = 1.0453\n",
      "Epoch 184: Generator Loss = 68.3726, Discriminator Loss = 1.1004\n",
      "Epoch 185: Generator Loss = 49.8019, Discriminator Loss = 0.5983\n",
      "Epoch 186: Generator Loss = 77.5290, Discriminator Loss = 0.4220\n",
      "Epoch 187: Generator Loss = 86.7734, Discriminator Loss = 0.9962\n",
      "Epoch 188: Generator Loss = 51.8302, Discriminator Loss = 7.4775\n",
      "Epoch 189: Generator Loss = 56.7032, Discriminator Loss = 2.2658\n",
      "Epoch 190: Generator Loss = 62.0647, Discriminator Loss = 3.2055\n",
      "Epoch 191: Generator Loss = 53.6841, Discriminator Loss = 1.9885\n",
      "Epoch 192: Generator Loss = 53.7401, Discriminator Loss = 2.8055\n",
      "Epoch 193: Generator Loss = 49.6698, Discriminator Loss = 2.2469\n",
      "Epoch 194: Generator Loss = 59.4628, Discriminator Loss = 1.2790\n",
      "Epoch 195: Generator Loss = 59.2759, Discriminator Loss = 1.0054\n",
      "Epoch 196: Generator Loss = 70.1943, Discriminator Loss = 1.8121\n",
      "Epoch 197: Generator Loss = 53.0130, Discriminator Loss = 3.8270\n",
      "Epoch 198: Generator Loss = 45.3443, Discriminator Loss = 2.9478\n",
      "Epoch 199: Generator Loss = 61.1234, Discriminator Loss = 2.7934\n",
      "Epoch 200: Generator Loss = 59.4734, Discriminator Loss = 1.3474\n",
      "Epoch 201: Generator Loss = 65.2902, Discriminator Loss = 2.6095\n",
      "Epoch 202: Generator Loss = 54.6922, Discriminator Loss = 2.2466\n",
      "Epoch 203: Generator Loss = 79.2310, Discriminator Loss = 0.8524\n",
      "Epoch 204: Generator Loss = 58.9573, Discriminator Loss = 1.3120\n",
      "Epoch 205: Generator Loss = 79.3842, Discriminator Loss = 3.5731\n",
      "Epoch 206: Generator Loss = 39.5393, Discriminator Loss = 9.9039\n",
      "Epoch 207: Generator Loss = 48.2524, Discriminator Loss = 3.6706\n",
      "Epoch 208: Generator Loss = 54.0709, Discriminator Loss = 1.9571\n",
      "Epoch 209: Generator Loss = 59.0555, Discriminator Loss = 3.4350\n",
      "Epoch 210: Generator Loss = 55.8986, Discriminator Loss = 1.9106\n",
      "Epoch 211: Generator Loss = 57.4238, Discriminator Loss = 1.2891\n",
      "Epoch 212: Generator Loss = 54.1732, Discriminator Loss = 4.6014\n",
      "Epoch 213: Generator Loss = 57.4490, Discriminator Loss = 1.7247\n",
      "Epoch 214: Generator Loss = 59.9715, Discriminator Loss = 0.9321\n",
      "Epoch 215: Generator Loss = 91.6192, Discriminator Loss = 0.7123\n",
      "Epoch 216: Generator Loss = 76.8084, Discriminator Loss = 1.3827\n",
      "Epoch 217: Generator Loss = 71.6205, Discriminator Loss = 1.0025\n",
      "Epoch 218: Generator Loss = 63.2644, Discriminator Loss = 1.0802\n",
      "Epoch 219: Generator Loss = 72.5397, Discriminator Loss = 1.6105\n",
      "Epoch 220: Generator Loss = 78.9444, Discriminator Loss = 1.5677\n",
      "Epoch 221: Generator Loss = 70.0894, Discriminator Loss = 3.2886\n",
      "Epoch 222: Generator Loss = 59.7317, Discriminator Loss = 1.0290\n",
      "Epoch 223: Generator Loss = 81.3348, Discriminator Loss = 2.6856\n",
      "Epoch 224: Generator Loss = 62.9199, Discriminator Loss = 1.0714\n",
      "Epoch 225: Generator Loss = 88.0022, Discriminator Loss = 0.7869\n",
      "Epoch 226: Generator Loss = 91.5385, Discriminator Loss = 1.4091\n",
      "Epoch 227: Generator Loss = 111.1346, Discriminator Loss = 1.8812\n",
      "Epoch 228: Generator Loss = 70.9320, Discriminator Loss = 1.0797\n",
      "Epoch 229: Generator Loss = 64.5796, Discriminator Loss = 1.0884\n",
      "Epoch 230: Generator Loss = 87.7234, Discriminator Loss = 2.7123\n",
      "Epoch 231: Generator Loss = 66.0211, Discriminator Loss = 0.5880\n",
      "Epoch 232: Generator Loss = 80.5152, Discriminator Loss = 0.7733\n",
      "Epoch 233: Generator Loss = 80.8021, Discriminator Loss = 1.3061\n",
      "Epoch 234: Generator Loss = 63.4689, Discriminator Loss = 6.1511\n",
      "Epoch 235: Generator Loss = 59.8176, Discriminator Loss = 1.2207\n",
      "Epoch 236: Generator Loss = 71.1664, Discriminator Loss = 2.9709\n",
      "Epoch 237: Generator Loss = 76.1596, Discriminator Loss = 1.3072\n",
      "Epoch 238: Generator Loss = 77.0111, Discriminator Loss = 0.5569\n",
      "Epoch 239: Generator Loss = 66.6104, Discriminator Loss = 0.8864\n",
      "Epoch 240: Generator Loss = 60.5597, Discriminator Loss = 0.7144\n",
      "Epoch 241: Generator Loss = 83.6178, Discriminator Loss = 0.5443\n",
      "Epoch 242: Generator Loss = 83.1717, Discriminator Loss = 0.5892\n",
      "Epoch 243: Generator Loss = 84.4154, Discriminator Loss = 0.7979\n",
      "Epoch 244: Generator Loss = 88.1042, Discriminator Loss = 0.6060\n",
      "Epoch 245: Generator Loss = 77.5583, Discriminator Loss = 1.2168\n",
      "Epoch 246: Generator Loss = 62.4192, Discriminator Loss = 0.7537\n",
      "Epoch 247: Generator Loss = 68.5522, Discriminator Loss = 1.5756\n",
      "Epoch 248: Generator Loss = 105.7897, Discriminator Loss = 2.1316\n",
      "Epoch 249: Generator Loss = 87.2196, Discriminator Loss = 0.8610\n",
      "Epoch 250: Generator Loss = 75.8804, Discriminator Loss = 0.8976\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.7554 Acc: 0.7232\n",
      "Epoch 2/15 | Train Loss: 0.4251 Acc: 0.7781\n",
      "Epoch 3/15 | Train Loss: 0.4365 Acc: 0.7755\n",
      "Epoch 4/15 | Train Loss: 0.4186 Acc: 0.7781\n",
      "Epoch 5/15 | Train Loss: 0.3474 Acc: 0.8407\n",
      "Epoch 6/15 | Train Loss: 0.3151 Acc: 0.8355\n",
      "Epoch 7/15 | Train Loss: 0.3430 Acc: 0.8303\n",
      "Epoch 8/15 | Train Loss: 0.3120 Acc: 0.8590\n",
      "Epoch 9/15 | Train Loss: 0.3305 Acc: 0.8355\n",
      "Epoch 10/15 | Train Loss: 0.3120 Acc: 0.8329\n",
      "Epoch 11/15 | Train Loss: 0.2967 Acc: 0.8538\n",
      "Epoch 12/15 | Train Loss: 0.2506 Acc: 0.8773\n",
      "Epoch 13/15 | Train Loss: 0.2958 Acc: 0.8642\n",
      "Epoch 14/15 | Train Loss: 0.2842 Acc: 0.8642\n",
      "Epoch 15/15 | Train Loss: 0.3046 Acc: 0.8381\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.6065 Acc: 0.7232\n",
      "Epoch 2/15 | Train Loss: 0.4469 Acc: 0.7624\n",
      "Epoch 3/15 | Train Loss: 0.4034 Acc: 0.7885\n",
      "Epoch 4/15 | Train Loss: 0.3484 Acc: 0.8355\n",
      "Epoch 5/15 | Train Loss: 0.3989 Acc: 0.7755\n",
      "Epoch 6/15 | Train Loss: 0.3285 Acc: 0.8225\n",
      "Epoch 7/15 | Train Loss: 0.3381 Acc: 0.8407\n",
      "Epoch 8/15 | Train Loss: 0.3152 Acc: 0.8486\n",
      "Epoch 9/15 | Train Loss: 0.3098 Acc: 0.8564\n",
      "Epoch 10/15 | Train Loss: 0.2594 Acc: 0.8956\n",
      "Epoch 11/15 | Train Loss: 0.2937 Acc: 0.8512\n",
      "Epoch 12/15 | Train Loss: 0.2931 Acc: 0.8460\n",
      "Epoch 13/15 | Train Loss: 0.2679 Acc: 0.8668\n",
      "Epoch 14/15 | Train Loss: 0.2958 Acc: 0.8407\n",
      "Epoch 15/15 | Train Loss: 0.2844 Acc: 0.8433\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4609 Acc: 0.7578\n",
      "Epoch 2/15 | Train Loss: 0.3978 Acc: 0.7943\n",
      "Epoch 3/15 | Train Loss: 0.3815 Acc: 0.7839\n",
      "Epoch 4/15 | Train Loss: 0.3646 Acc: 0.8047\n",
      "Epoch 5/15 | Train Loss: 0.3702 Acc: 0.7865\n",
      "Epoch 6/15 | Train Loss: 0.3597 Acc: 0.8021\n",
      "Epoch 7/15 | Train Loss: 0.3345 Acc: 0.8177\n",
      "Epoch 8/15 | Train Loss: 0.3143 Acc: 0.8307\n",
      "Epoch 9/15 | Train Loss: 0.3185 Acc: 0.8385\n",
      "Epoch 10/15 | Train Loss: 0.3308 Acc: 0.8333\n",
      "Epoch 11/15 | Train Loss: 0.2945 Acc: 0.8568\n",
      "Epoch 12/15 | Train Loss: 0.2982 Acc: 0.8490\n",
      "Epoch 13/15 | Train Loss: 0.3001 Acc: 0.8411\n",
      "Epoch 14/15 | Train Loss: 0.3047 Acc: 0.8281\n",
      "Epoch 15/15 | Train Loss: 0.2984 Acc: 0.8307\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5451 Acc: 0.7930\n",
      "Epoch 2/15 | Train Loss: 0.3858 Acc: 0.8312\n",
      "Epoch 3/15 | Train Loss: 0.4160 Acc: 0.8185\n",
      "Epoch 4/15 | Train Loss: 0.3938 Acc: 0.8312\n",
      "Epoch 5/15 | Train Loss: 0.3840 Acc: 0.8439\n",
      "Epoch 6/15 | Train Loss: 0.3391 Acc: 0.8503\n",
      "Epoch 7/15 | Train Loss: 0.3711 Acc: 0.8217\n",
      "Epoch 8/15 | Train Loss: 0.3239 Acc: 0.8599\n",
      "Epoch 9/15 | Train Loss: 0.3188 Acc: 0.8631\n",
      "Epoch 10/15 | Train Loss: 0.3038 Acc: 0.8599\n",
      "Epoch 11/15 | Train Loss: 0.2977 Acc: 0.8822\n",
      "Epoch 12/15 | Train Loss: 0.2796 Acc: 0.8822\n",
      "Epoch 13/15 | Train Loss: 0.3177 Acc: 0.8535\n",
      "Epoch 14/15 | Train Loss: 0.2921 Acc: 0.8822\n",
      "Epoch 15/15 | Train Loss: 0.2936 Acc: 0.8694\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5262 Acc: 0.8143\n",
      "Epoch 2/15 | Train Loss: 0.3896 Acc: 0.8250\n",
      "Epoch 3/15 | Train Loss: 0.3286 Acc: 0.8643\n",
      "Epoch 4/15 | Train Loss: 0.3444 Acc: 0.8750\n",
      "Epoch 5/15 | Train Loss: 0.3099 Acc: 0.8643\n",
      "Epoch 6/15 | Train Loss: 0.3337 Acc: 0.8857\n",
      "Epoch 7/15 | Train Loss: 0.2883 Acc: 0.9000\n",
      "Epoch 8/15 | Train Loss: 0.3063 Acc: 0.8929\n",
      "Epoch 9/15 | Train Loss: 0.2590 Acc: 0.8857\n",
      "Epoch 10/15 | Train Loss: 0.2629 Acc: 0.8893\n",
      "Epoch 11/15 | Train Loss: 0.2769 Acc: 0.8714\n",
      "Epoch 12/15 | Train Loss: 0.2589 Acc: 0.8964\n",
      "Epoch 13/15 | Train Loss: 0.2324 Acc: 0.9107\n",
      "Epoch 14/15 | Train Loss: 0.2165 Acc: 0.9036\n",
      "Epoch 15/15 | Train Loss: 0.2667 Acc: 0.8893\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7410 Acc: 0.6151\n",
      "Epoch 2/15 | Train Loss: 0.6154 Acc: 0.6984\n",
      "Epoch 3/15 | Train Loss: 0.5666 Acc: 0.6905\n",
      "Epoch 4/15 | Train Loss: 0.5510 Acc: 0.7024\n",
      "Epoch 5/15 | Train Loss: 0.4392 Acc: 0.7698\n",
      "Epoch 6/15 | Train Loss: 0.5268 Acc: 0.7302\n",
      "Epoch 7/15 | Train Loss: 0.4680 Acc: 0.7579\n",
      "Epoch 8/15 | Train Loss: 0.4489 Acc: 0.7817\n",
      "Epoch 9/15 | Train Loss: 0.4877 Acc: 0.7619\n",
      "Epoch 10/15 | Train Loss: 0.3860 Acc: 0.8056\n",
      "Epoch 11/15 | Train Loss: 0.3905 Acc: 0.8056\n",
      "Epoch 12/15 | Train Loss: 0.3900 Acc: 0.8175\n",
      "Epoch 13/15 | Train Loss: 0.4084 Acc: 0.7976\n",
      "Epoch 14/15 | Train Loss: 0.3703 Acc: 0.8254\n",
      "Epoch 15/15 | Train Loss: 0.3850 Acc: 0.8135\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6786 Acc: 0.6230\n",
      "Epoch 2/15 | Train Loss: 0.6091 Acc: 0.6905\n",
      "Epoch 3/15 | Train Loss: 0.5110 Acc: 0.7460\n",
      "Epoch 4/15 | Train Loss: 0.5142 Acc: 0.7302\n",
      "Epoch 5/15 | Train Loss: 0.5223 Acc: 0.7341\n",
      "Epoch 6/15 | Train Loss: 0.4897 Acc: 0.7460\n",
      "Epoch 7/15 | Train Loss: 0.4734 Acc: 0.7579\n",
      "Epoch 8/15 | Train Loss: 0.4429 Acc: 0.8016\n",
      "Epoch 9/15 | Train Loss: 0.4534 Acc: 0.7579\n",
      "Epoch 10/15 | Train Loss: 0.4370 Acc: 0.7778\n",
      "Epoch 11/15 | Train Loss: 0.4571 Acc: 0.7619\n",
      "Epoch 12/15 | Train Loss: 0.4305 Acc: 0.7659\n",
      "Epoch 13/15 | Train Loss: 0.4037 Acc: 0.7857\n",
      "Epoch 14/15 | Train Loss: 0.4069 Acc: 0.8095\n",
      "Epoch 15/15 | Train Loss: 0.3859 Acc: 0.8333\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.8116 Acc: 0.5771\n",
      "Epoch 2/15 | Train Loss: 0.5985 Acc: 0.6917\n",
      "Epoch 3/15 | Train Loss: 0.5466 Acc: 0.6957\n",
      "Epoch 4/15 | Train Loss: 0.5070 Acc: 0.7431\n",
      "Epoch 5/15 | Train Loss: 0.4999 Acc: 0.7549\n",
      "Epoch 6/15 | Train Loss: 0.5218 Acc: 0.7194\n",
      "Epoch 7/15 | Train Loss: 0.5003 Acc: 0.7391\n",
      "Epoch 8/15 | Train Loss: 0.4708 Acc: 0.7549\n",
      "Epoch 9/15 | Train Loss: 0.4482 Acc: 0.7708\n",
      "Epoch 10/15 | Train Loss: 0.4283 Acc: 0.7984\n",
      "Epoch 11/15 | Train Loss: 0.4868 Acc: 0.7787\n",
      "Epoch 12/15 | Train Loss: 0.4306 Acc: 0.7787\n",
      "Epoch 13/15 | Train Loss: 0.4085 Acc: 0.8261\n",
      "Epoch 14/15 | Train Loss: 0.4197 Acc: 0.7945\n",
      "Epoch 15/15 | Train Loss: 0.3784 Acc: 0.8024\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6324 Acc: 0.7288\n",
      "Epoch 2/15 | Train Loss: 0.6224 Acc: 0.7288\n",
      "Epoch 3/15 | Train Loss: 0.5230 Acc: 0.7839\n",
      "Epoch 4/15 | Train Loss: 0.4583 Acc: 0.7966\n",
      "Epoch 5/15 | Train Loss: 0.5088 Acc: 0.7458\n",
      "Epoch 6/15 | Train Loss: 0.4560 Acc: 0.8008\n",
      "Epoch 7/15 | Train Loss: 0.4327 Acc: 0.7839\n",
      "Epoch 8/15 | Train Loss: 0.5031 Acc: 0.7458\n",
      "Epoch 9/15 | Train Loss: 0.4298 Acc: 0.8051\n",
      "Epoch 10/15 | Train Loss: 0.4177 Acc: 0.8051\n",
      "Epoch 11/15 | Train Loss: 0.4028 Acc: 0.8263\n",
      "Epoch 12/15 | Train Loss: 0.4292 Acc: 0.7966\n",
      "Epoch 13/15 | Train Loss: 0.4048 Acc: 0.8263\n",
      "Epoch 14/15 | Train Loss: 0.3884 Acc: 0.8432\n",
      "Epoch 15/15 | Train Loss: 0.4126 Acc: 0.8008\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.7820 Acc: 0.5595\n",
      "Epoch 2/15 | Train Loss: 0.5004 Acc: 0.8106\n",
      "Epoch 3/15 | Train Loss: 0.5153 Acc: 0.7930\n",
      "Epoch 4/15 | Train Loss: 0.3937 Acc: 0.8370\n",
      "Epoch 5/15 | Train Loss: 0.3747 Acc: 0.8238\n",
      "Epoch 6/15 | Train Loss: 0.3569 Acc: 0.8502\n",
      "Epoch 7/15 | Train Loss: 0.4057 Acc: 0.8370\n",
      "Epoch 8/15 | Train Loss: 0.3600 Acc: 0.8678\n",
      "Epoch 9/15 | Train Loss: 0.3571 Acc: 0.8678\n",
      "Epoch 10/15 | Train Loss: 0.3247 Acc: 0.8722\n",
      "Epoch 11/15 | Train Loss: 0.3449 Acc: 0.8414\n",
      "Epoch 12/15 | Train Loss: 0.3325 Acc: 0.8767\n",
      "Epoch 13/15 | Train Loss: 0.3175 Acc: 0.8678\n",
      "Epoch 14/15 | Train Loss: 0.3025 Acc: 0.8811\n",
      "Epoch 15/15 | Train Loss: 0.2824 Acc: 0.8987\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6698 Acc: 0.6588\n",
      "Epoch 2/15 | Train Loss: 0.5215 Acc: 0.7297\n",
      "Epoch 3/15 | Train Loss: 0.5140 Acc: 0.7297\n",
      "Epoch 4/15 | Train Loss: 0.4116 Acc: 0.7872\n",
      "Epoch 5/15 | Train Loss: 0.4503 Acc: 0.7838\n",
      "Epoch 6/15 | Train Loss: 0.4148 Acc: 0.7669\n",
      "Epoch 7/15 | Train Loss: 0.4278 Acc: 0.7703\n",
      "Epoch 8/15 | Train Loss: 0.4257 Acc: 0.7905\n",
      "Epoch 9/15 | Train Loss: 0.3808 Acc: 0.8176\n",
      "Epoch 10/15 | Train Loss: 0.3671 Acc: 0.8108\n",
      "Epoch 11/15 | Train Loss: 0.3335 Acc: 0.8378\n",
      "Epoch 12/15 | Train Loss: 0.3515 Acc: 0.8378\n",
      "Epoch 13/15 | Train Loss: 0.3518 Acc: 0.8311\n",
      "Epoch 14/15 | Train Loss: 0.3207 Acc: 0.8480\n",
      "Epoch 15/15 | Train Loss: 0.3887 Acc: 0.8108\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7806 Acc: 0.6824\n",
      "Epoch 2/15 | Train Loss: 0.5915 Acc: 0.6993\n",
      "Epoch 3/15 | Train Loss: 0.5076 Acc: 0.7196\n",
      "Epoch 4/15 | Train Loss: 0.5328 Acc: 0.7196\n",
      "Epoch 5/15 | Train Loss: 0.4831 Acc: 0.7736\n",
      "Epoch 6/15 | Train Loss: 0.3923 Acc: 0.7905\n",
      "Epoch 7/15 | Train Loss: 0.4712 Acc: 0.7669\n",
      "Epoch 8/15 | Train Loss: 0.4268 Acc: 0.7669\n",
      "Epoch 9/15 | Train Loss: 0.3931 Acc: 0.8176\n",
      "Epoch 10/15 | Train Loss: 0.3604 Acc: 0.8277\n",
      "Epoch 11/15 | Train Loss: 0.3502 Acc: 0.8682\n",
      "Epoch 12/15 | Train Loss: 0.3589 Acc: 0.8412\n",
      "Epoch 13/15 | Train Loss: 0.3528 Acc: 0.8311\n",
      "Epoch 14/15 | Train Loss: 0.3610 Acc: 0.7973\n",
      "Epoch 15/15 | Train Loss: 0.3955 Acc: 0.8142\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5924 Acc: 0.6936\n",
      "Epoch 2/15 | Train Loss: 0.5631 Acc: 0.7104\n",
      "Epoch 3/15 | Train Loss: 0.5211 Acc: 0.7172\n",
      "Epoch 4/15 | Train Loss: 0.4767 Acc: 0.7239\n",
      "Epoch 5/15 | Train Loss: 0.4741 Acc: 0.7340\n",
      "Epoch 6/15 | Train Loss: 0.4565 Acc: 0.7374\n",
      "Epoch 7/15 | Train Loss: 0.3857 Acc: 0.8047\n",
      "Epoch 8/15 | Train Loss: 0.4095 Acc: 0.7845\n",
      "Epoch 9/15 | Train Loss: 0.4089 Acc: 0.7946\n",
      "Epoch 10/15 | Train Loss: 0.3275 Acc: 0.8451\n",
      "Epoch 11/15 | Train Loss: 0.3840 Acc: 0.8283\n",
      "Epoch 12/15 | Train Loss: 0.3890 Acc: 0.7946\n",
      "Epoch 13/15 | Train Loss: 0.3502 Acc: 0.8283\n",
      "Epoch 14/15 | Train Loss: 0.3951 Acc: 0.7845\n",
      "Epoch 15/15 | Train Loss: 0.3903 Acc: 0.7879\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6136 Acc: 0.7099\n",
      "Epoch 2/15 | Train Loss: 0.5004 Acc: 0.7710\n",
      "Epoch 3/15 | Train Loss: 0.4861 Acc: 0.7824\n",
      "Epoch 4/15 | Train Loss: 0.4207 Acc: 0.8092\n",
      "Epoch 5/15 | Train Loss: 0.4216 Acc: 0.8053\n",
      "Epoch 6/15 | Train Loss: 0.4446 Acc: 0.7901\n",
      "Epoch 7/15 | Train Loss: 0.3760 Acc: 0.8244\n",
      "Epoch 8/15 | Train Loss: 0.4534 Acc: 0.7748\n",
      "Epoch 9/15 | Train Loss: 0.3757 Acc: 0.8321\n",
      "Epoch 10/15 | Train Loss: 0.3697 Acc: 0.8511\n",
      "Epoch 11/15 | Train Loss: 0.3849 Acc: 0.8359\n",
      "Epoch 12/15 | Train Loss: 0.3484 Acc: 0.8626\n",
      "Epoch 13/15 | Train Loss: 0.3550 Acc: 0.8435\n",
      "Epoch 14/15 | Train Loss: 0.3897 Acc: 0.8130\n",
      "Epoch 15/15 | Train Loss: 0.3839 Acc: 0.8588\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5850 Acc: 0.7143\n",
      "Epoch 2/15 | Train Loss: 0.4139 Acc: 0.8490\n",
      "Epoch 3/15 | Train Loss: 0.4235 Acc: 0.8245\n",
      "Epoch 4/15 | Train Loss: 0.3635 Acc: 0.8490\n",
      "Epoch 5/15 | Train Loss: 0.3637 Acc: 0.8653\n",
      "Epoch 6/15 | Train Loss: 0.3260 Acc: 0.8490\n",
      "Epoch 7/15 | Train Loss: 0.2558 Acc: 0.9061\n",
      "Epoch 8/15 | Train Loss: 0.3291 Acc: 0.8694\n",
      "Epoch 9/15 | Train Loss: 0.3150 Acc: 0.8898\n",
      "Epoch 10/15 | Train Loss: 0.3133 Acc: 0.8653\n",
      "Epoch 11/15 | Train Loss: 0.3145 Acc: 0.8694\n",
      "Epoch 12/15 | Train Loss: 0.2807 Acc: 0.8694\n",
      "Epoch 13/15 | Train Loss: 0.2457 Acc: 0.8816\n",
      "Epoch 14/15 | Train Loss: 0.2969 Acc: 0.8980\n",
      "Epoch 15/15 | Train Loss: 0.2848 Acc: 0.8857\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6422 Acc: 0.6468\n",
      "Epoch 2/15 | Train Loss: 0.6605 Acc: 0.6667\n",
      "Epoch 3/15 | Train Loss: 0.6222 Acc: 0.6746\n",
      "Epoch 4/15 | Train Loss: 0.4928 Acc: 0.7381\n",
      "Epoch 5/15 | Train Loss: 0.5079 Acc: 0.7778\n",
      "Epoch 6/15 | Train Loss: 0.4799 Acc: 0.7897\n",
      "Epoch 7/15 | Train Loss: 0.4495 Acc: 0.7817\n",
      "Epoch 8/15 | Train Loss: 0.4028 Acc: 0.8452\n",
      "Epoch 9/15 | Train Loss: 0.3882 Acc: 0.8413\n",
      "Epoch 10/15 | Train Loss: 0.4500 Acc: 0.7857\n",
      "Epoch 11/15 | Train Loss: 0.3994 Acc: 0.8095\n",
      "Epoch 12/15 | Train Loss: 0.3980 Acc: 0.8214\n",
      "Epoch 13/15 | Train Loss: 0.3843 Acc: 0.8135\n",
      "Epoch 14/15 | Train Loss: 0.4221 Acc: 0.7778\n",
      "Epoch 15/15 | Train Loss: 0.3348 Acc: 0.8492\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7500 Acc: 0.6190\n",
      "Epoch 2/15 | Train Loss: 0.6601 Acc: 0.6310\n",
      "Epoch 3/15 | Train Loss: 0.6121 Acc: 0.6825\n",
      "Epoch 4/15 | Train Loss: 0.5886 Acc: 0.6746\n",
      "Epoch 5/15 | Train Loss: 0.5616 Acc: 0.6825\n",
      "Epoch 6/15 | Train Loss: 0.5541 Acc: 0.7063\n",
      "Epoch 7/15 | Train Loss: 0.4980 Acc: 0.7222\n",
      "Epoch 8/15 | Train Loss: 0.5081 Acc: 0.7421\n",
      "Epoch 9/15 | Train Loss: 0.4575 Acc: 0.7421\n",
      "Epoch 10/15 | Train Loss: 0.4739 Acc: 0.7659\n",
      "Epoch 11/15 | Train Loss: 0.4073 Acc: 0.8294\n",
      "Epoch 12/15 | Train Loss: 0.4183 Acc: 0.7778\n",
      "Epoch 13/15 | Train Loss: 0.5055 Acc: 0.7302\n",
      "Epoch 14/15 | Train Loss: 0.4519 Acc: 0.7897\n",
      "Epoch 15/15 | Train Loss: 0.4378 Acc: 0.7857\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6721 Acc: 0.6561\n",
      "Epoch 2/15 | Train Loss: 0.6529 Acc: 0.6561\n",
      "Epoch 3/15 | Train Loss: 0.6085 Acc: 0.6640\n",
      "Epoch 4/15 | Train Loss: 0.5216 Acc: 0.7312\n",
      "Epoch 5/15 | Train Loss: 0.5218 Acc: 0.7431\n",
      "Epoch 6/15 | Train Loss: 0.4765 Acc: 0.7194\n",
      "Epoch 7/15 | Train Loss: 0.4489 Acc: 0.7826\n",
      "Epoch 8/15 | Train Loss: 0.4688 Acc: 0.7352\n",
      "Epoch 9/15 | Train Loss: 0.4698 Acc: 0.8024\n",
      "Epoch 10/15 | Train Loss: 0.4795 Acc: 0.7510\n",
      "Epoch 11/15 | Train Loss: 0.4247 Acc: 0.8103\n",
      "Epoch 12/15 | Train Loss: 0.3792 Acc: 0.8340\n",
      "Epoch 13/15 | Train Loss: 0.4024 Acc: 0.8024\n",
      "Epoch 14/15 | Train Loss: 0.4101 Acc: 0.8182\n",
      "Epoch 15/15 | Train Loss: 0.4093 Acc: 0.7905\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7063 Acc: 0.6398\n",
      "Epoch 2/15 | Train Loss: 0.5537 Acc: 0.7415\n",
      "Epoch 3/15 | Train Loss: 0.5244 Acc: 0.7500\n",
      "Epoch 4/15 | Train Loss: 0.4854 Acc: 0.7797\n",
      "Epoch 5/15 | Train Loss: 0.4626 Acc: 0.7712\n",
      "Epoch 6/15 | Train Loss: 0.4958 Acc: 0.8008\n",
      "Epoch 7/15 | Train Loss: 0.4190 Acc: 0.8263\n",
      "Epoch 8/15 | Train Loss: 0.4195 Acc: 0.7881\n",
      "Epoch 9/15 | Train Loss: 0.3823 Acc: 0.8263\n",
      "Epoch 10/15 | Train Loss: 0.3822 Acc: 0.8390\n",
      "Epoch 11/15 | Train Loss: 0.3791 Acc: 0.8220\n",
      "Epoch 12/15 | Train Loss: 0.4060 Acc: 0.8347\n",
      "Epoch 13/15 | Train Loss: 0.4114 Acc: 0.8263\n",
      "Epoch 14/15 | Train Loss: 0.3875 Acc: 0.8475\n",
      "Epoch 15/15 | Train Loss: 0.4231 Acc: 0.8390\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5869 Acc: 0.7181\n",
      "Epoch 2/15 | Train Loss: 0.5167 Acc: 0.8106\n",
      "Epoch 3/15 | Train Loss: 0.3804 Acc: 0.8238\n",
      "Epoch 4/15 | Train Loss: 0.4008 Acc: 0.8326\n",
      "Epoch 5/15 | Train Loss: 0.3529 Acc: 0.8458\n",
      "Epoch 6/15 | Train Loss: 0.3609 Acc: 0.8855\n",
      "Epoch 7/15 | Train Loss: 0.3601 Acc: 0.8634\n",
      "Epoch 8/15 | Train Loss: 0.3617 Acc: 0.8414\n",
      "Epoch 9/15 | Train Loss: 0.3640 Acc: 0.8678\n",
      "Epoch 10/15 | Train Loss: 0.3127 Acc: 0.8943\n",
      "Epoch 11/15 | Train Loss: 0.2842 Acc: 0.8855\n",
      "Epoch 12/15 | Train Loss: 0.3088 Acc: 0.8722\n",
      "Epoch 13/15 | Train Loss: 0.3092 Acc: 0.8855\n",
      "Epoch 14/15 | Train Loss: 0.2697 Acc: 0.9031\n",
      "Epoch 15/15 | Train Loss: 0.2889 Acc: 0.8678\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5664 Acc: 0.7781\n",
      "Epoch 2/15 | Train Loss: 0.4267 Acc: 0.7676\n",
      "Epoch 3/15 | Train Loss: 0.3863 Acc: 0.7911\n",
      "Epoch 4/15 | Train Loss: 0.3883 Acc: 0.7911\n",
      "Epoch 5/15 | Train Loss: 0.3510 Acc: 0.8016\n",
      "Epoch 6/15 | Train Loss: 0.3351 Acc: 0.8277\n",
      "Epoch 7/15 | Train Loss: 0.3639 Acc: 0.8042\n",
      "Epoch 8/15 | Train Loss: 0.3299 Acc: 0.8277\n",
      "Epoch 9/15 | Train Loss: 0.3029 Acc: 0.8616\n",
      "Epoch 10/15 | Train Loss: 0.2950 Acc: 0.8616\n",
      "Epoch 11/15 | Train Loss: 0.2767 Acc: 0.8668\n",
      "Epoch 12/15 | Train Loss: 0.2983 Acc: 0.8564\n",
      "Epoch 13/15 | Train Loss: 0.2922 Acc: 0.8668\n",
      "Epoch 14/15 | Train Loss: 0.2879 Acc: 0.8460\n",
      "Epoch 15/15 | Train Loss: 0.2877 Acc: 0.8512\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5693 Acc: 0.7337\n",
      "Epoch 2/15 | Train Loss: 0.4563 Acc: 0.7781\n",
      "Epoch 3/15 | Train Loss: 0.4136 Acc: 0.7676\n",
      "Epoch 4/15 | Train Loss: 0.3733 Acc: 0.8042\n",
      "Epoch 5/15 | Train Loss: 0.3593 Acc: 0.8016\n",
      "Epoch 6/15 | Train Loss: 0.3350 Acc: 0.8303\n",
      "Epoch 7/15 | Train Loss: 0.3843 Acc: 0.7990\n",
      "Epoch 8/15 | Train Loss: 0.3112 Acc: 0.8355\n",
      "Epoch 9/15 | Train Loss: 0.3116 Acc: 0.8381\n",
      "Epoch 10/15 | Train Loss: 0.2808 Acc: 0.8747\n",
      "Epoch 11/15 | Train Loss: 0.2984 Acc: 0.8407\n",
      "Epoch 12/15 | Train Loss: 0.2938 Acc: 0.8460\n",
      "Epoch 13/15 | Train Loss: 0.2837 Acc: 0.8642\n",
      "Epoch 14/15 | Train Loss: 0.2825 Acc: 0.8590\n",
      "Epoch 15/15 | Train Loss: 0.2976 Acc: 0.8512\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.6038 Acc: 0.7682\n",
      "Epoch 2/15 | Train Loss: 0.5003 Acc: 0.7526\n",
      "Epoch 3/15 | Train Loss: 0.4018 Acc: 0.7812\n",
      "Epoch 4/15 | Train Loss: 0.3911 Acc: 0.7917\n",
      "Epoch 5/15 | Train Loss: 0.3833 Acc: 0.7891\n",
      "Epoch 6/15 | Train Loss: 0.3449 Acc: 0.8073\n",
      "Epoch 7/15 | Train Loss: 0.3572 Acc: 0.8177\n",
      "Epoch 8/15 | Train Loss: 0.3071 Acc: 0.8594\n",
      "Epoch 9/15 | Train Loss: 0.2944 Acc: 0.8542\n",
      "Epoch 10/15 | Train Loss: 0.2959 Acc: 0.8568\n",
      "Epoch 11/15 | Train Loss: 0.3066 Acc: 0.8490\n",
      "Epoch 12/15 | Train Loss: 0.2886 Acc: 0.8620\n",
      "Epoch 13/15 | Train Loss: 0.3027 Acc: 0.8464\n",
      "Epoch 14/15 | Train Loss: 0.3038 Acc: 0.8359\n",
      "Epoch 15/15 | Train Loss: 0.2927 Acc: 0.8698\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4720 Acc: 0.7803\n",
      "Epoch 2/15 | Train Loss: 0.4098 Acc: 0.8248\n",
      "Epoch 3/15 | Train Loss: 0.3712 Acc: 0.8344\n",
      "Epoch 4/15 | Train Loss: 0.3940 Acc: 0.8025\n",
      "Epoch 5/15 | Train Loss: 0.3407 Acc: 0.8471\n",
      "Epoch 6/15 | Train Loss: 0.3495 Acc: 0.8471\n",
      "Epoch 7/15 | Train Loss: 0.3479 Acc: 0.8312\n",
      "Epoch 8/15 | Train Loss: 0.3252 Acc: 0.8567\n",
      "Epoch 9/15 | Train Loss: 0.3141 Acc: 0.8567\n",
      "Epoch 10/15 | Train Loss: 0.3066 Acc: 0.8439\n",
      "Epoch 11/15 | Train Loss: 0.2929 Acc: 0.8631\n",
      "Epoch 12/15 | Train Loss: 0.2901 Acc: 0.8726\n",
      "Epoch 13/15 | Train Loss: 0.2907 Acc: 0.8694\n",
      "Epoch 14/15 | Train Loss: 0.3116 Acc: 0.8599\n",
      "Epoch 15/15 | Train Loss: 0.3071 Acc: 0.8567\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6482 Acc: 0.7393\n",
      "Epoch 2/15 | Train Loss: 0.3661 Acc: 0.8679\n",
      "Epoch 3/15 | Train Loss: 0.3375 Acc: 0.8571\n",
      "Epoch 4/15 | Train Loss: 0.2901 Acc: 0.8857\n",
      "Epoch 5/15 | Train Loss: 0.3164 Acc: 0.8714\n",
      "Epoch 6/15 | Train Loss: 0.3011 Acc: 0.8857\n",
      "Epoch 7/15 | Train Loss: 0.2900 Acc: 0.8857\n",
      "Epoch 8/15 | Train Loss: 0.2926 Acc: 0.8786\n",
      "Epoch 9/15 | Train Loss: 0.3177 Acc: 0.8786\n",
      "Epoch 10/15 | Train Loss: 0.2427 Acc: 0.8964\n",
      "Epoch 11/15 | Train Loss: 0.2878 Acc: 0.8893\n",
      "Epoch 12/15 | Train Loss: 0.2713 Acc: 0.8786\n",
      "Epoch 13/15 | Train Loss: 0.2319 Acc: 0.9250\n",
      "Epoch 14/15 | Train Loss: 0.2352 Acc: 0.9143\n",
      "Epoch 15/15 | Train Loss: 0.2335 Acc: 0.9071\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7553 Acc: 0.6111\n",
      "Epoch 2/15 | Train Loss: 0.5966 Acc: 0.7143\n",
      "Epoch 3/15 | Train Loss: 0.6301 Acc: 0.6587\n",
      "Epoch 4/15 | Train Loss: 0.4818 Acc: 0.7381\n",
      "Epoch 5/15 | Train Loss: 0.5809 Acc: 0.7183\n",
      "Epoch 6/15 | Train Loss: 0.5185 Acc: 0.7063\n",
      "Epoch 7/15 | Train Loss: 0.4725 Acc: 0.7500\n",
      "Epoch 8/15 | Train Loss: 0.4639 Acc: 0.7619\n",
      "Epoch 9/15 | Train Loss: 0.3861 Acc: 0.8135\n",
      "Epoch 10/15 | Train Loss: 0.4308 Acc: 0.7738\n",
      "Epoch 11/15 | Train Loss: 0.3535 Acc: 0.8532\n",
      "Epoch 12/15 | Train Loss: 0.4162 Acc: 0.8254\n",
      "Epoch 13/15 | Train Loss: 0.4379 Acc: 0.7698\n",
      "Epoch 14/15 | Train Loss: 0.4030 Acc: 0.8016\n",
      "Epoch 15/15 | Train Loss: 0.3693 Acc: 0.8333\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6654 Acc: 0.6111\n",
      "Epoch 2/15 | Train Loss: 0.6288 Acc: 0.6627\n",
      "Epoch 3/15 | Train Loss: 0.5424 Acc: 0.7302\n",
      "Epoch 4/15 | Train Loss: 0.5379 Acc: 0.7381\n",
      "Epoch 5/15 | Train Loss: 0.5481 Acc: 0.7143\n",
      "Epoch 6/15 | Train Loss: 0.5314 Acc: 0.7024\n",
      "Epoch 7/15 | Train Loss: 0.4910 Acc: 0.7500\n",
      "Epoch 8/15 | Train Loss: 0.4093 Acc: 0.8135\n",
      "Epoch 9/15 | Train Loss: 0.4565 Acc: 0.7579\n",
      "Epoch 10/15 | Train Loss: 0.4530 Acc: 0.7619\n",
      "Epoch 11/15 | Train Loss: 0.4135 Acc: 0.7897\n",
      "Epoch 12/15 | Train Loss: 0.4137 Acc: 0.7937\n",
      "Epoch 13/15 | Train Loss: 0.4091 Acc: 0.7976\n",
      "Epoch 14/15 | Train Loss: 0.4284 Acc: 0.7778\n",
      "Epoch 15/15 | Train Loss: 0.3917 Acc: 0.8214\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7834 Acc: 0.6640\n",
      "Epoch 2/15 | Train Loss: 0.6912 Acc: 0.6443\n",
      "Epoch 3/15 | Train Loss: 0.5943 Acc: 0.7154\n",
      "Epoch 4/15 | Train Loss: 0.5045 Acc: 0.7233\n",
      "Epoch 5/15 | Train Loss: 0.5593 Acc: 0.7233\n",
      "Epoch 6/15 | Train Loss: 0.6088 Acc: 0.6680\n",
      "Epoch 7/15 | Train Loss: 0.5176 Acc: 0.7233\n",
      "Epoch 8/15 | Train Loss: 0.5224 Acc: 0.7589\n",
      "Epoch 9/15 | Train Loss: 0.4416 Acc: 0.7787\n",
      "Epoch 10/15 | Train Loss: 0.4623 Acc: 0.7668\n",
      "Epoch 11/15 | Train Loss: 0.4501 Acc: 0.7628\n",
      "Epoch 12/15 | Train Loss: 0.4455 Acc: 0.7747\n",
      "Epoch 13/15 | Train Loss: 0.4777 Acc: 0.7549\n",
      "Epoch 14/15 | Train Loss: 0.3787 Acc: 0.8261\n",
      "Epoch 15/15 | Train Loss: 0.4455 Acc: 0.8024\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5687 Acc: 0.7288\n",
      "Epoch 2/15 | Train Loss: 0.6085 Acc: 0.7542\n",
      "Epoch 3/15 | Train Loss: 0.4802 Acc: 0.7754\n",
      "Epoch 4/15 | Train Loss: 0.4330 Acc: 0.8008\n",
      "Epoch 5/15 | Train Loss: 0.5121 Acc: 0.8178\n",
      "Epoch 6/15 | Train Loss: 0.5194 Acc: 0.8051\n",
      "Epoch 7/15 | Train Loss: 0.4571 Acc: 0.7797\n",
      "Epoch 8/15 | Train Loss: 0.3913 Acc: 0.8093\n",
      "Epoch 9/15 | Train Loss: 0.4281 Acc: 0.8136\n",
      "Epoch 10/15 | Train Loss: 0.3886 Acc: 0.8347\n",
      "Epoch 11/15 | Train Loss: 0.4597 Acc: 0.7881\n",
      "Epoch 12/15 | Train Loss: 0.3957 Acc: 0.8136\n",
      "Epoch 13/15 | Train Loss: 0.3886 Acc: 0.8008\n",
      "Epoch 14/15 | Train Loss: 0.3937 Acc: 0.8178\n",
      "Epoch 15/15 | Train Loss: 0.3896 Acc: 0.8305\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.6099 Acc: 0.6740\n",
      "Epoch 2/15 | Train Loss: 0.5078 Acc: 0.7930\n",
      "Epoch 3/15 | Train Loss: 0.4426 Acc: 0.8414\n",
      "Epoch 4/15 | Train Loss: 0.4316 Acc: 0.8282\n",
      "Epoch 5/15 | Train Loss: 0.3760 Acc: 0.8546\n",
      "Epoch 6/15 | Train Loss: 0.3827 Acc: 0.8458\n",
      "Epoch 7/15 | Train Loss: 0.3336 Acc: 0.8855\n",
      "Epoch 8/15 | Train Loss: 0.3218 Acc: 0.8767\n",
      "Epoch 9/15 | Train Loss: 0.3259 Acc: 0.8634\n",
      "Epoch 10/15 | Train Loss: 0.2997 Acc: 0.8811\n",
      "Epoch 11/15 | Train Loss: 0.3225 Acc: 0.8811\n",
      "Epoch 12/15 | Train Loss: 0.3004 Acc: 0.8855\n",
      "Epoch 13/15 | Train Loss: 0.2673 Acc: 0.9119\n",
      "Epoch 14/15 | Train Loss: 0.2760 Acc: 0.8678\n",
      "Epoch 15/15 | Train Loss: 0.2810 Acc: 0.8855\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.6min\n",
      "Epoch 1/15 | Train Loss: 0.5642 Acc: 0.7431\n",
      "Epoch 2/15 | Train Loss: 0.4639 Acc: 0.7752\n",
      "Epoch 3/15 | Train Loss: 0.4178 Acc: 0.7706\n",
      "Epoch 4/15 | Train Loss: 0.3925 Acc: 0.7982\n",
      "Epoch 5/15 | Train Loss: 0.3653 Acc: 0.8211\n",
      "Epoch 6/15 | Train Loss: 0.3764 Acc: 0.8257\n",
      "Epoch 7/15 | Train Loss: 0.3711 Acc: 0.8028\n",
      "Epoch 8/15 | Train Loss: 0.3547 Acc: 0.8028\n",
      "Epoch 9/15 | Train Loss: 0.3222 Acc: 0.8463\n",
      "Epoch 10/15 | Train Loss: 0.3109 Acc: 0.8601\n",
      "Epoch 11/15 | Train Loss: 0.2971 Acc: 0.8578\n",
      "Epoch 12/15 | Train Loss: 0.2931 Acc: 0.8693\n",
      "Epoch 13/15 | Train Loss: 0.3219 Acc: 0.8601\n",
      "Epoch 14/15 | Train Loss: 0.2779 Acc: 0.8624\n",
      "Epoch 15/15 | Train Loss: 0.2763 Acc: 0.8761\n",
      "Fold 8 Test Accuracy: 0.7500\n",
      "===== Fold 9 =====\n",
      "Epoch 1: Generator Loss = 10.0939, Discriminator Loss = 8.7037\n",
      "Epoch 2: Generator Loss = 10.7135, Discriminator Loss = 8.2187\n",
      "Epoch 3: Generator Loss = 14.0313, Discriminator Loss = 5.6718\n",
      "Epoch 4: Generator Loss = 22.7245, Discriminator Loss = 4.8828\n",
      "Epoch 5: Generator Loss = 31.4325, Discriminator Loss = 3.0187\n",
      "Epoch 6: Generator Loss = 31.6343, Discriminator Loss = 4.9045\n",
      "Epoch 7: Generator Loss = 32.2382, Discriminator Loss = 5.5030\n",
      "Epoch 8: Generator Loss = 25.5845, Discriminator Loss = 6.0995\n",
      "Epoch 9: Generator Loss = 25.7251, Discriminator Loss = 6.0081\n",
      "Epoch 10: Generator Loss = 24.0155, Discriminator Loss = 6.0183\n",
      "Epoch 11: Generator Loss = 23.9039, Discriminator Loss = 8.8027\n",
      "Epoch 12: Generator Loss = 22.6841, Discriminator Loss = 7.3616\n",
      "Epoch 13: Generator Loss = 24.9222, Discriminator Loss = 7.1174\n",
      "Epoch 14: Generator Loss = 25.0017, Discriminator Loss = 6.0228\n",
      "Epoch 15: Generator Loss = 24.8859, Discriminator Loss = 8.5940\n",
      "Epoch 16: Generator Loss = 21.7277, Discriminator Loss = 8.6117\n",
      "Epoch 17: Generator Loss = 22.1614, Discriminator Loss = 8.2125\n",
      "Epoch 18: Generator Loss = 21.9429, Discriminator Loss = 7.3992\n",
      "Epoch 19: Generator Loss = 19.3336, Discriminator Loss = 8.1059\n",
      "Epoch 20: Generator Loss = 19.4552, Discriminator Loss = 8.5216\n",
      "Epoch 21: Generator Loss = 20.7448, Discriminator Loss = 7.5537\n",
      "Epoch 22: Generator Loss = 21.1581, Discriminator Loss = 7.9995\n",
      "Epoch 23: Generator Loss = 18.2250, Discriminator Loss = 7.5573\n",
      "Epoch 24: Generator Loss = 21.4872, Discriminator Loss = 7.5549\n",
      "Epoch 25: Generator Loss = 26.1639, Discriminator Loss = 7.8793\n",
      "Epoch 26: Generator Loss = 20.0872, Discriminator Loss = 7.1325\n",
      "Epoch 27: Generator Loss = 23.8574, Discriminator Loss = 8.1411\n",
      "Epoch 28: Generator Loss = 21.1541, Discriminator Loss = 7.4153\n",
      "Epoch 29: Generator Loss = 21.5033, Discriminator Loss = 8.5052\n",
      "Epoch 30: Generator Loss = 19.6781, Discriminator Loss = 11.0008\n",
      "Epoch 31: Generator Loss = 16.4368, Discriminator Loss = 8.0893\n",
      "Epoch 32: Generator Loss = 19.5376, Discriminator Loss = 7.8181\n",
      "Epoch 33: Generator Loss = 18.7449, Discriminator Loss = 7.8017\n",
      "Epoch 34: Generator Loss = 14.7819, Discriminator Loss = 7.8119\n",
      "Epoch 35: Generator Loss = 18.2909, Discriminator Loss = 6.6453\n",
      "Epoch 36: Generator Loss = 21.0638, Discriminator Loss = 8.5726\n",
      "Epoch 37: Generator Loss = 23.8527, Discriminator Loss = 6.4985\n",
      "Epoch 38: Generator Loss = 23.8825, Discriminator Loss = 9.4935\n",
      "Epoch 39: Generator Loss = 19.9401, Discriminator Loss = 7.8649\n",
      "Epoch 40: Generator Loss = 16.9223, Discriminator Loss = 7.8091\n",
      "Epoch 41: Generator Loss = 21.1926, Discriminator Loss = 7.4208\n",
      "Epoch 42: Generator Loss = 21.2919, Discriminator Loss = 7.9164\n",
      "Epoch 43: Generator Loss = 22.9443, Discriminator Loss = 8.1074\n",
      "Epoch 44: Generator Loss = 18.0792, Discriminator Loss = 6.0088\n",
      "Epoch 45: Generator Loss = 25.6036, Discriminator Loss = 6.9456\n",
      "Epoch 46: Generator Loss = 20.1724, Discriminator Loss = 6.4730\n",
      "Epoch 47: Generator Loss = 26.0927, Discriminator Loss = 6.8262\n",
      "Epoch 48: Generator Loss = 18.0062, Discriminator Loss = 6.3644\n",
      "Epoch 49: Generator Loss = 26.4782, Discriminator Loss = 5.9750\n",
      "Epoch 50: Generator Loss = 29.2034, Discriminator Loss = 5.1407\n",
      "Epoch 51: Generator Loss = 27.8777, Discriminator Loss = 4.8951\n",
      "Epoch 52: Generator Loss = 29.4666, Discriminator Loss = 6.4242\n",
      "Epoch 53: Generator Loss = 29.9969, Discriminator Loss = 6.1365\n",
      "Epoch 54: Generator Loss = 28.8928, Discriminator Loss = 7.0255\n",
      "Epoch 55: Generator Loss = 25.9794, Discriminator Loss = 5.4688\n",
      "Epoch 56: Generator Loss = 29.8186, Discriminator Loss = 3.8286\n",
      "Epoch 57: Generator Loss = 27.0787, Discriminator Loss = 5.9725\n",
      "Epoch 58: Generator Loss = 32.8870, Discriminator Loss = 3.5683\n",
      "Epoch 59: Generator Loss = 47.4441, Discriminator Loss = 3.9762\n",
      "Epoch 60: Generator Loss = 30.6079, Discriminator Loss = 4.3634\n",
      "Epoch 61: Generator Loss = 36.7510, Discriminator Loss = 3.3790\n",
      "Epoch 62: Generator Loss = 45.9714, Discriminator Loss = 2.4133\n",
      "Epoch 63: Generator Loss = 45.7062, Discriminator Loss = 2.1994\n",
      "Epoch 64: Generator Loss = 39.9880, Discriminator Loss = 2.1524\n",
      "Epoch 65: Generator Loss = 48.0726, Discriminator Loss = 5.8779\n",
      "Epoch 66: Generator Loss = 40.2613, Discriminator Loss = 2.2755\n",
      "Epoch 67: Generator Loss = 50.1844, Discriminator Loss = 2.6677\n",
      "Epoch 68: Generator Loss = 43.7154, Discriminator Loss = 2.7028\n",
      "Epoch 69: Generator Loss = 39.4746, Discriminator Loss = 2.5500\n",
      "Epoch 70: Generator Loss = 47.5715, Discriminator Loss = 5.7419\n",
      "Epoch 71: Generator Loss = 47.2299, Discriminator Loss = 1.8697\n",
      "Epoch 72: Generator Loss = 43.3573, Discriminator Loss = 1.6850\n",
      "Epoch 73: Generator Loss = 57.9686, Discriminator Loss = 1.3680\n",
      "Epoch 74: Generator Loss = 52.1059, Discriminator Loss = 2.0126\n",
      "Epoch 75: Generator Loss = 55.1389, Discriminator Loss = 1.3917\n",
      "Epoch 76: Generator Loss = 56.1282, Discriminator Loss = 4.0573\n",
      "Epoch 77: Generator Loss = 55.8547, Discriminator Loss = 2.1229\n",
      "Epoch 78: Generator Loss = 47.8777, Discriminator Loss = 1.0851\n",
      "Epoch 79: Generator Loss = 58.1560, Discriminator Loss = 1.3447\n",
      "Epoch 80: Generator Loss = 47.1539, Discriminator Loss = 1.8149\n",
      "Epoch 81: Generator Loss = 66.3213, Discriminator Loss = 2.6458\n",
      "Epoch 82: Generator Loss = 65.4692, Discriminator Loss = 0.7275\n",
      "Epoch 83: Generator Loss = 70.3731, Discriminator Loss = 0.8866\n",
      "Epoch 84: Generator Loss = 96.2535, Discriminator Loss = 0.4718\n",
      "Epoch 85: Generator Loss = 81.0002, Discriminator Loss = 0.6814\n",
      "Epoch 86: Generator Loss = 101.9279, Discriminator Loss = 1.8805\n",
      "Epoch 87: Generator Loss = 148.1084, Discriminator Loss = 2.2007\n",
      "Epoch 88: Generator Loss = 155.6046, Discriminator Loss = 1.1433\n",
      "Epoch 89: Generator Loss = 128.6964, Discriminator Loss = 0.5490\n",
      "Epoch 90: Generator Loss = 130.6067, Discriminator Loss = 0.2120\n",
      "Epoch 91: Generator Loss = 115.2166, Discriminator Loss = 0.1453\n",
      "Epoch 92: Generator Loss = 106.7891, Discriminator Loss = 0.6121\n",
      "Epoch 93: Generator Loss = 99.8645, Discriminator Loss = 0.1538\n",
      "Epoch 94: Generator Loss = 84.5410, Discriminator Loss = 0.2974\n",
      "Epoch 95: Generator Loss = 92.2880, Discriminator Loss = 0.4464\n",
      "Epoch 96: Generator Loss = 62.1994, Discriminator Loss = 2.2595\n",
      "Epoch 97: Generator Loss = 76.8248, Discriminator Loss = 3.0377\n",
      "Epoch 98: Generator Loss = 63.9819, Discriminator Loss = 3.8523\n",
      "Epoch 99: Generator Loss = 49.8583, Discriminator Loss = 2.6226\n",
      "Epoch 100: Generator Loss = 72.5477, Discriminator Loss = 2.3294\n",
      "Epoch 101: Generator Loss = 61.3594, Discriminator Loss = 4.9150\n",
      "Epoch 102: Generator Loss = 65.6844, Discriminator Loss = 2.7792\n",
      "Epoch 103: Generator Loss = 69.1067, Discriminator Loss = 1.1357\n",
      "Epoch 104: Generator Loss = 71.9693, Discriminator Loss = 0.8043\n",
      "Epoch 105: Generator Loss = 53.3721, Discriminator Loss = 1.9202\n",
      "Epoch 106: Generator Loss = 68.2154, Discriminator Loss = 0.8850\n",
      "Epoch 107: Generator Loss = 72.8474, Discriminator Loss = 0.7203\n",
      "Epoch 108: Generator Loss = 70.0664, Discriminator Loss = 0.7400\n",
      "Epoch 109: Generator Loss = 66.7472, Discriminator Loss = 1.0381\n",
      "Epoch 110: Generator Loss = 66.5205, Discriminator Loss = 0.9097\n",
      "Epoch 111: Generator Loss = 75.9615, Discriminator Loss = 1.3186\n",
      "Epoch 112: Generator Loss = 73.0940, Discriminator Loss = 1.0418\n",
      "Epoch 113: Generator Loss = 76.4427, Discriminator Loss = 0.8834\n",
      "Epoch 114: Generator Loss = 65.0327, Discriminator Loss = 0.8832\n",
      "Epoch 115: Generator Loss = 74.5657, Discriminator Loss = 0.5683\n",
      "Epoch 116: Generator Loss = 69.2055, Discriminator Loss = 2.9876\n",
      "Epoch 117: Generator Loss = 69.6852, Discriminator Loss = 1.2815\n",
      "Epoch 118: Generator Loss = 78.6714, Discriminator Loss = 0.5654\n",
      "Epoch 119: Generator Loss = 86.8886, Discriminator Loss = 0.3649\n",
      "Epoch 120: Generator Loss = 81.4219, Discriminator Loss = 0.5956\n",
      "Epoch 121: Generator Loss = 70.8816, Discriminator Loss = 0.6085\n",
      "Epoch 122: Generator Loss = 75.2585, Discriminator Loss = 0.6251\n",
      "Epoch 123: Generator Loss = 94.5030, Discriminator Loss = 1.7022\n",
      "Epoch 124: Generator Loss = 88.1422, Discriminator Loss = 1.1785\n",
      "Epoch 125: Generator Loss = 68.5726, Discriminator Loss = 1.0650\n",
      "Epoch 126: Generator Loss = 54.6814, Discriminator Loss = 13.8158\n",
      "Epoch 127: Generator Loss = 43.7998, Discriminator Loss = 5.1931\n",
      "Epoch 128: Generator Loss = 38.8763, Discriminator Loss = 8.8464\n",
      "Epoch 129: Generator Loss = 45.9413, Discriminator Loss = 2.4437\n",
      "Epoch 130: Generator Loss = 55.5142, Discriminator Loss = 1.8201\n",
      "Epoch 131: Generator Loss = 61.2509, Discriminator Loss = 1.2064\n",
      "Epoch 132: Generator Loss = 68.8284, Discriminator Loss = 0.8703\n",
      "Epoch 133: Generator Loss = 73.0421, Discriminator Loss = 0.6553\n",
      "Epoch 134: Generator Loss = 69.2712, Discriminator Loss = 0.6617\n",
      "Epoch 135: Generator Loss = 70.2900, Discriminator Loss = 0.9842\n",
      "Epoch 136: Generator Loss = 77.1238, Discriminator Loss = 0.8928\n",
      "Epoch 137: Generator Loss = 81.5304, Discriminator Loss = 0.6297\n",
      "Epoch 138: Generator Loss = 73.8360, Discriminator Loss = 1.1020\n",
      "Epoch 139: Generator Loss = 67.8004, Discriminator Loss = 0.7651\n",
      "Epoch 140: Generator Loss = 76.7944, Discriminator Loss = 0.8279\n",
      "Epoch 141: Generator Loss = 84.9118, Discriminator Loss = 0.5891\n",
      "Epoch 142: Generator Loss = 77.5537, Discriminator Loss = 0.5143\n",
      "Epoch 143: Generator Loss = 89.8076, Discriminator Loss = 0.5492\n",
      "Epoch 144: Generator Loss = 71.7037, Discriminator Loss = 0.8841\n",
      "Epoch 145: Generator Loss = 93.5309, Discriminator Loss = 0.5598\n",
      "Epoch 146: Generator Loss = 72.2904, Discriminator Loss = 0.7197\n",
      "Epoch 147: Generator Loss = 73.4929, Discriminator Loss = 0.4411\n",
      "Epoch 148: Generator Loss = 75.3152, Discriminator Loss = 0.4429\n",
      "Epoch 149: Generator Loss = 89.0694, Discriminator Loss = 0.2993\n",
      "Epoch 150: Generator Loss = 82.2698, Discriminator Loss = 0.3183\n",
      "Epoch 151: Generator Loss = 78.8884, Discriminator Loss = 0.5730\n",
      "Epoch 152: Generator Loss = 82.0701, Discriminator Loss = 0.2956\n",
      "Epoch 153: Generator Loss = 85.4353, Discriminator Loss = 0.4364\n",
      "Epoch 154: Generator Loss = 62.5532, Discriminator Loss = 0.7269\n",
      "Epoch 155: Generator Loss = 78.0166, Discriminator Loss = 0.7113\n",
      "Epoch 156: Generator Loss = 89.1413, Discriminator Loss = 1.2328\n",
      "Epoch 157: Generator Loss = 88.5710, Discriminator Loss = 0.7714\n",
      "Epoch 158: Generator Loss = 86.3057, Discriminator Loss = 0.6500\n",
      "Epoch 159: Generator Loss = 75.9908, Discriminator Loss = 1.4010\n",
      "Epoch 160: Generator Loss = 71.3590, Discriminator Loss = 5.8154\n",
      "Epoch 161: Generator Loss = 45.5997, Discriminator Loss = 4.9764\n",
      "Epoch 162: Generator Loss = 56.8088, Discriminator Loss = 1.1330\n",
      "Epoch 163: Generator Loss = 70.5553, Discriminator Loss = 0.8245\n",
      "Epoch 164: Generator Loss = 75.3187, Discriminator Loss = 0.8064\n",
      "Epoch 165: Generator Loss = 77.2128, Discriminator Loss = 2.4542\n",
      "Epoch 166: Generator Loss = 66.3775, Discriminator Loss = 1.5681\n",
      "Epoch 167: Generator Loss = 67.3548, Discriminator Loss = 1.2009\n",
      "Epoch 168: Generator Loss = 86.7332, Discriminator Loss = 1.0280\n",
      "Epoch 169: Generator Loss = 88.4110, Discriminator Loss = 0.6761\n",
      "Epoch 170: Generator Loss = 84.1282, Discriminator Loss = 1.3199\n",
      "Epoch 171: Generator Loss = 78.3189, Discriminator Loss = 1.0831\n",
      "Epoch 172: Generator Loss = 104.7967, Discriminator Loss = 2.3042\n",
      "Epoch 173: Generator Loss = 97.3325, Discriminator Loss = 0.6452\n",
      "Epoch 174: Generator Loss = 67.3096, Discriminator Loss = 0.9246\n",
      "Epoch 175: Generator Loss = 81.5852, Discriminator Loss = 1.1230\n",
      "Epoch 176: Generator Loss = 73.1155, Discriminator Loss = 0.8773\n",
      "Epoch 177: Generator Loss = 54.2456, Discriminator Loss = 0.9811\n",
      "Epoch 178: Generator Loss = 89.0491, Discriminator Loss = 1.3464\n",
      "Epoch 179: Generator Loss = 64.1364, Discriminator Loss = 1.8363\n",
      "Epoch 180: Generator Loss = 69.2393, Discriminator Loss = 1.8313\n",
      "Epoch 181: Generator Loss = 84.3015, Discriminator Loss = 1.2708\n",
      "Epoch 182: Generator Loss = 76.3500, Discriminator Loss = 1.5608\n",
      "Epoch 183: Generator Loss = 64.7876, Discriminator Loss = 4.9143\n",
      "Epoch 184: Generator Loss = 54.6737, Discriminator Loss = 3.3280\n",
      "Epoch 185: Generator Loss = 77.4291, Discriminator Loss = 5.6381\n",
      "Epoch 186: Generator Loss = 57.5521, Discriminator Loss = 4.3012\n",
      "Epoch 187: Generator Loss = 58.7569, Discriminator Loss = 2.3313\n",
      "Epoch 188: Generator Loss = 82.4534, Discriminator Loss = 1.0960\n",
      "Epoch 189: Generator Loss = 56.7658, Discriminator Loss = 2.8100\n",
      "Epoch 190: Generator Loss = 86.5745, Discriminator Loss = 1.1782\n",
      "Epoch 191: Generator Loss = 74.4817, Discriminator Loss = 0.9276\n",
      "Epoch 192: Generator Loss = 74.4208, Discriminator Loss = 0.9954\n",
      "Epoch 193: Generator Loss = 69.5645, Discriminator Loss = 1.2995\n",
      "Epoch 194: Generator Loss = 73.1981, Discriminator Loss = 0.7982\n",
      "Epoch 195: Generator Loss = 72.8869, Discriminator Loss = 0.9500\n",
      "Epoch 196: Generator Loss = 81.5544, Discriminator Loss = 1.7912\n",
      "Epoch 197: Generator Loss = 87.0537, Discriminator Loss = 1.1040\n",
      "Epoch 198: Generator Loss = 82.3844, Discriminator Loss = 0.9536\n",
      "Epoch 199: Generator Loss = 89.1735, Discriminator Loss = 0.7400\n",
      "Epoch 200: Generator Loss = 76.9159, Discriminator Loss = 0.4920\n",
      "Epoch 201: Generator Loss = 75.6192, Discriminator Loss = 1.8041\n",
      "Epoch 202: Generator Loss = 69.8508, Discriminator Loss = 0.4468\n",
      "Epoch 203: Generator Loss = 74.3570, Discriminator Loss = 1.0605\n",
      "Epoch 204: Generator Loss = 88.9036, Discriminator Loss = 0.4941\n",
      "Epoch 205: Generator Loss = 84.8674, Discriminator Loss = 0.7283\n",
      "Epoch 206: Generator Loss = 86.3306, Discriminator Loss = 0.9984\n",
      "Epoch 207: Generator Loss = 76.4891, Discriminator Loss = 1.0094\n",
      "Epoch 208: Generator Loss = 91.4301, Discriminator Loss = 0.4538\n",
      "Epoch 209: Generator Loss = 65.8368, Discriminator Loss = 1.1152\n",
      "Epoch 210: Generator Loss = 89.2763, Discriminator Loss = 0.8762\n",
      "Epoch 211: Generator Loss = 77.3374, Discriminator Loss = 0.8602\n",
      "Epoch 212: Generator Loss = 84.9871, Discriminator Loss = 1.9468\n",
      "Epoch 213: Generator Loss = 65.2883, Discriminator Loss = 3.1732\n",
      "Epoch 214: Generator Loss = 66.8422, Discriminator Loss = 1.8824\n",
      "Epoch 215: Generator Loss = 81.9783, Discriminator Loss = 0.7711\n",
      "Epoch 216: Generator Loss = 75.9326, Discriminator Loss = 4.9068\n",
      "Epoch 217: Generator Loss = 64.0482, Discriminator Loss = 3.0088\n",
      "Epoch 218: Generator Loss = 75.5121, Discriminator Loss = 1.3577\n",
      "Epoch 219: Generator Loss = 69.5094, Discriminator Loss = 1.5726\n",
      "Epoch 220: Generator Loss = 58.0510, Discriminator Loss = 2.0836\n",
      "Epoch 221: Generator Loss = 64.4212, Discriminator Loss = 0.8981\n",
      "Epoch 222: Generator Loss = 74.4206, Discriminator Loss = 1.6413\n",
      "Epoch 223: Generator Loss = 71.8376, Discriminator Loss = 0.6172\n",
      "Epoch 224: Generator Loss = 112.3378, Discriminator Loss = 0.3178\n",
      "Epoch 225: Generator Loss = 84.4034, Discriminator Loss = 0.9561\n",
      "Epoch 226: Generator Loss = 89.2503, Discriminator Loss = 0.3623\n",
      "Epoch 227: Generator Loss = 88.9267, Discriminator Loss = 3.1360\n",
      "Epoch 228: Generator Loss = 78.0765, Discriminator Loss = 1.2499\n",
      "Epoch 229: Generator Loss = 75.6501, Discriminator Loss = 1.7237\n",
      "Epoch 230: Generator Loss = 68.7172, Discriminator Loss = 0.8470\n",
      "Epoch 231: Generator Loss = 76.8138, Discriminator Loss = 1.3207\n",
      "Epoch 232: Generator Loss = 87.7574, Discriminator Loss = 1.0026\n",
      "Epoch 233: Generator Loss = 85.1460, Discriminator Loss = 1.2199\n",
      "Epoch 234: Generator Loss = 69.6808, Discriminator Loss = 0.5641\n",
      "Epoch 235: Generator Loss = 88.3829, Discriminator Loss = 0.4106\n",
      "Epoch 236: Generator Loss = 79.3878, Discriminator Loss = 0.7479\n",
      "Epoch 237: Generator Loss = 89.3625, Discriminator Loss = 0.6223\n",
      "Epoch 238: Generator Loss = 79.5020, Discriminator Loss = 0.9142\n",
      "Epoch 239: Generator Loss = 78.8544, Discriminator Loss = 1.2596\n",
      "Epoch 240: Generator Loss = 80.1734, Discriminator Loss = 0.4671\n",
      "Epoch 241: Generator Loss = 97.2912, Discriminator Loss = 0.4466\n",
      "Epoch 242: Generator Loss = 88.7405, Discriminator Loss = 0.4181\n",
      "Epoch 243: Generator Loss = 67.0817, Discriminator Loss = 1.8762\n",
      "Epoch 244: Generator Loss = 79.1363, Discriminator Loss = 1.4967\n",
      "Epoch 245: Generator Loss = 66.7226, Discriminator Loss = 1.1832\n",
      "Epoch 246: Generator Loss = 74.3007, Discriminator Loss = 1.5292\n",
      "Epoch 247: Generator Loss = 107.8813, Discriminator Loss = 2.2382\n",
      "Epoch 248: Generator Loss = 67.1432, Discriminator Loss = 2.4902\n",
      "Epoch 249: Generator Loss = 82.8526, Discriminator Loss = 3.0314\n",
      "Epoch 250: Generator Loss = 82.8581, Discriminator Loss = 7.2573\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.5892 Acc: 0.7565\n",
      "Epoch 2/15 | Train Loss: 0.4535 Acc: 0.7772\n",
      "Epoch 3/15 | Train Loss: 0.3456 Acc: 0.8187\n",
      "Epoch 4/15 | Train Loss: 0.3492 Acc: 0.8420\n",
      "Epoch 5/15 | Train Loss: 0.3763 Acc: 0.8135\n",
      "Epoch 6/15 | Train Loss: 0.3286 Acc: 0.8290\n",
      "Epoch 7/15 | Train Loss: 0.3280 Acc: 0.8394\n",
      "Epoch 8/15 | Train Loss: 0.2874 Acc: 0.8653\n",
      "Epoch 9/15 | Train Loss: 0.2615 Acc: 0.8834\n",
      "Epoch 10/15 | Train Loss: 0.2548 Acc: 0.8808\n",
      "Epoch 11/15 | Train Loss: 0.2898 Acc: 0.8705\n",
      "Epoch 12/15 | Train Loss: 0.2783 Acc: 0.8679\n",
      "Epoch 13/15 | Train Loss: 0.2803 Acc: 0.8653\n",
      "Epoch 14/15 | Train Loss: 0.2548 Acc: 0.8938\n",
      "Epoch 15/15 | Train Loss: 0.2437 Acc: 0.8886\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.6907 Acc: 0.7306\n",
      "Epoch 2/15 | Train Loss: 0.4789 Acc: 0.7617\n",
      "Epoch 3/15 | Train Loss: 0.4218 Acc: 0.7565\n",
      "Epoch 4/15 | Train Loss: 0.3991 Acc: 0.7772\n",
      "Epoch 5/15 | Train Loss: 0.3692 Acc: 0.8109\n",
      "Epoch 6/15 | Train Loss: 0.3782 Acc: 0.7876\n",
      "Epoch 7/15 | Train Loss: 0.3715 Acc: 0.7979\n",
      "Epoch 8/15 | Train Loss: 0.3427 Acc: 0.8083\n",
      "Epoch 9/15 | Train Loss: 0.3199 Acc: 0.8472\n",
      "Epoch 10/15 | Train Loss: 0.3052 Acc: 0.8368\n",
      "Epoch 11/15 | Train Loss: 0.3164 Acc: 0.8368\n",
      "Epoch 12/15 | Train Loss: 0.3054 Acc: 0.8601\n",
      "Epoch 13/15 | Train Loss: 0.2821 Acc: 0.8782\n",
      "Epoch 14/15 | Train Loss: 0.2829 Acc: 0.8756\n",
      "Epoch 15/15 | Train Loss: 0.2879 Acc: 0.8627\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.4737 Acc: 0.7617\n",
      "Epoch 2/15 | Train Loss: 0.3819 Acc: 0.8005\n",
      "Epoch 3/15 | Train Loss: 0.3864 Acc: 0.7953\n",
      "Epoch 4/15 | Train Loss: 0.3915 Acc: 0.8031\n",
      "Epoch 5/15 | Train Loss: 0.3481 Acc: 0.8187\n",
      "Epoch 6/15 | Train Loss: 0.3362 Acc: 0.8238\n",
      "Epoch 7/15 | Train Loss: 0.3248 Acc: 0.8109\n",
      "Epoch 8/15 | Train Loss: 0.3286 Acc: 0.8316\n",
      "Epoch 9/15 | Train Loss: 0.2819 Acc: 0.8523\n",
      "Epoch 10/15 | Train Loss: 0.2926 Acc: 0.8368\n",
      "Epoch 11/15 | Train Loss: 0.3067 Acc: 0.8316\n",
      "Epoch 12/15 | Train Loss: 0.2805 Acc: 0.8679\n",
      "Epoch 13/15 | Train Loss: 0.2926 Acc: 0.8731\n",
      "Epoch 14/15 | Train Loss: 0.2840 Acc: 0.8549\n",
      "Epoch 15/15 | Train Loss: 0.2974 Acc: 0.8627\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4430 Acc: 0.7968\n",
      "Epoch 2/15 | Train Loss: 0.3804 Acc: 0.8286\n",
      "Epoch 3/15 | Train Loss: 0.3784 Acc: 0.8444\n",
      "Epoch 4/15 | Train Loss: 0.3688 Acc: 0.8254\n",
      "Epoch 5/15 | Train Loss: 0.3710 Acc: 0.8444\n",
      "Epoch 6/15 | Train Loss: 0.3706 Acc: 0.8476\n",
      "Epoch 7/15 | Train Loss: 0.3643 Acc: 0.8127\n",
      "Epoch 8/15 | Train Loss: 0.3197 Acc: 0.8508\n",
      "Epoch 9/15 | Train Loss: 0.3204 Acc: 0.8603\n",
      "Epoch 10/15 | Train Loss: 0.2868 Acc: 0.8667\n",
      "Epoch 11/15 | Train Loss: 0.2871 Acc: 0.8698\n",
      "Epoch 12/15 | Train Loss: 0.3071 Acc: 0.8730\n",
      "Epoch 13/15 | Train Loss: 0.2600 Acc: 0.8794\n",
      "Epoch 14/15 | Train Loss: 0.2627 Acc: 0.8857\n",
      "Epoch 15/15 | Train Loss: 0.2930 Acc: 0.8698\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4922 Acc: 0.8269\n",
      "Epoch 2/15 | Train Loss: 0.3501 Acc: 0.8587\n",
      "Epoch 3/15 | Train Loss: 0.3436 Acc: 0.8657\n",
      "Epoch 4/15 | Train Loss: 0.3490 Acc: 0.8799\n",
      "Epoch 5/15 | Train Loss: 0.3261 Acc: 0.8587\n",
      "Epoch 6/15 | Train Loss: 0.3125 Acc: 0.8799\n",
      "Epoch 7/15 | Train Loss: 0.3101 Acc: 0.8657\n",
      "Epoch 8/15 | Train Loss: 0.2866 Acc: 0.8799\n",
      "Epoch 9/15 | Train Loss: 0.2824 Acc: 0.8834\n",
      "Epoch 10/15 | Train Loss: 0.2997 Acc: 0.8728\n",
      "Epoch 11/15 | Train Loss: 0.2406 Acc: 0.9011\n",
      "Epoch 12/15 | Train Loss: 0.2656 Acc: 0.8975\n",
      "Epoch 13/15 | Train Loss: 0.2461 Acc: 0.9187\n",
      "Epoch 14/15 | Train Loss: 0.2491 Acc: 0.9081\n",
      "Epoch 15/15 | Train Loss: 0.2423 Acc: 0.9046\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6808 Acc: 0.6260\n",
      "Epoch 2/15 | Train Loss: 0.6429 Acc: 0.7047\n",
      "Epoch 3/15 | Train Loss: 0.5587 Acc: 0.7087\n",
      "Epoch 4/15 | Train Loss: 0.5428 Acc: 0.7008\n",
      "Epoch 5/15 | Train Loss: 0.5408 Acc: 0.7205\n",
      "Epoch 6/15 | Train Loss: 0.4482 Acc: 0.7874\n",
      "Epoch 7/15 | Train Loss: 0.4261 Acc: 0.7913\n",
      "Epoch 8/15 | Train Loss: 0.3839 Acc: 0.8386\n",
      "Epoch 9/15 | Train Loss: 0.4030 Acc: 0.8189\n",
      "Epoch 10/15 | Train Loss: 0.3932 Acc: 0.8465\n",
      "Epoch 11/15 | Train Loss: 0.3473 Acc: 0.8543\n",
      "Epoch 12/15 | Train Loss: 0.3686 Acc: 0.8307\n",
      "Epoch 13/15 | Train Loss: 0.3680 Acc: 0.8346\n",
      "Epoch 14/15 | Train Loss: 0.4074 Acc: 0.8268\n",
      "Epoch 15/15 | Train Loss: 0.3464 Acc: 0.8465\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6992 Acc: 0.6220\n",
      "Epoch 2/15 | Train Loss: 0.6108 Acc: 0.6850\n",
      "Epoch 3/15 | Train Loss: 0.5347 Acc: 0.7165\n",
      "Epoch 4/15 | Train Loss: 0.5733 Acc: 0.7087\n",
      "Epoch 5/15 | Train Loss: 0.5025 Acc: 0.7244\n",
      "Epoch 6/15 | Train Loss: 0.5020 Acc: 0.7480\n",
      "Epoch 7/15 | Train Loss: 0.4700 Acc: 0.7677\n",
      "Epoch 8/15 | Train Loss: 0.4690 Acc: 0.7638\n",
      "Epoch 9/15 | Train Loss: 0.4098 Acc: 0.8189\n",
      "Epoch 10/15 | Train Loss: 0.4762 Acc: 0.7756\n",
      "Epoch 11/15 | Train Loss: 0.4015 Acc: 0.8110\n",
      "Epoch 12/15 | Train Loss: 0.3795 Acc: 0.8110\n",
      "Epoch 13/15 | Train Loss: 0.4277 Acc: 0.7598\n",
      "Epoch 14/15 | Train Loss: 0.4458 Acc: 0.7795\n",
      "Epoch 15/15 | Train Loss: 0.3814 Acc: 0.8228\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6901 Acc: 0.6417\n",
      "Epoch 2/15 | Train Loss: 0.6438 Acc: 0.6378\n",
      "Epoch 3/15 | Train Loss: 0.5575 Acc: 0.6969\n",
      "Epoch 4/15 | Train Loss: 0.5327 Acc: 0.7323\n",
      "Epoch 5/15 | Train Loss: 0.5156 Acc: 0.7441\n",
      "Epoch 6/15 | Train Loss: 0.5015 Acc: 0.7362\n",
      "Epoch 7/15 | Train Loss: 0.4875 Acc: 0.7874\n",
      "Epoch 8/15 | Train Loss: 0.4319 Acc: 0.8031\n",
      "Epoch 9/15 | Train Loss: 0.4493 Acc: 0.7953\n",
      "Epoch 10/15 | Train Loss: 0.4316 Acc: 0.7953\n",
      "Epoch 11/15 | Train Loss: 0.4287 Acc: 0.7953\n",
      "Epoch 12/15 | Train Loss: 0.4120 Acc: 0.7992\n",
      "Epoch 13/15 | Train Loss: 0.4123 Acc: 0.8228\n",
      "Epoch 14/15 | Train Loss: 0.4284 Acc: 0.7953\n",
      "Epoch 15/15 | Train Loss: 0.4129 Acc: 0.7717\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5957 Acc: 0.7257\n",
      "Epoch 2/15 | Train Loss: 0.5408 Acc: 0.7722\n",
      "Epoch 3/15 | Train Loss: 0.4717 Acc: 0.8017\n",
      "Epoch 4/15 | Train Loss: 0.4971 Acc: 0.7468\n",
      "Epoch 5/15 | Train Loss: 0.4589 Acc: 0.7975\n",
      "Epoch 6/15 | Train Loss: 0.4411 Acc: 0.8186\n",
      "Epoch 7/15 | Train Loss: 0.4321 Acc: 0.8228\n",
      "Epoch 8/15 | Train Loss: 0.3676 Acc: 0.8354\n",
      "Epoch 9/15 | Train Loss: 0.3897 Acc: 0.8101\n",
      "Epoch 10/15 | Train Loss: 0.4162 Acc: 0.8059\n",
      "Epoch 11/15 | Train Loss: 0.3733 Acc: 0.8186\n",
      "Epoch 12/15 | Train Loss: 0.3697 Acc: 0.8608\n",
      "Epoch 13/15 | Train Loss: 0.3841 Acc: 0.8228\n",
      "Epoch 14/15 | Train Loss: 0.3148 Acc: 0.8565\n",
      "Epoch 15/15 | Train Loss: 0.4009 Acc: 0.8354\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6554 Acc: 0.6332\n",
      "Epoch 2/15 | Train Loss: 0.4877 Acc: 0.7991\n",
      "Epoch 3/15 | Train Loss: 0.4598 Acc: 0.8079\n",
      "Epoch 4/15 | Train Loss: 0.4264 Acc: 0.8122\n",
      "Epoch 5/15 | Train Loss: 0.3947 Acc: 0.8384\n",
      "Epoch 6/15 | Train Loss: 0.2839 Acc: 0.8908\n",
      "Epoch 7/15 | Train Loss: 0.4350 Acc: 0.8297\n",
      "Epoch 8/15 | Train Loss: 0.3591 Acc: 0.8603\n",
      "Epoch 9/15 | Train Loss: 0.3502 Acc: 0.8603\n",
      "Epoch 10/15 | Train Loss: 0.3180 Acc: 0.8690\n",
      "Epoch 11/15 | Train Loss: 0.3188 Acc: 0.8777\n",
      "Epoch 12/15 | Train Loss: 0.3374 Acc: 0.8646\n",
      "Epoch 13/15 | Train Loss: 0.3217 Acc: 0.8777\n",
      "Epoch 14/15 | Train Loss: 0.3086 Acc: 0.8734\n",
      "Epoch 15/15 | Train Loss: 0.2676 Acc: 0.8821\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6238 Acc: 0.6946\n",
      "Epoch 2/15 | Train Loss: 0.5066 Acc: 0.7282\n",
      "Epoch 3/15 | Train Loss: 0.4256 Acc: 0.7685\n",
      "Epoch 4/15 | Train Loss: 0.4538 Acc: 0.7718\n",
      "Epoch 5/15 | Train Loss: 0.4281 Acc: 0.7685\n",
      "Epoch 6/15 | Train Loss: 0.4018 Acc: 0.8121\n",
      "Epoch 7/15 | Train Loss: 0.3783 Acc: 0.8322\n",
      "Epoch 8/15 | Train Loss: 0.4256 Acc: 0.8087\n",
      "Epoch 9/15 | Train Loss: 0.3761 Acc: 0.8121\n",
      "Epoch 10/15 | Train Loss: 0.3420 Acc: 0.8557\n",
      "Epoch 11/15 | Train Loss: 0.3514 Acc: 0.8389\n",
      "Epoch 12/15 | Train Loss: 0.2969 Acc: 0.8523\n",
      "Epoch 13/15 | Train Loss: 0.3345 Acc: 0.8322\n",
      "Epoch 14/15 | Train Loss: 0.3659 Acc: 0.8188\n",
      "Epoch 15/15 | Train Loss: 0.3283 Acc: 0.8624\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6097 Acc: 0.6879\n",
      "Epoch 2/15 | Train Loss: 0.5500 Acc: 0.7181\n",
      "Epoch 3/15 | Train Loss: 0.5177 Acc: 0.7483\n",
      "Epoch 4/15 | Train Loss: 0.4485 Acc: 0.7550\n",
      "Epoch 5/15 | Train Loss: 0.4688 Acc: 0.7550\n",
      "Epoch 6/15 | Train Loss: 0.4543 Acc: 0.7718\n",
      "Epoch 7/15 | Train Loss: 0.3887 Acc: 0.7953\n",
      "Epoch 8/15 | Train Loss: 0.4099 Acc: 0.7987\n",
      "Epoch 9/15 | Train Loss: 0.3794 Acc: 0.8054\n",
      "Epoch 10/15 | Train Loss: 0.3362 Acc: 0.8456\n",
      "Epoch 11/15 | Train Loss: 0.3593 Acc: 0.8356\n",
      "Epoch 12/15 | Train Loss: 0.3683 Acc: 0.8221\n",
      "Epoch 13/15 | Train Loss: 0.3606 Acc: 0.8356\n",
      "Epoch 14/15 | Train Loss: 0.3297 Acc: 0.8591\n",
      "Epoch 15/15 | Train Loss: 0.3532 Acc: 0.8322\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5992 Acc: 0.7148\n",
      "Epoch 2/15 | Train Loss: 0.5541 Acc: 0.7450\n",
      "Epoch 3/15 | Train Loss: 0.5352 Acc: 0.7081\n",
      "Epoch 4/15 | Train Loss: 0.4617 Acc: 0.7215\n",
      "Epoch 5/15 | Train Loss: 0.4693 Acc: 0.7752\n",
      "Epoch 6/15 | Train Loss: 0.4328 Acc: 0.7953\n",
      "Epoch 7/15 | Train Loss: 0.4235 Acc: 0.7819\n",
      "Epoch 8/15 | Train Loss: 0.3654 Acc: 0.8322\n",
      "Epoch 9/15 | Train Loss: 0.4004 Acc: 0.7987\n",
      "Epoch 10/15 | Train Loss: 0.3620 Acc: 0.8322\n",
      "Epoch 11/15 | Train Loss: 0.3583 Acc: 0.8188\n",
      "Epoch 12/15 | Train Loss: 0.3634 Acc: 0.8255\n",
      "Epoch 13/15 | Train Loss: 0.3850 Acc: 0.7953\n",
      "Epoch 14/15 | Train Loss: 0.3109 Acc: 0.8725\n",
      "Epoch 15/15 | Train Loss: 0.3914 Acc: 0.7886\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6436 Acc: 0.6882\n",
      "Epoch 2/15 | Train Loss: 0.4874 Acc: 0.7605\n",
      "Epoch 3/15 | Train Loss: 0.4634 Acc: 0.7985\n",
      "Epoch 4/15 | Train Loss: 0.4304 Acc: 0.8023\n",
      "Epoch 5/15 | Train Loss: 0.3892 Acc: 0.8137\n",
      "Epoch 6/15 | Train Loss: 0.3967 Acc: 0.8365\n",
      "Epoch 7/15 | Train Loss: 0.3982 Acc: 0.8137\n",
      "Epoch 8/15 | Train Loss: 0.3512 Acc: 0.8251\n",
      "Epoch 9/15 | Train Loss: 0.3860 Acc: 0.8213\n",
      "Epoch 10/15 | Train Loss: 0.3609 Acc: 0.8403\n",
      "Epoch 11/15 | Train Loss: 0.3623 Acc: 0.8289\n",
      "Epoch 12/15 | Train Loss: 0.3438 Acc: 0.8631\n",
      "Epoch 13/15 | Train Loss: 0.3442 Acc: 0.8517\n",
      "Epoch 14/15 | Train Loss: 0.3188 Acc: 0.8821\n",
      "Epoch 15/15 | Train Loss: 0.3267 Acc: 0.8403\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.4892 Acc: 0.7935\n",
      "Epoch 2/15 | Train Loss: 0.4585 Acc: 0.8340\n",
      "Epoch 3/15 | Train Loss: 0.3935 Acc: 0.8259\n",
      "Epoch 4/15 | Train Loss: 0.3872 Acc: 0.8623\n",
      "Epoch 5/15 | Train Loss: 0.3535 Acc: 0.8623\n",
      "Epoch 6/15 | Train Loss: 0.3767 Acc: 0.8340\n",
      "Epoch 7/15 | Train Loss: 0.2993 Acc: 0.8866\n",
      "Epoch 8/15 | Train Loss: 0.3530 Acc: 0.8462\n",
      "Epoch 9/15 | Train Loss: 0.2883 Acc: 0.8907\n",
      "Epoch 10/15 | Train Loss: 0.2793 Acc: 0.8745\n",
      "Epoch 11/15 | Train Loss: 0.2485 Acc: 0.8866\n",
      "Epoch 12/15 | Train Loss: 0.2862 Acc: 0.8745\n",
      "Epoch 13/15 | Train Loss: 0.2843 Acc: 0.8704\n",
      "Epoch 14/15 | Train Loss: 0.2549 Acc: 0.9069\n",
      "Epoch 15/15 | Train Loss: 0.2961 Acc: 0.8826\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6280 Acc: 0.6535\n",
      "Epoch 2/15 | Train Loss: 0.6146 Acc: 0.6772\n",
      "Epoch 3/15 | Train Loss: 0.5647 Acc: 0.7165\n",
      "Epoch 4/15 | Train Loss: 0.4908 Acc: 0.7520\n",
      "Epoch 5/15 | Train Loss: 0.4923 Acc: 0.7362\n",
      "Epoch 6/15 | Train Loss: 0.4169 Acc: 0.7953\n",
      "Epoch 7/15 | Train Loss: 0.4846 Acc: 0.7598\n",
      "Epoch 8/15 | Train Loss: 0.4459 Acc: 0.7913\n",
      "Epoch 9/15 | Train Loss: 0.4381 Acc: 0.7953\n",
      "Epoch 10/15 | Train Loss: 0.3868 Acc: 0.8189\n",
      "Epoch 11/15 | Train Loss: 0.3183 Acc: 0.8819\n",
      "Epoch 12/15 | Train Loss: 0.3299 Acc: 0.8661\n",
      "Epoch 13/15 | Train Loss: 0.3188 Acc: 0.8504\n",
      "Epoch 14/15 | Train Loss: 0.2741 Acc: 0.8976\n",
      "Epoch 15/15 | Train Loss: 0.3256 Acc: 0.8661\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6922 Acc: 0.6614\n",
      "Epoch 2/15 | Train Loss: 0.5802 Acc: 0.6969\n",
      "Epoch 3/15 | Train Loss: 0.5972 Acc: 0.6772\n",
      "Epoch 4/15 | Train Loss: 0.5275 Acc: 0.6969\n",
      "Epoch 5/15 | Train Loss: 0.5458 Acc: 0.7087\n",
      "Epoch 6/15 | Train Loss: 0.5127 Acc: 0.7087\n",
      "Epoch 7/15 | Train Loss: 0.5440 Acc: 0.6850\n",
      "Epoch 8/15 | Train Loss: 0.4882 Acc: 0.7520\n",
      "Epoch 9/15 | Train Loss: 0.3973 Acc: 0.7953\n",
      "Epoch 10/15 | Train Loss: 0.4086 Acc: 0.8228\n",
      "Epoch 11/15 | Train Loss: 0.4284 Acc: 0.7913\n",
      "Epoch 12/15 | Train Loss: 0.4617 Acc: 0.7520\n",
      "Epoch 13/15 | Train Loss: 0.4012 Acc: 0.8228\n",
      "Epoch 14/15 | Train Loss: 0.3948 Acc: 0.8307\n",
      "Epoch 15/15 | Train Loss: 0.4333 Acc: 0.7638\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7269 Acc: 0.5945\n",
      "Epoch 2/15 | Train Loss: 0.6049 Acc: 0.6929\n",
      "Epoch 3/15 | Train Loss: 0.5420 Acc: 0.7008\n",
      "Epoch 4/15 | Train Loss: 0.5333 Acc: 0.6929\n",
      "Epoch 5/15 | Train Loss: 0.5266 Acc: 0.7283\n",
      "Epoch 6/15 | Train Loss: 0.5507 Acc: 0.7402\n",
      "Epoch 7/15 | Train Loss: 0.5086 Acc: 0.7047\n",
      "Epoch 8/15 | Train Loss: 0.5175 Acc: 0.7520\n",
      "Epoch 9/15 | Train Loss: 0.4283 Acc: 0.7992\n",
      "Epoch 10/15 | Train Loss: 0.4512 Acc: 0.7795\n",
      "Epoch 11/15 | Train Loss: 0.4029 Acc: 0.8071\n",
      "Epoch 12/15 | Train Loss: 0.4214 Acc: 0.8110\n",
      "Epoch 13/15 | Train Loss: 0.4436 Acc: 0.7677\n",
      "Epoch 14/15 | Train Loss: 0.3641 Acc: 0.8543\n",
      "Epoch 15/15 | Train Loss: 0.4513 Acc: 0.7559\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6350 Acc: 0.7342\n",
      "Epoch 2/15 | Train Loss: 0.5258 Acc: 0.7806\n",
      "Epoch 3/15 | Train Loss: 0.4442 Acc: 0.7722\n",
      "Epoch 4/15 | Train Loss: 0.5334 Acc: 0.7468\n",
      "Epoch 5/15 | Train Loss: 0.4490 Acc: 0.7890\n",
      "Epoch 6/15 | Train Loss: 0.4867 Acc: 0.7806\n",
      "Epoch 7/15 | Train Loss: 0.4284 Acc: 0.7975\n",
      "Epoch 8/15 | Train Loss: 0.4548 Acc: 0.7764\n",
      "Epoch 9/15 | Train Loss: 0.4213 Acc: 0.8017\n",
      "Epoch 10/15 | Train Loss: 0.4256 Acc: 0.8228\n",
      "Epoch 11/15 | Train Loss: 0.3882 Acc: 0.8397\n",
      "Epoch 12/15 | Train Loss: 0.3759 Acc: 0.8354\n",
      "Epoch 13/15 | Train Loss: 0.3884 Acc: 0.8312\n",
      "Epoch 14/15 | Train Loss: 0.3592 Acc: 0.8397\n",
      "Epoch 15/15 | Train Loss: 0.3800 Acc: 0.8270\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5029 Acc: 0.7511\n",
      "Epoch 2/15 | Train Loss: 0.4330 Acc: 0.8253\n",
      "Epoch 3/15 | Train Loss: 0.3771 Acc: 0.8559\n",
      "Epoch 4/15 | Train Loss: 0.4589 Acc: 0.8297\n",
      "Epoch 5/15 | Train Loss: 0.4228 Acc: 0.8297\n",
      "Epoch 6/15 | Train Loss: 0.3730 Acc: 0.8384\n",
      "Epoch 7/15 | Train Loss: 0.3313 Acc: 0.8734\n",
      "Epoch 8/15 | Train Loss: 0.3727 Acc: 0.8079\n",
      "Epoch 9/15 | Train Loss: 0.3127 Acc: 0.8734\n",
      "Epoch 10/15 | Train Loss: 0.3278 Acc: 0.8690\n",
      "Epoch 11/15 | Train Loss: 0.2972 Acc: 0.8996\n",
      "Epoch 12/15 | Train Loss: 0.2756 Acc: 0.8821\n",
      "Epoch 13/15 | Train Loss: 0.2319 Acc: 0.8996\n",
      "Epoch 14/15 | Train Loss: 0.2861 Acc: 0.8821\n",
      "Epoch 15/15 | Train Loss: 0.2673 Acc: 0.8865\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5363 Acc: 0.7461\n",
      "Epoch 2/15 | Train Loss: 0.3956 Acc: 0.7798\n",
      "Epoch 3/15 | Train Loss: 0.4099 Acc: 0.7617\n",
      "Epoch 4/15 | Train Loss: 0.3677 Acc: 0.7979\n",
      "Epoch 5/15 | Train Loss: 0.3643 Acc: 0.8083\n",
      "Epoch 6/15 | Train Loss: 0.3321 Acc: 0.8290\n",
      "Epoch 7/15 | Train Loss: 0.3169 Acc: 0.8523\n",
      "Epoch 8/15 | Train Loss: 0.2737 Acc: 0.8731\n",
      "Epoch 9/15 | Train Loss: 0.2957 Acc: 0.8472\n",
      "Epoch 10/15 | Train Loss: 0.2525 Acc: 0.8834\n",
      "Epoch 11/15 | Train Loss: 0.2443 Acc: 0.8808\n",
      "Epoch 12/15 | Train Loss: 0.2424 Acc: 0.8756\n",
      "Epoch 13/15 | Train Loss: 0.2644 Acc: 0.8705\n",
      "Epoch 14/15 | Train Loss: 0.2767 Acc: 0.8549\n",
      "Epoch 15/15 | Train Loss: 0.2370 Acc: 0.8886\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.6399 Acc: 0.7280\n",
      "Epoch 2/15 | Train Loss: 0.3976 Acc: 0.7824\n",
      "Epoch 3/15 | Train Loss: 0.4139 Acc: 0.7642\n",
      "Epoch 4/15 | Train Loss: 0.3736 Acc: 0.7953\n",
      "Epoch 5/15 | Train Loss: 0.3505 Acc: 0.8135\n",
      "Epoch 6/15 | Train Loss: 0.3613 Acc: 0.7850\n",
      "Epoch 7/15 | Train Loss: 0.3606 Acc: 0.7902\n",
      "Epoch 8/15 | Train Loss: 0.3190 Acc: 0.8264\n",
      "Epoch 9/15 | Train Loss: 0.3226 Acc: 0.8264\n",
      "Epoch 10/15 | Train Loss: 0.3058 Acc: 0.8627\n",
      "Epoch 11/15 | Train Loss: 0.3207 Acc: 0.8264\n",
      "Epoch 12/15 | Train Loss: 0.2976 Acc: 0.8575\n",
      "Epoch 13/15 | Train Loss: 0.2727 Acc: 0.8912\n",
      "Epoch 14/15 | Train Loss: 0.2846 Acc: 0.8731\n",
      "Epoch 15/15 | Train Loss: 0.2573 Acc: 0.8912\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5314 Acc: 0.7694\n",
      "Epoch 2/15 | Train Loss: 0.4101 Acc: 0.7902\n",
      "Epoch 3/15 | Train Loss: 0.3882 Acc: 0.8083\n",
      "Epoch 4/15 | Train Loss: 0.3750 Acc: 0.7927\n",
      "Epoch 5/15 | Train Loss: 0.3761 Acc: 0.7953\n",
      "Epoch 6/15 | Train Loss: 0.3779 Acc: 0.8135\n",
      "Epoch 7/15 | Train Loss: 0.3274 Acc: 0.8368\n",
      "Epoch 8/15 | Train Loss: 0.2970 Acc: 0.8575\n",
      "Epoch 9/15 | Train Loss: 0.3287 Acc: 0.8238\n",
      "Epoch 10/15 | Train Loss: 0.3163 Acc: 0.8446\n",
      "Epoch 11/15 | Train Loss: 0.2765 Acc: 0.8756\n",
      "Epoch 12/15 | Train Loss: 0.2920 Acc: 0.8549\n",
      "Epoch 13/15 | Train Loss: 0.2846 Acc: 0.8679\n",
      "Epoch 14/15 | Train Loss: 0.3141 Acc: 0.8290\n",
      "Epoch 15/15 | Train Loss: 0.2849 Acc: 0.8446\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5267 Acc: 0.7746\n",
      "Epoch 2/15 | Train Loss: 0.5320 Acc: 0.7746\n",
      "Epoch 3/15 | Train Loss: 0.4020 Acc: 0.8159\n",
      "Epoch 4/15 | Train Loss: 0.3602 Acc: 0.8413\n",
      "Epoch 5/15 | Train Loss: 0.3804 Acc: 0.8381\n",
      "Epoch 6/15 | Train Loss: 0.3368 Acc: 0.8381\n",
      "Epoch 7/15 | Train Loss: 0.3484 Acc: 0.8476\n",
      "Epoch 8/15 | Train Loss: 0.3093 Acc: 0.8540\n",
      "Epoch 9/15 | Train Loss: 0.2832 Acc: 0.8889\n",
      "Epoch 10/15 | Train Loss: 0.3133 Acc: 0.8667\n",
      "Epoch 11/15 | Train Loss: 0.2751 Acc: 0.8762\n",
      "Epoch 12/15 | Train Loss: 0.2702 Acc: 0.8889\n",
      "Epoch 13/15 | Train Loss: 0.2826 Acc: 0.8635\n",
      "Epoch 14/15 | Train Loss: 0.2967 Acc: 0.8635\n",
      "Epoch 15/15 | Train Loss: 0.2713 Acc: 0.8857\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4741 Acc: 0.8269\n",
      "Epoch 2/15 | Train Loss: 0.4064 Acc: 0.8693\n",
      "Epoch 3/15 | Train Loss: 0.3519 Acc: 0.8410\n",
      "Epoch 4/15 | Train Loss: 0.3489 Acc: 0.8657\n",
      "Epoch 5/15 | Train Loss: 0.3178 Acc: 0.8834\n",
      "Epoch 6/15 | Train Loss: 0.3234 Acc: 0.8799\n",
      "Epoch 7/15 | Train Loss: 0.2695 Acc: 0.8975\n",
      "Epoch 8/15 | Train Loss: 0.2669 Acc: 0.8869\n",
      "Epoch 9/15 | Train Loss: 0.2674 Acc: 0.8763\n",
      "Epoch 10/15 | Train Loss: 0.2476 Acc: 0.9187\n",
      "Epoch 11/15 | Train Loss: 0.2594 Acc: 0.8975\n",
      "Epoch 12/15 | Train Loss: 0.2261 Acc: 0.9046\n",
      "Epoch 13/15 | Train Loss: 0.2034 Acc: 0.9258\n",
      "Epoch 14/15 | Train Loss: 0.2273 Acc: 0.9046\n",
      "Epoch 15/15 | Train Loss: 0.2207 Acc: 0.9187\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.7402 Acc: 0.5945\n",
      "Epoch 2/15 | Train Loss: 0.5845 Acc: 0.6969\n",
      "Epoch 3/15 | Train Loss: 0.5808 Acc: 0.7087\n",
      "Epoch 4/15 | Train Loss: 0.5547 Acc: 0.7008\n",
      "Epoch 5/15 | Train Loss: 0.4756 Acc: 0.7795\n",
      "Epoch 6/15 | Train Loss: 0.4619 Acc: 0.7874\n",
      "Epoch 7/15 | Train Loss: 0.4097 Acc: 0.8031\n",
      "Epoch 8/15 | Train Loss: 0.4011 Acc: 0.8189\n",
      "Epoch 9/15 | Train Loss: 0.4214 Acc: 0.8228\n",
      "Epoch 10/15 | Train Loss: 0.3645 Acc: 0.8425\n",
      "Epoch 11/15 | Train Loss: 0.4036 Acc: 0.7992\n",
      "Epoch 12/15 | Train Loss: 0.3730 Acc: 0.7756\n",
      "Epoch 13/15 | Train Loss: 0.3555 Acc: 0.8425\n",
      "Epoch 14/15 | Train Loss: 0.3362 Acc: 0.8189\n",
      "Epoch 15/15 | Train Loss: 0.3523 Acc: 0.8504\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6948 Acc: 0.5945\n",
      "Epoch 2/15 | Train Loss: 0.5703 Acc: 0.6732\n",
      "Epoch 3/15 | Train Loss: 0.5321 Acc: 0.7283\n",
      "Epoch 4/15 | Train Loss: 0.5466 Acc: 0.7205\n",
      "Epoch 5/15 | Train Loss: 0.4936 Acc: 0.7480\n",
      "Epoch 6/15 | Train Loss: 0.6211 Acc: 0.7165\n",
      "Epoch 7/15 | Train Loss: 0.4660 Acc: 0.7638\n",
      "Epoch 8/15 | Train Loss: 0.4322 Acc: 0.8071\n",
      "Epoch 9/15 | Train Loss: 0.4716 Acc: 0.7559\n",
      "Epoch 10/15 | Train Loss: 0.4011 Acc: 0.8228\n",
      "Epoch 11/15 | Train Loss: 0.4465 Acc: 0.7756\n",
      "Epoch 12/15 | Train Loss: 0.4155 Acc: 0.8071\n",
      "Epoch 13/15 | Train Loss: 0.4263 Acc: 0.7953\n",
      "Epoch 14/15 | Train Loss: 0.4131 Acc: 0.8031\n",
      "Epoch 15/15 | Train Loss: 0.3966 Acc: 0.7874\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7055 Acc: 0.6378\n",
      "Epoch 2/15 | Train Loss: 0.5866 Acc: 0.7205\n",
      "Epoch 3/15 | Train Loss: 0.5643 Acc: 0.7244\n",
      "Epoch 4/15 | Train Loss: 0.5264 Acc: 0.7165\n",
      "Epoch 5/15 | Train Loss: 0.5342 Acc: 0.7008\n",
      "Epoch 6/15 | Train Loss: 0.4438 Acc: 0.7677\n",
      "Epoch 7/15 | Train Loss: 0.4858 Acc: 0.7559\n",
      "Epoch 8/15 | Train Loss: 0.4722 Acc: 0.7717\n",
      "Epoch 9/15 | Train Loss: 0.4389 Acc: 0.7795\n",
      "Epoch 10/15 | Train Loss: 0.4376 Acc: 0.8031\n",
      "Epoch 11/15 | Train Loss: 0.3971 Acc: 0.8071\n",
      "Epoch 12/15 | Train Loss: 0.4581 Acc: 0.7835\n",
      "Epoch 13/15 | Train Loss: 0.4356 Acc: 0.7913\n",
      "Epoch 14/15 | Train Loss: 0.3846 Acc: 0.7913\n",
      "Epoch 15/15 | Train Loss: 0.4505 Acc: 0.7402\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6913 Acc: 0.6287\n",
      "Epoch 2/15 | Train Loss: 0.5115 Acc: 0.7468\n",
      "Epoch 3/15 | Train Loss: 0.5356 Acc: 0.7806\n",
      "Epoch 4/15 | Train Loss: 0.4495 Acc: 0.7890\n",
      "Epoch 5/15 | Train Loss: 0.4612 Acc: 0.7932\n",
      "Epoch 6/15 | Train Loss: 0.5001 Acc: 0.7637\n",
      "Epoch 7/15 | Train Loss: 0.4376 Acc: 0.8017\n",
      "Epoch 8/15 | Train Loss: 0.4363 Acc: 0.7848\n",
      "Epoch 9/15 | Train Loss: 0.4550 Acc: 0.7975\n",
      "Epoch 10/15 | Train Loss: 0.3901 Acc: 0.8439\n",
      "Epoch 11/15 | Train Loss: 0.4004 Acc: 0.8228\n",
      "Epoch 12/15 | Train Loss: 0.3891 Acc: 0.8228\n",
      "Epoch 13/15 | Train Loss: 0.3721 Acc: 0.8397\n",
      "Epoch 14/15 | Train Loss: 0.3844 Acc: 0.8481\n",
      "Epoch 15/15 | Train Loss: 0.4047 Acc: 0.8059\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5301 Acc: 0.7380\n",
      "Epoch 2/15 | Train Loss: 0.4602 Acc: 0.8253\n",
      "Epoch 3/15 | Train Loss: 0.3587 Acc: 0.8777\n",
      "Epoch 4/15 | Train Loss: 0.4236 Acc: 0.8253\n",
      "Epoch 5/15 | Train Loss: 0.3668 Acc: 0.8341\n",
      "Epoch 6/15 | Train Loss: 0.3514 Acc: 0.8559\n",
      "Epoch 7/15 | Train Loss: 0.3367 Acc: 0.8646\n",
      "Epoch 8/15 | Train Loss: 0.3525 Acc: 0.8428\n",
      "Epoch 9/15 | Train Loss: 0.3033 Acc: 0.8821\n",
      "Epoch 10/15 | Train Loss: 0.2584 Acc: 0.8865\n",
      "Epoch 11/15 | Train Loss: 0.3155 Acc: 0.8777\n",
      "Epoch 12/15 | Train Loss: 0.3123 Acc: 0.8777\n",
      "Epoch 13/15 | Train Loss: 0.2829 Acc: 0.8952\n",
      "Epoch 14/15 | Train Loss: 0.2979 Acc: 0.8865\n",
      "Epoch 15/15 | Train Loss: 0.2472 Acc: 0.9083\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5080 Acc: 0.7608\n",
      "Epoch 2/15 | Train Loss: 0.4143 Acc: 0.7882\n",
      "Epoch 3/15 | Train Loss: 0.3603 Acc: 0.8087\n",
      "Epoch 4/15 | Train Loss: 0.3572 Acc: 0.8292\n",
      "Epoch 5/15 | Train Loss: 0.3109 Acc: 0.8383\n",
      "Epoch 6/15 | Train Loss: 0.3701 Acc: 0.7950\n",
      "Epoch 7/15 | Train Loss: 0.3363 Acc: 0.8155\n",
      "Epoch 8/15 | Train Loss: 0.3094 Acc: 0.8405\n",
      "Epoch 9/15 | Train Loss: 0.3279 Acc: 0.8360\n",
      "Epoch 10/15 | Train Loss: 0.2716 Acc: 0.8747\n",
      "Epoch 11/15 | Train Loss: 0.2803 Acc: 0.8702\n",
      "Epoch 12/15 | Train Loss: 0.2758 Acc: 0.8679\n",
      "Epoch 13/15 | Train Loss: 0.2612 Acc: 0.8815\n",
      "Epoch 14/15 | Train Loss: 0.2542 Acc: 0.8907\n",
      "Epoch 15/15 | Train Loss: 0.2964 Acc: 0.8610\n",
      "Fold 9 Test Accuracy: 0.7040\n",
      "===== Fold 10 =====\n",
      "Epoch 1: Generator Loss = 9.7758, Discriminator Loss = 8.7374\n",
      "Epoch 2: Generator Loss = 10.3810, Discriminator Loss = 8.8566\n",
      "Epoch 3: Generator Loss = 13.3836, Discriminator Loss = 7.2058\n",
      "Epoch 4: Generator Loss = 18.8253, Discriminator Loss = 5.0699\n",
      "Epoch 5: Generator Loss = 24.4430, Discriminator Loss = 6.4292\n",
      "Epoch 6: Generator Loss = 25.8259, Discriminator Loss = 5.2969\n",
      "Epoch 7: Generator Loss = 21.5308, Discriminator Loss = 5.2595\n",
      "Epoch 8: Generator Loss = 26.0146, Discriminator Loss = 5.4870\n",
      "Epoch 9: Generator Loss = 21.6192, Discriminator Loss = 10.0731\n",
      "Epoch 10: Generator Loss = 17.7018, Discriminator Loss = 9.2805\n",
      "Epoch 11: Generator Loss = 15.6532, Discriminator Loss = 7.7578\n",
      "Epoch 12: Generator Loss = 17.8102, Discriminator Loss = 7.8675\n",
      "Epoch 13: Generator Loss = 18.9025, Discriminator Loss = 7.5409\n",
      "Epoch 14: Generator Loss = 21.1303, Discriminator Loss = 6.8349\n",
      "Epoch 15: Generator Loss = 22.4262, Discriminator Loss = 6.6481\n",
      "Epoch 16: Generator Loss = 22.1953, Discriminator Loss = 8.1486\n",
      "Epoch 17: Generator Loss = 22.1659, Discriminator Loss = 7.2861\n",
      "Epoch 18: Generator Loss = 18.1592, Discriminator Loss = 8.9680\n",
      "Epoch 19: Generator Loss = 16.7173, Discriminator Loss = 7.6458\n",
      "Epoch 20: Generator Loss = 13.0740, Discriminator Loss = 8.3544\n",
      "Epoch 21: Generator Loss = 17.1061, Discriminator Loss = 10.3827\n",
      "Epoch 22: Generator Loss = 14.6167, Discriminator Loss = 9.0205\n",
      "Epoch 23: Generator Loss = 16.1951, Discriminator Loss = 8.7779\n",
      "Epoch 24: Generator Loss = 16.2797, Discriminator Loss = 7.2472\n",
      "Epoch 25: Generator Loss = 17.8212, Discriminator Loss = 10.2458\n",
      "Epoch 26: Generator Loss = 17.1970, Discriminator Loss = 7.8887\n",
      "Epoch 27: Generator Loss = 16.4431, Discriminator Loss = 7.5054\n",
      "Epoch 28: Generator Loss = 25.3017, Discriminator Loss = 6.9173\n",
      "Epoch 29: Generator Loss = 22.2116, Discriminator Loss = 8.1695\n",
      "Epoch 30: Generator Loss = 18.7264, Discriminator Loss = 8.1602\n",
      "Epoch 31: Generator Loss = 21.2335, Discriminator Loss = 7.5146\n",
      "Epoch 32: Generator Loss = 21.1773, Discriminator Loss = 6.1936\n",
      "Epoch 33: Generator Loss = 22.9428, Discriminator Loss = 8.4162\n",
      "Epoch 34: Generator Loss = 20.4307, Discriminator Loss = 8.1198\n",
      "Epoch 35: Generator Loss = 20.0453, Discriminator Loss = 7.9581\n",
      "Epoch 36: Generator Loss = 17.1997, Discriminator Loss = 8.2620\n",
      "Epoch 37: Generator Loss = 15.6286, Discriminator Loss = 9.9121\n",
      "Epoch 38: Generator Loss = 16.6719, Discriminator Loss = 8.0038\n",
      "Epoch 39: Generator Loss = 14.5026, Discriminator Loss = 8.3927\n",
      "Epoch 40: Generator Loss = 19.5693, Discriminator Loss = 6.8426\n",
      "Epoch 41: Generator Loss = 18.1014, Discriminator Loss = 7.3917\n",
      "Epoch 42: Generator Loss = 18.3605, Discriminator Loss = 8.4774\n",
      "Epoch 43: Generator Loss = 15.1990, Discriminator Loss = 8.0050\n",
      "Epoch 44: Generator Loss = 22.5797, Discriminator Loss = 7.3863\n",
      "Epoch 45: Generator Loss = 18.5855, Discriminator Loss = 6.7747\n",
      "Epoch 46: Generator Loss = 23.1011, Discriminator Loss = 5.6917\n",
      "Epoch 47: Generator Loss = 31.3572, Discriminator Loss = 6.5758\n",
      "Epoch 48: Generator Loss = 33.0335, Discriminator Loss = 6.2681\n",
      "Epoch 49: Generator Loss = 20.8429, Discriminator Loss = 7.9483\n",
      "Epoch 50: Generator Loss = 24.5567, Discriminator Loss = 6.7931\n",
      "Epoch 51: Generator Loss = 33.5306, Discriminator Loss = 5.1373\n",
      "Epoch 52: Generator Loss = 27.2367, Discriminator Loss = 5.8098\n",
      "Epoch 53: Generator Loss = 25.4315, Discriminator Loss = 4.5848\n",
      "Epoch 54: Generator Loss = 24.4454, Discriminator Loss = 6.4995\n",
      "Epoch 55: Generator Loss = 32.0291, Discriminator Loss = 4.5577\n",
      "Epoch 56: Generator Loss = 23.1812, Discriminator Loss = 5.4147\n",
      "Epoch 57: Generator Loss = 28.5023, Discriminator Loss = 6.5013\n",
      "Epoch 58: Generator Loss = 32.4855, Discriminator Loss = 3.8174\n",
      "Epoch 59: Generator Loss = 33.1426, Discriminator Loss = 5.0099\n",
      "Epoch 60: Generator Loss = 40.7236, Discriminator Loss = 3.0618\n",
      "Epoch 61: Generator Loss = 42.0572, Discriminator Loss = 6.4795\n",
      "Epoch 62: Generator Loss = 31.9543, Discriminator Loss = 5.8647\n",
      "Epoch 63: Generator Loss = 31.5559, Discriminator Loss = 5.5026\n",
      "Epoch 64: Generator Loss = 36.6537, Discriminator Loss = 4.3108\n",
      "Epoch 65: Generator Loss = 33.2001, Discriminator Loss = 4.3649\n",
      "Epoch 66: Generator Loss = 30.5918, Discriminator Loss = 4.7345\n",
      "Epoch 67: Generator Loss = 48.9129, Discriminator Loss = 3.0188\n",
      "Epoch 68: Generator Loss = 28.1690, Discriminator Loss = 4.1095\n",
      "Epoch 69: Generator Loss = 33.0405, Discriminator Loss = 8.0395\n",
      "Epoch 70: Generator Loss = 35.6589, Discriminator Loss = 4.0549\n",
      "Epoch 71: Generator Loss = 35.2953, Discriminator Loss = 4.0182\n",
      "Epoch 72: Generator Loss = 47.5577, Discriminator Loss = 3.0247\n",
      "Epoch 73: Generator Loss = 41.0558, Discriminator Loss = 4.3022\n",
      "Epoch 74: Generator Loss = 36.2562, Discriminator Loss = 3.8732\n",
      "Epoch 75: Generator Loss = 41.5475, Discriminator Loss = 4.0038\n",
      "Epoch 76: Generator Loss = 45.3245, Discriminator Loss = 3.0706\n",
      "Epoch 77: Generator Loss = 48.8615, Discriminator Loss = 2.2369\n",
      "Epoch 78: Generator Loss = 47.2013, Discriminator Loss = 3.2106\n",
      "Epoch 79: Generator Loss = 37.9197, Discriminator Loss = 3.9891\n",
      "Epoch 80: Generator Loss = 39.0117, Discriminator Loss = 2.6292\n",
      "Epoch 81: Generator Loss = 45.2439, Discriminator Loss = 1.7134\n",
      "Epoch 82: Generator Loss = 69.5725, Discriminator Loss = 2.2940\n",
      "Epoch 83: Generator Loss = 56.0718, Discriminator Loss = 3.2184\n",
      "Epoch 84: Generator Loss = 42.5898, Discriminator Loss = 1.6716\n",
      "Epoch 85: Generator Loss = 49.5894, Discriminator Loss = 2.6810\n",
      "Epoch 86: Generator Loss = 57.6918, Discriminator Loss = 2.7181\n",
      "Epoch 87: Generator Loss = 41.2135, Discriminator Loss = 4.1924\n",
      "Epoch 88: Generator Loss = 58.1972, Discriminator Loss = 2.4166\n",
      "Epoch 89: Generator Loss = 59.7767, Discriminator Loss = 1.6823\n",
      "Epoch 90: Generator Loss = 46.9831, Discriminator Loss = 3.5292\n",
      "Epoch 91: Generator Loss = 57.4308, Discriminator Loss = 1.7854\n",
      "Epoch 92: Generator Loss = 52.5796, Discriminator Loss = 1.2546\n",
      "Epoch 93: Generator Loss = 53.2008, Discriminator Loss = 0.8888\n",
      "Epoch 94: Generator Loss = 55.6413, Discriminator Loss = 1.5536\n",
      "Epoch 95: Generator Loss = 52.5367, Discriminator Loss = 2.0239\n",
      "Epoch 96: Generator Loss = 57.1969, Discriminator Loss = 2.3822\n",
      "Epoch 97: Generator Loss = 62.8532, Discriminator Loss = 1.4333\n",
      "Epoch 98: Generator Loss = 57.8801, Discriminator Loss = 1.5314\n",
      "Epoch 99: Generator Loss = 54.9218, Discriminator Loss = 0.8415\n",
      "Epoch 100: Generator Loss = 79.6535, Discriminator Loss = 1.9708\n",
      "Epoch 101: Generator Loss = 49.4533, Discriminator Loss = 2.2661\n",
      "Epoch 102: Generator Loss = 67.6405, Discriminator Loss = 2.1185\n",
      "Epoch 103: Generator Loss = 65.5684, Discriminator Loss = 0.8213\n",
      "Epoch 104: Generator Loss = 64.5551, Discriminator Loss = 0.9100\n",
      "Epoch 105: Generator Loss = 69.0526, Discriminator Loss = 0.7311\n",
      "Epoch 106: Generator Loss = 63.8524, Discriminator Loss = 1.5709\n",
      "Epoch 107: Generator Loss = 52.1508, Discriminator Loss = 0.9943\n",
      "Epoch 108: Generator Loss = 75.1816, Discriminator Loss = 0.7768\n",
      "Epoch 109: Generator Loss = 81.3232, Discriminator Loss = 1.5171\n",
      "Epoch 110: Generator Loss = 59.3115, Discriminator Loss = 0.9958\n",
      "Epoch 111: Generator Loss = 77.1932, Discriminator Loss = 0.9455\n",
      "Epoch 112: Generator Loss = 76.6148, Discriminator Loss = 0.5674\n",
      "Epoch 113: Generator Loss = 57.7459, Discriminator Loss = 10.8104\n",
      "Epoch 114: Generator Loss = 30.8626, Discriminator Loss = 5.2511\n",
      "Epoch 115: Generator Loss = 50.5079, Discriminator Loss = 4.3079\n",
      "Epoch 116: Generator Loss = 41.2990, Discriminator Loss = 2.7238\n",
      "Epoch 117: Generator Loss = 50.7920, Discriminator Loss = 3.5087\n",
      "Epoch 118: Generator Loss = 49.3389, Discriminator Loss = 3.5559\n",
      "Epoch 119: Generator Loss = 46.5673, Discriminator Loss = 2.4452\n",
      "Epoch 120: Generator Loss = 58.3087, Discriminator Loss = 1.2848\n",
      "Epoch 121: Generator Loss = 54.5286, Discriminator Loss = 1.0116\n",
      "Epoch 122: Generator Loss = 77.5161, Discriminator Loss = 1.5062\n",
      "Epoch 123: Generator Loss = 61.7737, Discriminator Loss = 0.7940\n",
      "Epoch 124: Generator Loss = 69.9791, Discriminator Loss = 1.4278\n",
      "Epoch 125: Generator Loss = 77.6239, Discriminator Loss = 0.9897\n",
      "Epoch 126: Generator Loss = 58.4136, Discriminator Loss = 0.9040\n",
      "Epoch 127: Generator Loss = 56.5276, Discriminator Loss = 0.8959\n",
      "Epoch 128: Generator Loss = 82.8867, Discriminator Loss = 0.9330\n",
      "Epoch 129: Generator Loss = 76.1409, Discriminator Loss = 0.3738\n",
      "Epoch 130: Generator Loss = 93.4877, Discriminator Loss = 0.7336\n",
      "Epoch 131: Generator Loss = 80.3726, Discriminator Loss = 0.5696\n",
      "Epoch 132: Generator Loss = 68.3595, Discriminator Loss = 0.7477\n",
      "Epoch 133: Generator Loss = 69.4270, Discriminator Loss = 0.9527\n",
      "Epoch 134: Generator Loss = 73.0283, Discriminator Loss = 0.8973\n",
      "Epoch 135: Generator Loss = 70.1748, Discriminator Loss = 0.8973\n",
      "Epoch 136: Generator Loss = 72.2849, Discriminator Loss = 1.9452\n",
      "Epoch 137: Generator Loss = 75.7855, Discriminator Loss = 0.7447\n",
      "Epoch 138: Generator Loss = 87.5944, Discriminator Loss = 1.2880\n",
      "Epoch 139: Generator Loss = 79.5059, Discriminator Loss = 0.6810\n",
      "Epoch 140: Generator Loss = 91.4667, Discriminator Loss = 0.7830\n",
      "Epoch 141: Generator Loss = 79.5474, Discriminator Loss = 0.7847\n",
      "Epoch 142: Generator Loss = 76.6112, Discriminator Loss = 0.7420\n",
      "Epoch 143: Generator Loss = 73.6120, Discriminator Loss = 0.9055\n",
      "Epoch 144: Generator Loss = 80.4698, Discriminator Loss = 0.8651\n",
      "Epoch 145: Generator Loss = 85.9770, Discriminator Loss = 0.7132\n",
      "Epoch 146: Generator Loss = 70.8625, Discriminator Loss = 1.6136\n",
      "Epoch 147: Generator Loss = 65.4279, Discriminator Loss = 1.6471\n",
      "Epoch 148: Generator Loss = 57.9474, Discriminator Loss = 1.0167\n",
      "Epoch 149: Generator Loss = 97.0454, Discriminator Loss = 2.1672\n",
      "Epoch 150: Generator Loss = 61.1235, Discriminator Loss = 1.3716\n",
      "Epoch 151: Generator Loss = 80.4024, Discriminator Loss = 1.1338\n",
      "Epoch 152: Generator Loss = 77.2748, Discriminator Loss = 0.5778\n",
      "Epoch 153: Generator Loss = 76.5902, Discriminator Loss = 0.7834\n",
      "Epoch 154: Generator Loss = 68.5266, Discriminator Loss = 1.6838\n",
      "Epoch 155: Generator Loss = 91.0865, Discriminator Loss = 0.9179\n",
      "Epoch 156: Generator Loss = 54.4739, Discriminator Loss = 5.9499\n",
      "Epoch 157: Generator Loss = 55.4616, Discriminator Loss = 2.9246\n",
      "Epoch 158: Generator Loss = 68.2013, Discriminator Loss = 1.4644\n",
      "Epoch 159: Generator Loss = 67.3922, Discriminator Loss = 1.2704\n",
      "Epoch 160: Generator Loss = 67.2627, Discriminator Loss = 1.1073\n",
      "Epoch 161: Generator Loss = 67.6644, Discriminator Loss = 0.9186\n",
      "Epoch 162: Generator Loss = 92.5034, Discriminator Loss = 0.7443\n",
      "Epoch 163: Generator Loss = 79.4000, Discriminator Loss = 0.5918\n",
      "Epoch 164: Generator Loss = 95.6720, Discriminator Loss = 1.1327\n",
      "Epoch 165: Generator Loss = 92.5337, Discriminator Loss = 0.6020\n",
      "Epoch 166: Generator Loss = 85.5188, Discriminator Loss = 1.3965\n",
      "Epoch 167: Generator Loss = 95.5208, Discriminator Loss = 3.2235\n",
      "Epoch 168: Generator Loss = 95.1776, Discriminator Loss = 0.7899\n",
      "Epoch 169: Generator Loss = 94.0654, Discriminator Loss = 0.9679\n",
      "Epoch 170: Generator Loss = 70.2212, Discriminator Loss = 0.3942\n",
      "Epoch 171: Generator Loss = 85.4231, Discriminator Loss = 1.0431\n",
      "Epoch 172: Generator Loss = 85.3996, Discriminator Loss = 1.1179\n",
      "Epoch 173: Generator Loss = 85.1073, Discriminator Loss = 0.5111\n",
      "Epoch 174: Generator Loss = 84.0654, Discriminator Loss = 0.4565\n",
      "Epoch 175: Generator Loss = 83.6028, Discriminator Loss = 1.1932\n",
      "Epoch 176: Generator Loss = 76.1306, Discriminator Loss = 0.4946\n",
      "Epoch 177: Generator Loss = 101.1862, Discriminator Loss = 0.4324\n",
      "Epoch 178: Generator Loss = 82.2345, Discriminator Loss = 0.6658\n",
      "Epoch 179: Generator Loss = 78.6089, Discriminator Loss = 1.7376\n",
      "Epoch 180: Generator Loss = 89.9312, Discriminator Loss = 5.7820\n",
      "Epoch 181: Generator Loss = 63.2821, Discriminator Loss = 5.3374\n",
      "Epoch 182: Generator Loss = 98.6116, Discriminator Loss = 8.4992\n",
      "Epoch 183: Generator Loss = 72.2986, Discriminator Loss = 1.8967\n",
      "Epoch 184: Generator Loss = 73.4453, Discriminator Loss = 1.1437\n",
      "Epoch 185: Generator Loss = 69.9678, Discriminator Loss = 2.0307\n",
      "Epoch 186: Generator Loss = 69.3805, Discriminator Loss = 1.6030\n",
      "Epoch 187: Generator Loss = 69.1270, Discriminator Loss = 0.6778\n",
      "Epoch 188: Generator Loss = 68.3748, Discriminator Loss = 1.4596\n",
      "Epoch 189: Generator Loss = 100.6745, Discriminator Loss = 0.7314\n",
      "Epoch 190: Generator Loss = 85.5636, Discriminator Loss = 0.9717\n",
      "Epoch 191: Generator Loss = 78.5818, Discriminator Loss = 9.1094\n",
      "Epoch 192: Generator Loss = 41.9761, Discriminator Loss = 7.5774\n",
      "Epoch 193: Generator Loss = 41.8439, Discriminator Loss = 8.3124\n",
      "Epoch 194: Generator Loss = 49.1561, Discriminator Loss = 4.1425\n",
      "Epoch 195: Generator Loss = 41.2357, Discriminator Loss = 3.2460\n",
      "Epoch 196: Generator Loss = 45.5155, Discriminator Loss = 2.2221\n",
      "Epoch 197: Generator Loss = 59.0409, Discriminator Loss = 2.4944\n",
      "Epoch 198: Generator Loss = 57.2758, Discriminator Loss = 2.5912\n",
      "Epoch 199: Generator Loss = 82.3585, Discriminator Loss = 2.2049\n",
      "Epoch 200: Generator Loss = 73.2701, Discriminator Loss = 1.3174\n",
      "Epoch 201: Generator Loss = 56.8254, Discriminator Loss = 3.2133\n",
      "Epoch 202: Generator Loss = 66.4584, Discriminator Loss = 1.4126\n",
      "Epoch 203: Generator Loss = 76.2227, Discriminator Loss = 1.1894\n",
      "Epoch 204: Generator Loss = 79.0886, Discriminator Loss = 2.6650\n",
      "Epoch 205: Generator Loss = 54.8754, Discriminator Loss = 2.3663\n",
      "Epoch 206: Generator Loss = 64.4531, Discriminator Loss = 0.6881\n",
      "Epoch 207: Generator Loss = 72.7359, Discriminator Loss = 0.8222\n",
      "Epoch 208: Generator Loss = 67.1371, Discriminator Loss = 1.1364\n",
      "Epoch 209: Generator Loss = 63.3267, Discriminator Loss = 4.0684\n",
      "Epoch 210: Generator Loss = 70.4792, Discriminator Loss = 2.0432\n",
      "Epoch 211: Generator Loss = 81.0710, Discriminator Loss = 1.0710\n",
      "Epoch 212: Generator Loss = 55.1220, Discriminator Loss = 1.5522\n",
      "Epoch 213: Generator Loss = 62.3300, Discriminator Loss = 2.3618\n",
      "Epoch 214: Generator Loss = 75.6871, Discriminator Loss = 2.1832\n",
      "Epoch 215: Generator Loss = 78.8104, Discriminator Loss = 1.8299\n",
      "Epoch 216: Generator Loss = 62.3457, Discriminator Loss = 1.4765\n",
      "Epoch 217: Generator Loss = 87.9368, Discriminator Loss = 1.0326\n",
      "Epoch 218: Generator Loss = 52.3431, Discriminator Loss = 3.9602\n",
      "Epoch 219: Generator Loss = 83.1987, Discriminator Loss = 6.6285\n",
      "Epoch 220: Generator Loss = 58.6914, Discriminator Loss = 2.5695\n",
      "Epoch 221: Generator Loss = 59.0869, Discriminator Loss = 2.3579\n",
      "Epoch 222: Generator Loss = 76.7926, Discriminator Loss = 2.6194\n",
      "Epoch 223: Generator Loss = 45.3540, Discriminator Loss = 3.0363\n",
      "Epoch 224: Generator Loss = 65.8567, Discriminator Loss = 2.5634\n",
      "Epoch 225: Generator Loss = 66.5554, Discriminator Loss = 1.3065\n",
      "Epoch 226: Generator Loss = 72.8346, Discriminator Loss = 1.5723\n",
      "Epoch 227: Generator Loss = 66.7636, Discriminator Loss = 1.2557\n",
      "Epoch 228: Generator Loss = 65.6919, Discriminator Loss = 0.7846\n",
      "Epoch 229: Generator Loss = 57.5866, Discriminator Loss = 2.8142\n",
      "Epoch 230: Generator Loss = 71.0226, Discriminator Loss = 4.8645\n",
      "Epoch 231: Generator Loss = 79.1915, Discriminator Loss = 1.7945\n",
      "Epoch 232: Generator Loss = 41.7511, Discriminator Loss = 7.6025\n",
      "Epoch 233: Generator Loss = 52.0356, Discriminator Loss = 2.1568\n",
      "Epoch 234: Generator Loss = 59.3595, Discriminator Loss = 1.7201\n",
      "Epoch 235: Generator Loss = 61.4107, Discriminator Loss = 1.9954\n",
      "Epoch 236: Generator Loss = 67.3831, Discriminator Loss = 0.9246\n",
      "Epoch 237: Generator Loss = 64.2879, Discriminator Loss = 0.9475\n",
      "Epoch 238: Generator Loss = 60.4817, Discriminator Loss = 0.6569\n",
      "Epoch 239: Generator Loss = 91.2609, Discriminator Loss = 0.5827\n",
      "Epoch 240: Generator Loss = 94.7367, Discriminator Loss = 0.7352\n",
      "Epoch 241: Generator Loss = 74.6643, Discriminator Loss = 2.2638\n",
      "Epoch 242: Generator Loss = 46.5265, Discriminator Loss = 6.4347\n",
      "Epoch 243: Generator Loss = 77.2893, Discriminator Loss = 1.3476\n",
      "Epoch 244: Generator Loss = 74.6536, Discriminator Loss = 2.1352\n",
      "Epoch 245: Generator Loss = 64.0353, Discriminator Loss = 2.0520\n",
      "Epoch 246: Generator Loss = 65.9808, Discriminator Loss = 1.6271\n",
      "Epoch 247: Generator Loss = 50.4859, Discriminator Loss = 1.7151\n",
      "Epoch 248: Generator Loss = 67.3649, Discriminator Loss = 1.6928\n",
      "Epoch 249: Generator Loss = 59.4729, Discriminator Loss = 2.0274\n",
      "Epoch 250: Generator Loss = 87.8822, Discriminator Loss = 0.9101\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/15 | Train Loss: 0.6169 Acc: 0.7161\n",
      "Epoch 2/15 | Train Loss: 0.4299 Acc: 0.7943\n",
      "Epoch 3/15 | Train Loss: 0.4099 Acc: 0.7656\n",
      "Epoch 4/15 | Train Loss: 0.3471 Acc: 0.8203\n",
      "Epoch 5/15 | Train Loss: 0.3346 Acc: 0.8177\n",
      "Epoch 6/15 | Train Loss: 0.3702 Acc: 0.8125\n",
      "Epoch 7/15 | Train Loss: 0.3684 Acc: 0.7865\n",
      "Epoch 8/15 | Train Loss: 0.3785 Acc: 0.8255\n",
      "Epoch 9/15 | Train Loss: 0.3080 Acc: 0.8359\n",
      "Epoch 10/15 | Train Loss: 0.2545 Acc: 0.8802\n",
      "Epoch 11/15 | Train Loss: 0.2786 Acc: 0.8568\n",
      "Epoch 12/15 | Train Loss: 0.2676 Acc: 0.8828\n",
      "Epoch 13/15 | Train Loss: 0.2773 Acc: 0.8698\n",
      "Epoch 14/15 | Train Loss: 0.2664 Acc: 0.8828\n",
      "Epoch 15/15 | Train Loss: 0.2666 Acc: 0.8776\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 3.0min\n",
      "Epoch 1/15 | Train Loss: 0.4982 Acc: 0.7344\n",
      "Epoch 2/15 | Train Loss: 0.3869 Acc: 0.7865\n",
      "Epoch 3/15 | Train Loss: 0.3626 Acc: 0.8151\n",
      "Epoch 4/15 | Train Loss: 0.3536 Acc: 0.7917\n",
      "Epoch 5/15 | Train Loss: 0.3534 Acc: 0.8021\n",
      "Epoch 6/15 | Train Loss: 0.3301 Acc: 0.8255\n",
      "Epoch 7/15 | Train Loss: 0.3394 Acc: 0.8203\n",
      "Epoch 8/15 | Train Loss: 0.3170 Acc: 0.8203\n",
      "Epoch 9/15 | Train Loss: 0.3147 Acc: 0.8385\n",
      "Epoch 10/15 | Train Loss: 0.3183 Acc: 0.8229\n",
      "Epoch 11/15 | Train Loss: 0.2897 Acc: 0.8594\n",
      "Epoch 12/15 | Train Loss: 0.3015 Acc: 0.8411\n",
      "Epoch 13/15 | Train Loss: 0.2892 Acc: 0.8542\n",
      "Epoch 14/15 | Train Loss: 0.2895 Acc: 0.8646\n",
      "Epoch 15/15 | Train Loss: 0.2740 Acc: 0.8776\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.4955 Acc: 0.7318\n",
      "Epoch 2/15 | Train Loss: 0.4133 Acc: 0.7917\n",
      "Epoch 3/15 | Train Loss: 0.3977 Acc: 0.7786\n",
      "Epoch 4/15 | Train Loss: 0.3635 Acc: 0.8073\n",
      "Epoch 5/15 | Train Loss: 0.3428 Acc: 0.7839\n",
      "Epoch 6/15 | Train Loss: 0.3399 Acc: 0.8281\n",
      "Epoch 7/15 | Train Loss: 0.3518 Acc: 0.8255\n",
      "Epoch 8/15 | Train Loss: 0.3272 Acc: 0.8177\n",
      "Epoch 9/15 | Train Loss: 0.2947 Acc: 0.8776\n",
      "Epoch 10/15 | Train Loss: 0.2914 Acc: 0.8594\n",
      "Epoch 11/15 | Train Loss: 0.3010 Acc: 0.8646\n",
      "Epoch 12/15 | Train Loss: 0.2980 Acc: 0.8438\n",
      "Epoch 13/15 | Train Loss: 0.2992 Acc: 0.8568\n",
      "Epoch 14/15 | Train Loss: 0.2765 Acc: 0.8828\n",
      "Epoch 15/15 | Train Loss: 0.2602 Acc: 0.8776\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5583 Acc: 0.8000\n",
      "Epoch 2/15 | Train Loss: 0.4141 Acc: 0.8095\n",
      "Epoch 3/15 | Train Loss: 0.4240 Acc: 0.8159\n",
      "Epoch 4/15 | Train Loss: 0.4020 Acc: 0.8508\n",
      "Epoch 5/15 | Train Loss: 0.3761 Acc: 0.8413\n",
      "Epoch 6/15 | Train Loss: 0.3477 Acc: 0.8444\n",
      "Epoch 7/15 | Train Loss: 0.3547 Acc: 0.8349\n",
      "Epoch 8/15 | Train Loss: 0.3849 Acc: 0.8254\n",
      "Epoch 9/15 | Train Loss: 0.3227 Acc: 0.8667\n",
      "Epoch 10/15 | Train Loss: 0.3377 Acc: 0.8603\n",
      "Epoch 11/15 | Train Loss: 0.3181 Acc: 0.8794\n",
      "Epoch 12/15 | Train Loss: 0.2978 Acc: 0.8667\n",
      "Epoch 13/15 | Train Loss: 0.3319 Acc: 0.8540\n",
      "Epoch 14/15 | Train Loss: 0.3180 Acc: 0.8762\n",
      "Epoch 15/15 | Train Loss: 0.2867 Acc: 0.8730\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4717 Acc: 0.7972\n",
      "Epoch 2/15 | Train Loss: 0.3934 Acc: 0.8683\n",
      "Epoch 3/15 | Train Loss: 0.3492 Acc: 0.8790\n",
      "Epoch 4/15 | Train Loss: 0.3090 Acc: 0.8897\n",
      "Epoch 5/15 | Train Loss: 0.3094 Acc: 0.8683\n",
      "Epoch 6/15 | Train Loss: 0.2549 Acc: 0.9039\n",
      "Epoch 7/15 | Train Loss: 0.2753 Acc: 0.9004\n",
      "Epoch 8/15 | Train Loss: 0.2759 Acc: 0.8861\n",
      "Epoch 9/15 | Train Loss: 0.2652 Acc: 0.9075\n",
      "Epoch 10/15 | Train Loss: 0.2467 Acc: 0.9146\n",
      "Epoch 11/15 | Train Loss: 0.2398 Acc: 0.8897\n",
      "Epoch 12/15 | Train Loss: 0.2213 Acc: 0.9004\n",
      "Epoch 13/15 | Train Loss: 0.2192 Acc: 0.9039\n",
      "Epoch 14/15 | Train Loss: 0.2176 Acc: 0.9146\n",
      "Epoch 15/15 | Train Loss: 0.2524 Acc: 0.9039\n",
      "[CV] END .....multiplier_generated_samples=2, scale_factor=1; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.7671 Acc: 0.6008\n",
      "Epoch 2/15 | Train Loss: 0.5954 Acc: 0.6957\n",
      "Epoch 3/15 | Train Loss: 0.4991 Acc: 0.7549\n",
      "Epoch 4/15 | Train Loss: 0.5532 Acc: 0.7154\n",
      "Epoch 5/15 | Train Loss: 0.5039 Acc: 0.7391\n",
      "Epoch 6/15 | Train Loss: 0.4155 Acc: 0.8182\n",
      "Epoch 7/15 | Train Loss: 0.4448 Acc: 0.7945\n",
      "Epoch 8/15 | Train Loss: 0.3832 Acc: 0.8182\n",
      "Epoch 9/15 | Train Loss: 0.4075 Acc: 0.8182\n",
      "Epoch 10/15 | Train Loss: 0.3623 Acc: 0.8340\n",
      "Epoch 11/15 | Train Loss: 0.3230 Acc: 0.8538\n",
      "Epoch 12/15 | Train Loss: 0.3473 Acc: 0.8656\n",
      "Epoch 13/15 | Train Loss: 0.4001 Acc: 0.7905\n",
      "Epoch 14/15 | Train Loss: 0.3362 Acc: 0.8538\n",
      "Epoch 15/15 | Train Loss: 0.3529 Acc: 0.8656\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7032 Acc: 0.6166\n",
      "Epoch 2/15 | Train Loss: 0.6178 Acc: 0.6443\n",
      "Epoch 3/15 | Train Loss: 0.5663 Acc: 0.6957\n",
      "Epoch 4/15 | Train Loss: 0.5294 Acc: 0.6838\n",
      "Epoch 5/15 | Train Loss: 0.5626 Acc: 0.7115\n",
      "Epoch 6/15 | Train Loss: 0.4867 Acc: 0.8063\n",
      "Epoch 7/15 | Train Loss: 0.4740 Acc: 0.7470\n",
      "Epoch 8/15 | Train Loss: 0.4584 Acc: 0.7668\n",
      "Epoch 9/15 | Train Loss: 0.4723 Acc: 0.7747\n",
      "Epoch 10/15 | Train Loss: 0.4645 Acc: 0.7747\n",
      "Epoch 11/15 | Train Loss: 0.4191 Acc: 0.8103\n",
      "Epoch 12/15 | Train Loss: 0.3945 Acc: 0.8300\n",
      "Epoch 13/15 | Train Loss: 0.4407 Acc: 0.7945\n",
      "Epoch 14/15 | Train Loss: 0.4662 Acc: 0.7826\n",
      "Epoch 15/15 | Train Loss: 0.3954 Acc: 0.8103\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7227 Acc: 0.6443\n",
      "Epoch 2/15 | Train Loss: 0.6297 Acc: 0.6719\n",
      "Epoch 3/15 | Train Loss: 0.5581 Acc: 0.6996\n",
      "Epoch 4/15 | Train Loss: 0.5621 Acc: 0.7194\n",
      "Epoch 5/15 | Train Loss: 0.4810 Acc: 0.7431\n",
      "Epoch 6/15 | Train Loss: 0.5086 Acc: 0.7589\n",
      "Epoch 7/15 | Train Loss: 0.4675 Acc: 0.7668\n",
      "Epoch 8/15 | Train Loss: 0.5244 Acc: 0.7668\n",
      "Epoch 9/15 | Train Loss: 0.5010 Acc: 0.7312\n",
      "Epoch 10/15 | Train Loss: 0.4567 Acc: 0.7668\n",
      "Epoch 11/15 | Train Loss: 0.4363 Acc: 0.7984\n",
      "Epoch 12/15 | Train Loss: 0.4319 Acc: 0.8063\n",
      "Epoch 13/15 | Train Loss: 0.4321 Acc: 0.7866\n",
      "Epoch 14/15 | Train Loss: 0.4461 Acc: 0.7589\n",
      "Epoch 15/15 | Train Loss: 0.3896 Acc: 0.8379\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6423 Acc: 0.7089\n",
      "Epoch 2/15 | Train Loss: 0.5215 Acc: 0.7848\n",
      "Epoch 3/15 | Train Loss: 0.4750 Acc: 0.7848\n",
      "Epoch 4/15 | Train Loss: 0.5819 Acc: 0.7173\n",
      "Epoch 5/15 | Train Loss: 0.4806 Acc: 0.8101\n",
      "Epoch 6/15 | Train Loss: 0.4597 Acc: 0.8017\n",
      "Epoch 7/15 | Train Loss: 0.4734 Acc: 0.8017\n",
      "Epoch 8/15 | Train Loss: 0.5040 Acc: 0.7764\n",
      "Epoch 9/15 | Train Loss: 0.3798 Acc: 0.8270\n",
      "Epoch 10/15 | Train Loss: 0.4544 Acc: 0.7679\n",
      "Epoch 11/15 | Train Loss: 0.3827 Acc: 0.8228\n",
      "Epoch 12/15 | Train Loss: 0.4119 Acc: 0.8101\n",
      "Epoch 13/15 | Train Loss: 0.3933 Acc: 0.8397\n",
      "Epoch 14/15 | Train Loss: 0.4002 Acc: 0.8186\n",
      "Epoch 15/15 | Train Loss: 0.3958 Acc: 0.8228\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5174 Acc: 0.7588\n",
      "Epoch 2/15 | Train Loss: 0.4718 Acc: 0.8202\n",
      "Epoch 3/15 | Train Loss: 0.4114 Acc: 0.8465\n",
      "Epoch 4/15 | Train Loss: 0.4433 Acc: 0.8289\n",
      "Epoch 5/15 | Train Loss: 0.3507 Acc: 0.8640\n",
      "Epoch 6/15 | Train Loss: 0.3930 Acc: 0.8640\n",
      "Epoch 7/15 | Train Loss: 0.2988 Acc: 0.8947\n",
      "Epoch 8/15 | Train Loss: 0.3735 Acc: 0.8553\n",
      "Epoch 9/15 | Train Loss: 0.3479 Acc: 0.8772\n",
      "Epoch 10/15 | Train Loss: 0.2872 Acc: 0.8772\n",
      "Epoch 11/15 | Train Loss: 0.2952 Acc: 0.8728\n",
      "Epoch 12/15 | Train Loss: 0.3360 Acc: 0.8728\n",
      "Epoch 13/15 | Train Loss: 0.2694 Acc: 0.8816\n",
      "Epoch 14/15 | Train Loss: 0.2541 Acc: 0.8947\n",
      "Epoch 15/15 | Train Loss: 0.2969 Acc: 0.8860\n",
      "[CV] END ...multiplier_generated_samples=0.5, scale_factor=1; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6038 Acc: 0.7003\n",
      "Epoch 2/15 | Train Loss: 0.4789 Acc: 0.7205\n",
      "Epoch 3/15 | Train Loss: 0.5394 Acc: 0.7340\n",
      "Epoch 4/15 | Train Loss: 0.4487 Acc: 0.7912\n",
      "Epoch 5/15 | Train Loss: 0.4116 Acc: 0.7845\n",
      "Epoch 6/15 | Train Loss: 0.4198 Acc: 0.8047\n",
      "Epoch 7/15 | Train Loss: 0.3816 Acc: 0.7912\n",
      "Epoch 8/15 | Train Loss: 0.3783 Acc: 0.8350\n",
      "Epoch 9/15 | Train Loss: 0.3540 Acc: 0.8384\n",
      "Epoch 10/15 | Train Loss: 0.3357 Acc: 0.8485\n",
      "Epoch 11/15 | Train Loss: 0.3599 Acc: 0.8148\n",
      "Epoch 12/15 | Train Loss: 0.2882 Acc: 0.8788\n",
      "Epoch 13/15 | Train Loss: 0.3057 Acc: 0.8485\n",
      "Epoch 14/15 | Train Loss: 0.3467 Acc: 0.8316\n",
      "Epoch 15/15 | Train Loss: 0.2783 Acc: 0.8855\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.6774 Acc: 0.6599\n",
      "Epoch 2/15 | Train Loss: 0.6192 Acc: 0.6970\n",
      "Epoch 3/15 | Train Loss: 0.4989 Acc: 0.7374\n",
      "Epoch 4/15 | Train Loss: 0.4716 Acc: 0.7172\n",
      "Epoch 5/15 | Train Loss: 0.4587 Acc: 0.7845\n",
      "Epoch 6/15 | Train Loss: 0.4515 Acc: 0.7407\n",
      "Epoch 7/15 | Train Loss: 0.4599 Acc: 0.7407\n",
      "Epoch 8/15 | Train Loss: 0.3954 Acc: 0.7879\n",
      "Epoch 9/15 | Train Loss: 0.4165 Acc: 0.7710\n",
      "Epoch 10/15 | Train Loss: 0.4395 Acc: 0.7441\n",
      "Epoch 11/15 | Train Loss: 0.3541 Acc: 0.8350\n",
      "Epoch 12/15 | Train Loss: 0.3731 Acc: 0.8215\n",
      "Epoch 13/15 | Train Loss: 0.3630 Acc: 0.8384\n",
      "Epoch 14/15 | Train Loss: 0.3621 Acc: 0.8215\n",
      "Epoch 15/15 | Train Loss: 0.3492 Acc: 0.8215\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5991 Acc: 0.6667\n",
      "Epoch 2/15 | Train Loss: 0.5397 Acc: 0.7138\n",
      "Epoch 3/15 | Train Loss: 0.4637 Acc: 0.7374\n",
      "Epoch 4/15 | Train Loss: 0.4872 Acc: 0.7273\n",
      "Epoch 5/15 | Train Loss: 0.4610 Acc: 0.7441\n",
      "Epoch 6/15 | Train Loss: 0.4190 Acc: 0.7811\n",
      "Epoch 7/15 | Train Loss: 0.4165 Acc: 0.7879\n",
      "Epoch 8/15 | Train Loss: 0.4212 Acc: 0.7845\n",
      "Epoch 9/15 | Train Loss: 0.4039 Acc: 0.7879\n",
      "Epoch 10/15 | Train Loss: 0.3705 Acc: 0.8148\n",
      "Epoch 11/15 | Train Loss: 0.3946 Acc: 0.8047\n",
      "Epoch 12/15 | Train Loss: 0.3504 Acc: 0.8418\n",
      "Epoch 13/15 | Train Loss: 0.3758 Acc: 0.8215\n",
      "Epoch 14/15 | Train Loss: 0.3495 Acc: 0.8249\n",
      "Epoch 15/15 | Train Loss: 0.3608 Acc: 0.8316\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.5083 Acc: 0.7529\n",
      "Epoch 2/15 | Train Loss: 0.5267 Acc: 0.7947\n",
      "Epoch 3/15 | Train Loss: 0.4604 Acc: 0.7871\n",
      "Epoch 4/15 | Train Loss: 0.4331 Acc: 0.7985\n",
      "Epoch 5/15 | Train Loss: 0.4365 Acc: 0.7833\n",
      "Epoch 6/15 | Train Loss: 0.3966 Acc: 0.8403\n",
      "Epoch 7/15 | Train Loss: 0.4098 Acc: 0.8099\n",
      "Epoch 8/15 | Train Loss: 0.4095 Acc: 0.8365\n",
      "Epoch 9/15 | Train Loss: 0.3632 Acc: 0.8365\n",
      "Epoch 10/15 | Train Loss: 0.3639 Acc: 0.8441\n",
      "Epoch 11/15 | Train Loss: 0.3344 Acc: 0.8555\n",
      "Epoch 12/15 | Train Loss: 0.3447 Acc: 0.8403\n",
      "Epoch 13/15 | Train Loss: 0.3548 Acc: 0.8365\n",
      "Epoch 14/15 | Train Loss: 0.3512 Acc: 0.8251\n",
      "Epoch 15/15 | Train Loss: 0.3520 Acc: 0.8441\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5157 Acc: 0.7439\n",
      "Epoch 2/15 | Train Loss: 0.4260 Acc: 0.8415\n",
      "Epoch 3/15 | Train Loss: 0.3687 Acc: 0.8333\n",
      "Epoch 4/15 | Train Loss: 0.3894 Acc: 0.8496\n",
      "Epoch 5/15 | Train Loss: 0.3430 Acc: 0.8618\n",
      "Epoch 6/15 | Train Loss: 0.3146 Acc: 0.8821\n",
      "Epoch 7/15 | Train Loss: 0.3001 Acc: 0.8740\n",
      "Epoch 8/15 | Train Loss: 0.3081 Acc: 0.8821\n",
      "Epoch 9/15 | Train Loss: 0.3214 Acc: 0.8780\n",
      "Epoch 10/15 | Train Loss: 0.2714 Acc: 0.9065\n",
      "Epoch 11/15 | Train Loss: 0.2521 Acc: 0.8984\n",
      "Epoch 12/15 | Train Loss: 0.2970 Acc: 0.8862\n",
      "Epoch 13/15 | Train Loss: 0.3139 Acc: 0.8984\n",
      "Epoch 14/15 | Train Loss: 0.2766 Acc: 0.8862\n",
      "Epoch 15/15 | Train Loss: 0.2947 Acc: 0.8780\n",
      "[CV] END ...multiplier_generated_samples=1, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7314 Acc: 0.5968\n",
      "Epoch 2/15 | Train Loss: 0.5466 Acc: 0.7273\n",
      "Epoch 3/15 | Train Loss: 0.5573 Acc: 0.7154\n",
      "Epoch 4/15 | Train Loss: 0.5052 Acc: 0.7866\n",
      "Epoch 5/15 | Train Loss: 0.4660 Acc: 0.7787\n",
      "Epoch 6/15 | Train Loss: 0.4591 Acc: 0.7708\n",
      "Epoch 7/15 | Train Loss: 0.4773 Acc: 0.7431\n",
      "Epoch 8/15 | Train Loss: 0.4181 Acc: 0.8024\n",
      "Epoch 9/15 | Train Loss: 0.4387 Acc: 0.8103\n",
      "Epoch 10/15 | Train Loss: 0.3638 Acc: 0.8379\n",
      "Epoch 11/15 | Train Loss: 0.3580 Acc: 0.8379\n",
      "Epoch 12/15 | Train Loss: 0.3949 Acc: 0.8261\n",
      "Epoch 13/15 | Train Loss: 0.3583 Acc: 0.8379\n",
      "Epoch 14/15 | Train Loss: 0.3482 Acc: 0.8458\n",
      "Epoch 15/15 | Train Loss: 0.3668 Acc: 0.8182\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7411 Acc: 0.5889\n",
      "Epoch 2/15 | Train Loss: 0.5938 Acc: 0.6957\n",
      "Epoch 3/15 | Train Loss: 0.6180 Acc: 0.6798\n",
      "Epoch 4/15 | Train Loss: 0.5055 Acc: 0.7549\n",
      "Epoch 5/15 | Train Loss: 0.5434 Acc: 0.7036\n",
      "Epoch 6/15 | Train Loss: 0.5353 Acc: 0.6957\n",
      "Epoch 7/15 | Train Loss: 0.5017 Acc: 0.7589\n",
      "Epoch 8/15 | Train Loss: 0.5017 Acc: 0.7628\n",
      "Epoch 9/15 | Train Loss: 0.4836 Acc: 0.7787\n",
      "Epoch 10/15 | Train Loss: 0.4365 Acc: 0.7905\n",
      "Epoch 11/15 | Train Loss: 0.4344 Acc: 0.7945\n",
      "Epoch 12/15 | Train Loss: 0.4468 Acc: 0.7549\n",
      "Epoch 13/15 | Train Loss: 0.4314 Acc: 0.7826\n",
      "Epoch 14/15 | Train Loss: 0.4160 Acc: 0.7984\n",
      "Epoch 15/15 | Train Loss: 0.4034 Acc: 0.8221\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6721 Acc: 0.6166\n",
      "Epoch 2/15 | Train Loss: 0.6614 Acc: 0.6443\n",
      "Epoch 3/15 | Train Loss: 0.5885 Acc: 0.6719\n",
      "Epoch 4/15 | Train Loss: 0.5400 Acc: 0.7233\n",
      "Epoch 5/15 | Train Loss: 0.5493 Acc: 0.7115\n",
      "Epoch 6/15 | Train Loss: 0.4895 Acc: 0.7549\n",
      "Epoch 7/15 | Train Loss: 0.4735 Acc: 0.7826\n",
      "Epoch 8/15 | Train Loss: 0.4594 Acc: 0.7589\n",
      "Epoch 9/15 | Train Loss: 0.4028 Acc: 0.8024\n",
      "Epoch 10/15 | Train Loss: 0.4099 Acc: 0.7984\n",
      "Epoch 11/15 | Train Loss: 0.4563 Acc: 0.7668\n",
      "Epoch 12/15 | Train Loss: 0.4096 Acc: 0.8063\n",
      "Epoch 13/15 | Train Loss: 0.3972 Acc: 0.7905\n",
      "Epoch 14/15 | Train Loss: 0.3814 Acc: 0.8221\n",
      "Epoch 15/15 | Train Loss: 0.3781 Acc: 0.8221\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7024 Acc: 0.5865\n",
      "Epoch 2/15 | Train Loss: 0.6206 Acc: 0.7722\n",
      "Epoch 3/15 | Train Loss: 0.5657 Acc: 0.7511\n",
      "Epoch 4/15 | Train Loss: 0.4674 Acc: 0.7806\n",
      "Epoch 5/15 | Train Loss: 0.5013 Acc: 0.8017\n",
      "Epoch 6/15 | Train Loss: 0.4922 Acc: 0.7637\n",
      "Epoch 7/15 | Train Loss: 0.4391 Acc: 0.7932\n",
      "Epoch 8/15 | Train Loss: 0.4355 Acc: 0.8143\n",
      "Epoch 9/15 | Train Loss: 0.3983 Acc: 0.8228\n",
      "Epoch 10/15 | Train Loss: 0.3738 Acc: 0.8439\n",
      "Epoch 11/15 | Train Loss: 0.4274 Acc: 0.8270\n",
      "Epoch 12/15 | Train Loss: 0.3663 Acc: 0.8312\n",
      "Epoch 13/15 | Train Loss: 0.3978 Acc: 0.8439\n",
      "Epoch 14/15 | Train Loss: 0.3771 Acc: 0.8270\n",
      "Epoch 15/15 | Train Loss: 0.3573 Acc: 0.8481\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6159 Acc: 0.7018\n",
      "Epoch 2/15 | Train Loss: 0.4879 Acc: 0.7982\n",
      "Epoch 3/15 | Train Loss: 0.3995 Acc: 0.8421\n",
      "Epoch 4/15 | Train Loss: 0.4078 Acc: 0.8377\n",
      "Epoch 5/15 | Train Loss: 0.4138 Acc: 0.8553\n",
      "Epoch 6/15 | Train Loss: 0.3212 Acc: 0.8991\n",
      "Epoch 7/15 | Train Loss: 0.3262 Acc: 0.8596\n",
      "Epoch 8/15 | Train Loss: 0.2827 Acc: 0.8860\n",
      "Epoch 9/15 | Train Loss: 0.2821 Acc: 0.8904\n",
      "Epoch 10/15 | Train Loss: 0.3127 Acc: 0.8684\n",
      "Epoch 11/15 | Train Loss: 0.2470 Acc: 0.9035\n",
      "Epoch 12/15 | Train Loss: 0.3007 Acc: 0.8904\n",
      "Epoch 13/15 | Train Loss: 0.2659 Acc: 0.8947\n",
      "Epoch 14/15 | Train Loss: 0.2533 Acc: 0.9035\n",
      "Epoch 15/15 | Train Loss: 0.2500 Acc: 0.9035\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=0.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.7567 Acc: 0.7266\n",
      "Epoch 2/15 | Train Loss: 0.4106 Acc: 0.7865\n",
      "Epoch 3/15 | Train Loss: 0.3968 Acc: 0.7943\n",
      "Epoch 4/15 | Train Loss: 0.3557 Acc: 0.8125\n",
      "Epoch 5/15 | Train Loss: 0.3306 Acc: 0.8281\n",
      "Epoch 6/15 | Train Loss: 0.3485 Acc: 0.8073\n",
      "Epoch 7/15 | Train Loss: 0.3031 Acc: 0.8333\n",
      "Epoch 8/15 | Train Loss: 0.3094 Acc: 0.8385\n",
      "Epoch 9/15 | Train Loss: 0.2467 Acc: 0.8724\n",
      "Epoch 10/15 | Train Loss: 0.2467 Acc: 0.9010\n",
      "Epoch 11/15 | Train Loss: 0.2690 Acc: 0.8776\n",
      "Epoch 12/15 | Train Loss: 0.2753 Acc: 0.8854\n",
      "Epoch 13/15 | Train Loss: 0.2689 Acc: 0.8776\n",
      "Epoch 14/15 | Train Loss: 0.2593 Acc: 0.8854\n",
      "Epoch 15/15 | Train Loss: 0.2384 Acc: 0.8984\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.6980 Acc: 0.7422\n",
      "Epoch 2/15 | Train Loss: 0.3967 Acc: 0.7839\n",
      "Epoch 3/15 | Train Loss: 0.4286 Acc: 0.7708\n",
      "Epoch 4/15 | Train Loss: 0.3961 Acc: 0.8021\n",
      "Epoch 5/15 | Train Loss: 0.3689 Acc: 0.7917\n",
      "Epoch 6/15 | Train Loss: 0.4005 Acc: 0.8047\n",
      "Epoch 7/15 | Train Loss: 0.3536 Acc: 0.8021\n",
      "Epoch 8/15 | Train Loss: 0.3029 Acc: 0.8333\n",
      "Epoch 9/15 | Train Loss: 0.3115 Acc: 0.8411\n",
      "Epoch 10/15 | Train Loss: 0.3277 Acc: 0.8047\n",
      "Epoch 11/15 | Train Loss: 0.2900 Acc: 0.8438\n",
      "Epoch 12/15 | Train Loss: 0.2799 Acc: 0.8698\n",
      "Epoch 13/15 | Train Loss: 0.3098 Acc: 0.8438\n",
      "Epoch 14/15 | Train Loss: 0.2893 Acc: 0.8620\n",
      "Epoch 15/15 | Train Loss: 0.3070 Acc: 0.8542\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.7357 Acc: 0.7266\n",
      "Epoch 2/15 | Train Loss: 0.4498 Acc: 0.7734\n",
      "Epoch 3/15 | Train Loss: 0.3946 Acc: 0.8177\n",
      "Epoch 4/15 | Train Loss: 0.3801 Acc: 0.7917\n",
      "Epoch 5/15 | Train Loss: 0.3962 Acc: 0.8073\n",
      "Epoch 6/15 | Train Loss: 0.3305 Acc: 0.8099\n",
      "Epoch 7/15 | Train Loss: 0.3236 Acc: 0.8438\n",
      "Epoch 8/15 | Train Loss: 0.3319 Acc: 0.8307\n",
      "Epoch 9/15 | Train Loss: 0.3192 Acc: 0.8516\n",
      "Epoch 10/15 | Train Loss: 0.2766 Acc: 0.8750\n",
      "Epoch 11/15 | Train Loss: 0.3117 Acc: 0.8411\n",
      "Epoch 12/15 | Train Loss: 0.3103 Acc: 0.8516\n",
      "Epoch 13/15 | Train Loss: 0.3040 Acc: 0.8438\n",
      "Epoch 14/15 | Train Loss: 0.2690 Acc: 0.8646\n",
      "Epoch 15/15 | Train Loss: 0.2860 Acc: 0.8438\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.9min\n",
      "Epoch 1/15 | Train Loss: 0.5775 Acc: 0.7873\n",
      "Epoch 2/15 | Train Loss: 0.4191 Acc: 0.8032\n",
      "Epoch 3/15 | Train Loss: 0.3780 Acc: 0.8381\n",
      "Epoch 4/15 | Train Loss: 0.3690 Acc: 0.8476\n",
      "Epoch 5/15 | Train Loss: 0.3473 Acc: 0.8571\n",
      "Epoch 6/15 | Train Loss: 0.3273 Acc: 0.8571\n",
      "Epoch 7/15 | Train Loss: 0.3351 Acc: 0.8540\n",
      "Epoch 8/15 | Train Loss: 0.3393 Acc: 0.8349\n",
      "Epoch 9/15 | Train Loss: 0.2933 Acc: 0.8698\n",
      "Epoch 10/15 | Train Loss: 0.3337 Acc: 0.8540\n",
      "Epoch 11/15 | Train Loss: 0.2824 Acc: 0.8667\n",
      "Epoch 12/15 | Train Loss: 0.3360 Acc: 0.8381\n",
      "Epoch 13/15 | Train Loss: 0.2760 Acc: 0.8857\n",
      "Epoch 14/15 | Train Loss: 0.3170 Acc: 0.8571\n",
      "Epoch 15/15 | Train Loss: 0.3045 Acc: 0.8603\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.8min\n",
      "Epoch 1/15 | Train Loss: 0.4630 Acc: 0.7794\n",
      "Epoch 2/15 | Train Loss: 0.3699 Acc: 0.8754\n",
      "Epoch 3/15 | Train Loss: 0.3699 Acc: 0.8577\n",
      "Epoch 4/15 | Train Loss: 0.3591 Acc: 0.8719\n",
      "Epoch 5/15 | Train Loss: 0.2996 Acc: 0.8754\n",
      "Epoch 6/15 | Train Loss: 0.2918 Acc: 0.8754\n",
      "Epoch 7/15 | Train Loss: 0.2797 Acc: 0.9004\n",
      "Epoch 8/15 | Train Loss: 0.2766 Acc: 0.8932\n",
      "Epoch 9/15 | Train Loss: 0.2513 Acc: 0.9075\n",
      "Epoch 10/15 | Train Loss: 0.2402 Acc: 0.9110\n",
      "Epoch 11/15 | Train Loss: 0.2190 Acc: 0.9110\n",
      "Epoch 12/15 | Train Loss: 0.2440 Acc: 0.9039\n",
      "Epoch 13/15 | Train Loss: 0.2562 Acc: 0.9039\n",
      "Epoch 14/15 | Train Loss: 0.2346 Acc: 0.9039\n",
      "Epoch 15/15 | Train Loss: 0.2491 Acc: 0.8897\n",
      "[CV] END ...multiplier_generated_samples=2, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6668 Acc: 0.6245\n",
      "Epoch 2/15 | Train Loss: 0.6104 Acc: 0.6798\n",
      "Epoch 3/15 | Train Loss: 0.5047 Acc: 0.7431\n",
      "Epoch 4/15 | Train Loss: 0.5172 Acc: 0.7470\n",
      "Epoch 5/15 | Train Loss: 0.5233 Acc: 0.7470\n",
      "Epoch 6/15 | Train Loss: 0.4570 Acc: 0.7747\n",
      "Epoch 7/15 | Train Loss: 0.4618 Acc: 0.7945\n",
      "Epoch 8/15 | Train Loss: 0.4224 Acc: 0.7866\n",
      "Epoch 9/15 | Train Loss: 0.4072 Acc: 0.7905\n",
      "Epoch 10/15 | Train Loss: 0.3611 Acc: 0.8340\n",
      "Epoch 11/15 | Train Loss: 0.3743 Acc: 0.8182\n",
      "Epoch 12/15 | Train Loss: 0.3764 Acc: 0.8261\n",
      "Epoch 13/15 | Train Loss: 0.3579 Acc: 0.8577\n",
      "Epoch 14/15 | Train Loss: 0.3825 Acc: 0.8340\n",
      "Epoch 15/15 | Train Loss: 0.3548 Acc: 0.8458\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6935 Acc: 0.6008\n",
      "Epoch 2/15 | Train Loss: 0.6297 Acc: 0.6957\n",
      "Epoch 3/15 | Train Loss: 0.5489 Acc: 0.7036\n",
      "Epoch 4/15 | Train Loss: 0.5589 Acc: 0.6877\n",
      "Epoch 5/15 | Train Loss: 0.5037 Acc: 0.7708\n",
      "Epoch 6/15 | Train Loss: 0.5117 Acc: 0.7549\n",
      "Epoch 7/15 | Train Loss: 0.4868 Acc: 0.7510\n",
      "Epoch 8/15 | Train Loss: 0.4788 Acc: 0.7510\n",
      "Epoch 9/15 | Train Loss: 0.5306 Acc: 0.7154\n",
      "Epoch 10/15 | Train Loss: 0.4275 Acc: 0.7866\n",
      "Epoch 11/15 | Train Loss: 0.4319 Acc: 0.7866\n",
      "Epoch 12/15 | Train Loss: 0.4576 Acc: 0.7708\n",
      "Epoch 13/15 | Train Loss: 0.4224 Acc: 0.7945\n",
      "Epoch 14/15 | Train Loss: 0.3932 Acc: 0.8063\n",
      "Epoch 15/15 | Train Loss: 0.3871 Acc: 0.8221\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6860 Acc: 0.6403\n",
      "Epoch 2/15 | Train Loss: 0.5660 Acc: 0.6640\n",
      "Epoch 3/15 | Train Loss: 0.5084 Acc: 0.7036\n",
      "Epoch 4/15 | Train Loss: 0.5692 Acc: 0.6838\n",
      "Epoch 5/15 | Train Loss: 0.5191 Acc: 0.7352\n",
      "Epoch 6/15 | Train Loss: 0.5773 Acc: 0.6798\n",
      "Epoch 7/15 | Train Loss: 0.4770 Acc: 0.7787\n",
      "Epoch 8/15 | Train Loss: 0.4867 Acc: 0.7352\n",
      "Epoch 9/15 | Train Loss: 0.4489 Acc: 0.7747\n",
      "Epoch 10/15 | Train Loss: 0.4250 Acc: 0.7945\n",
      "Epoch 11/15 | Train Loss: 0.5014 Acc: 0.7589\n",
      "Epoch 12/15 | Train Loss: 0.4176 Acc: 0.8103\n",
      "Epoch 13/15 | Train Loss: 0.4038 Acc: 0.7787\n",
      "Epoch 14/15 | Train Loss: 0.4054 Acc: 0.8261\n",
      "Epoch 15/15 | Train Loss: 0.4212 Acc: 0.7826\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.6221 Acc: 0.6751\n",
      "Epoch 2/15 | Train Loss: 0.6323 Acc: 0.7468\n",
      "Epoch 3/15 | Train Loss: 0.5868 Acc: 0.7595\n",
      "Epoch 4/15 | Train Loss: 0.5415 Acc: 0.7595\n",
      "Epoch 5/15 | Train Loss: 0.4575 Acc: 0.7890\n",
      "Epoch 6/15 | Train Loss: 0.4593 Acc: 0.8059\n",
      "Epoch 7/15 | Train Loss: 0.4242 Acc: 0.8143\n",
      "Epoch 8/15 | Train Loss: 0.5172 Acc: 0.7806\n",
      "Epoch 9/15 | Train Loss: 0.4493 Acc: 0.7932\n",
      "Epoch 10/15 | Train Loss: 0.4411 Acc: 0.8143\n",
      "Epoch 11/15 | Train Loss: 0.4228 Acc: 0.8101\n",
      "Epoch 12/15 | Train Loss: 0.3975 Acc: 0.8186\n",
      "Epoch 13/15 | Train Loss: 0.4159 Acc: 0.8143\n",
      "Epoch 14/15 | Train Loss: 0.3931 Acc: 0.8397\n",
      "Epoch 15/15 | Train Loss: 0.3784 Acc: 0.8481\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5390 Acc: 0.7895\n",
      "Epoch 2/15 | Train Loss: 0.4467 Acc: 0.8509\n",
      "Epoch 3/15 | Train Loss: 0.4240 Acc: 0.8509\n",
      "Epoch 4/15 | Train Loss: 0.3963 Acc: 0.8421\n",
      "Epoch 5/15 | Train Loss: 0.4147 Acc: 0.8465\n",
      "Epoch 6/15 | Train Loss: 0.3581 Acc: 0.8553\n",
      "Epoch 7/15 | Train Loss: 0.3593 Acc: 0.8509\n",
      "Epoch 8/15 | Train Loss: 0.3141 Acc: 0.8772\n",
      "Epoch 9/15 | Train Loss: 0.3200 Acc: 0.8772\n",
      "Epoch 10/15 | Train Loss: 0.3220 Acc: 0.8816\n",
      "Epoch 11/15 | Train Loss: 0.2985 Acc: 0.8904\n",
      "Epoch 12/15 | Train Loss: 0.2818 Acc: 0.9123\n",
      "Epoch 13/15 | Train Loss: 0.2830 Acc: 0.8816\n",
      "Epoch 14/15 | Train Loss: 0.2860 Acc: 0.8904\n",
      "Epoch 15/15 | Train Loss: 0.2716 Acc: 0.8904\n",
      "[CV] END .multiplier_generated_samples=0.5, scale_factor=1.5; total time= 2.7min\n",
      "Epoch 1/15 | Train Loss: 0.5132 Acc: 0.7712\n",
      "Epoch 2/15 | Train Loss: 0.4228 Acc: 0.8009\n",
      "Epoch 3/15 | Train Loss: 0.3697 Acc: 0.8101\n",
      "Epoch 4/15 | Train Loss: 0.3686 Acc: 0.8124\n",
      "Epoch 5/15 | Train Loss: 0.3627 Acc: 0.8238\n",
      "Epoch 6/15 | Train Loss: 0.3325 Acc: 0.8375\n",
      "Epoch 7/15 | Train Loss: 0.3589 Acc: 0.8352\n",
      "Epoch 8/15 | Train Loss: 0.2932 Acc: 0.8604\n",
      "Epoch 9/15 | Train Loss: 0.2795 Acc: 0.8604\n",
      "Epoch 10/15 | Train Loss: 0.3127 Acc: 0.8558\n",
      "Epoch 11/15 | Train Loss: 0.2857 Acc: 0.8673\n",
      "Epoch 12/15 | Train Loss: 0.2541 Acc: 0.8924\n",
      "Epoch 13/15 | Train Loss: 0.2690 Acc: 0.8787\n",
      "Epoch 14/15 | Train Loss: 0.2624 Acc: 0.8924\n",
      "Epoch 15/15 | Train Loss: 0.2749 Acc: 0.8604\n",
      "Fold 10 Test Accuracy: 0.6924\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "3775bb66b3ec0b30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNNGAN: Generowanie jedynie syntetycznego zbioru",
   "id": "80ac9a11d58ab73b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T19:08:29.308545Z",
     "start_time": "2025-05-28T19:07:12.831170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "CLASSIFIER_EPOCHS = 20\n",
    "OVERSAMPLER_EPOCHS = 200\n",
    "\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())\n",
    "\n",
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "\n",
    "    CnnGan = CNNGANWrapper(dataset,device,BATCH_SIZE,OVERSAMPLER_EPOCHS)\n",
    "\n",
    "    CnnGan.fit(train_idx)\n",
    "\n",
    "\n",
    "    estimator = CNNGANResNetEstimator(\n",
    "        dataset=dataset,\n",
    "        gan_model=CnnGan.gan_model,\n",
    "        device=device,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classifier_epochs=CLASSIFIER_EPOCHS,\n",
    "        multiplier_generated_samples='synthetic'\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'scale_factor': [0.5, 1, 1.5]  # Czynnik skalujący szum w GAN\n",
    "    }\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=2,\n",
    "        cv=None,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(train_idx)\n",
    "\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    test_ds = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    f2, bal_acc, recall, specificity = best_estimator.trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f2_score': f2,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        **best_params\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Zapis wyników z każdego folda\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('CNNGAN_synthetic_cross_validation_results.csv', index=False)"
   ],
   "id": "48063847943c17eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "===== Fold 1 =====\n",
      "Epoch 1: Generator Loss = 9.9420, Discriminator Loss = 8.7419\n",
      "Epoch 2: Generator Loss = 10.4801, Discriminator Loss = 7.9568\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/1 | Train Loss: 0.1175 Acc: 0.9385\n",
      "[CV] END ...................................scale_factor=0.5; total time=  30.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 47\u001B[0m\n\u001B[1;32m     32\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscale_factor\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1.5\u001B[39m]  \u001B[38;5;66;03m# Czynnik skalujący szum w GAN\u001B[39;00m\n\u001B[1;32m     34\u001B[0m }\n\u001B[1;32m     37\u001B[0m random_search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[1;32m     38\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[1;32m     39\u001B[0m     param_distributions\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     45\u001B[0m )\n\u001B[0;32m---> 47\u001B[0m \u001B[43mrandom_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m best_estimator \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m     50\u001B[0m best_params \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1020\u001B[0m     )\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1024\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1951\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1950\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1951\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1953\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m   1954\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1955\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    967\u001B[0m         )\n\u001B[1;32m    968\u001B[0m     )\n\u001B[0;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:864\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_train \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 864\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    866\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/wrappers.py:206\u001B[0m, in \u001B[0;36mCNNGANResNetEstimator.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_neg_only_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_neg_only_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultiplier_generated_samples \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msynthetic\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 206\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgan_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_new_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnum_neg\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscale_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_factor\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgan_model\u001B[38;5;241m.\u001B[39mgenerate_new_data(\n\u001B[1;32m    213\u001B[0m         num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(num_neg \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultiplier_generated_samples),\n\u001B[1;32m    214\u001B[0m         output_dir\u001B[38;5;241m=\u001B[39mgen_dir,\n\u001B[1;32m    215\u001B[0m         scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_factor\n\u001B[1;32m    216\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/CNNGAN.py:274\u001B[0m, in \u001B[0;36mCNNGAN.generate_new_data\u001B[0;34m(self, num_samples, output_dir, scale_factor)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Zapisz każdy obraz jako osobny plik\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, img \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(fake_imgs):\n\u001B[0;32m--> 274\u001B[0m     \u001B[43mvutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msample_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.png\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/utils.py:148\u001B[0m, in \u001B[0;36msave_image\u001B[0;34m(tensor, fp, format, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m grid \u001B[38;5;241m=\u001B[39m make_grid(tensor, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001B[39;00m\n\u001B[0;32m--> 148\u001B[0m ndarr \u001B[38;5;241m=\u001B[39m \u001B[43mgrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclamp_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    149\u001B[0m im \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(ndarr)\n\u001B[1;32m    150\u001B[0m im\u001B[38;5;241m.\u001B[39msave(fp, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f86383a2cbf6728a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNNVAE: Generowanie jedynie syntetycznego zbioru",
   "id": "5384aeb89d42b6ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T19:01:24.549483Z",
     "start_time": "2025-05-28T18:49:43.742063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "IMG_SIZE   = 128\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS   = 3\n",
    "CLASSIFIER_EPOCHS = 20\n",
    "OVERSAMPLER_EPOCHS = 200\n",
    "\n",
    "dataset = CapsuleDataset(pos_dir=pos_folder, neg_dirs=neg_folder, transform=transform)\n",
    "print(dataset.__len__())\n",
    "\n",
    "labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(rskf.split(X=np.zeros(len(labels)), y=labels), start=1):\n",
    "    print(f\"===== Fold {fold_idx} =====\")\n",
    "\n",
    "\n",
    "    CnnVae = CNNVAEWrapper(dataset,device,BATCH_SIZE,OVERSAMPLER_EPOCHS)\n",
    "\n",
    "    CnnVae.fit(train_idx)\n",
    "\n",
    "\n",
    "    estimator = CNNVAEResNetEstimator(\n",
    "        dataset=dataset,\n",
    "        vae_model=CnnVae.vae_model,\n",
    "        device=device,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classifier_epochs=CLASSIFIER_EPOCHS,\n",
    "        multiplier_generated_samples='synthetic'\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'mu_multiplier': [0.8, 1.2],\n",
    "        'logvar_multiplier': [0.5, 1.5]\n",
    "    }\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1,\n",
    "        cv=None,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(train_idx)\n",
    "\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    test_ds = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    f2, bal_acc, recall, specificity = best_estimator.trainer.validate(test_loader)\n",
    "    print(f\"Fold {fold_idx} Test Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f2_score': f2,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        **best_params\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Zapis wyników z każdego folda\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('CNNVAE_synthetic_cross_validation_results.csv', index=False)"
   ],
   "id": "5326b7fac0e68825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "===== Fold 1 =====\n",
      "Train Epoch: 1 | Loss: 2340.9797\n",
      "Train Epoch: 2 | Loss: 2321.9733\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/5 | Train Loss: 0.0738 Acc: 0.9795\n",
      "Epoch 2/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 5/5 | Train Loss: 0.0002 Acc: 1.0000\n",
      "[CV] END ...........logvar_multiplier=0.5, mu_multiplier=1.2; total time= 1.4min\n",
      "Epoch 1/5 | Train Loss: 0.0975 Acc: 0.9549\n",
      "Epoch 2/5 | Train Loss: 0.0006 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0033 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0167 Acc: 0.9959\n",
      "Epoch 5/5 | Train Loss: 0.0001 Acc: 1.0000\n",
      "[CV] END ...........logvar_multiplier=0.5, mu_multiplier=1.2; total time= 1.6min\n",
      "Epoch 1/5 | Train Loss: 0.0423 Acc: 0.9919\n",
      "Epoch 2/5 | Train Loss: 0.0007 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0017 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0002 Acc: 1.0000\n",
      "Epoch 5/5 | Train Loss: 0.0003 Acc: 1.0000\n",
      "[CV] END ...........logvar_multiplier=0.5, mu_multiplier=1.2; total time= 1.7min\n",
      "Epoch 1/5 | Train Loss: 0.0471 Acc: 0.9715\n",
      "Epoch 2/5 | Train Loss: 0.0019 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 5/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "[CV] END ...........logvar_multiplier=0.5, mu_multiplier=1.2; total time= 1.9min\n",
      "Epoch 1/5 | Train Loss: 0.0596 Acc: 0.9800\n",
      "Epoch 2/5 | Train Loss: 0.0011 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "Epoch 5/5 | Train Loss: 0.0005 Acc: 1.0000\n",
      "[CV] END ...........logvar_multiplier=0.5, mu_multiplier=1.2; total time= 2.2min\n",
      "Epoch 1/5 | Train Loss: 0.0524 Acc: 0.9714\n",
      "Epoch 2/5 | Train Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 3/5 | Train Loss: 0.0004 Acc: 1.0000\n",
      "Epoch 4/5 | Train Loss: 0.0003 Acc: 1.0000\n",
      "Epoch 5/5 | Train Loss: 0.0002 Acc: 1.0000\n",
      "Fold 1 Test Accuracy: 0.5000\n",
      "===== Fold 2 =====\n",
      "Train Epoch: 1 | Loss: 2332.3847\n",
      "Train Epoch: 2 | Loss: 2309.7331\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/5 | Train Loss: 0.1234 Acc: 0.9590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 48\u001B[0m\n\u001B[1;32m     32\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmu_multiplier\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m1.2\u001B[39m],\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogvar_multiplier\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1.5\u001B[39m]\n\u001B[1;32m     35\u001B[0m }\n\u001B[1;32m     38\u001B[0m random_search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[1;32m     39\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[1;32m     40\u001B[0m     param_distributions\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     46\u001B[0m )\n\u001B[0;32m---> 48\u001B[0m \u001B[43mrandom_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m best_estimator \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m     51\u001B[0m best_params \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1020\u001B[0m     )\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1024\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1951\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1950\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1951\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1953\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m   1954\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1955\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    967\u001B[0m         )\n\u001B[1;32m    968\u001B[0m     )\n\u001B[0;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:864\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_train \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 864\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    866\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/wrappers.py:103\u001B[0m, in \u001B[0;36mCNNVAEResNetEstimator.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# 3. Trening ResNet na wygenerowanych danych\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m ResNetTrainer()\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/ResNet34.py:82\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(self, train_loader, num_epochs)\u001B[0m\n\u001B[1;32m     79\u001B[0m inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     80\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 82\u001B[0m inputs_resnet \u001B[38;5;241m=\u001B[39m (inputs \u001B[38;5;241m-\u001B[39m imagenet_mean) \u001B[38;5;241m/\u001B[39m imagenet_std\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     85\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(inputs_resnet)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:275\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)\n\u001B[1;32m    274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[0;32m--> 275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer4(x)\n\u001B[1;32m    278\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:96\u001B[0m, in \u001B[0;36mBasicBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     93\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(out)\n\u001B[1;32m     94\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n\u001B[0;32m---> 96\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(out)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImbalancedDataProject/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd00a2c6ac1af0ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
